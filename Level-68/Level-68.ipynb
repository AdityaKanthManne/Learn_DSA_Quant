{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-05T03:33:35.194638Z",
     "start_time": "2025-12-05T03:33:30.154772Z"
    }
   },
   "source": [
    "# level68_hrp_riskparity.py\n",
    "# Hierarchical Risk Parity (HRP) vs Risk-Parity vs Equal-Weight\n",
    "# Universe: liquid ETFs (SPY, QQQ, IWM, EFA, EEM, TLT, LQD, GLD)\n",
    "# Data: free from yfinance (daily close)\n",
    "#\n",
    "# Outputs:\n",
    "#   - level68_hrp_portfolio.csv\n",
    "#   - level68_hrp_summary.json\n",
    "#\n",
    "# Rebalancing:\n",
    "#   - Month-end (ME), rolling covariance window.\n",
    "#   - Long-only, fully invested.\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import Sequence, Tuple, List, Dict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "\n",
    "\n",
    "# --------------------------- Config ---------------------------\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    symbols: Tuple[str, ...] = (\n",
    "        \"SPY\", \"QQQ\", \"IWM\", \"EFA\", \"EEM\", \"TLT\", \"LQD\", \"GLD\"\n",
    "    )\n",
    "    start: str = \"2010-01-01\"\n",
    "    cov_lookback: int = 252     # trading days for covariance estimation\n",
    "    min_lookback: int = 126     # minimum days before starting\n",
    "    rebalance_freq: str = \"ME\"  # month-end\n",
    "    out_csv: str = \"level68_hrp_portfolio.csv\"\n",
    "    out_json: str = \"level68_hrp_summary.json\"\n",
    "    seed: int = 42\n",
    "\n",
    "\n",
    "# --------------------------- Data Loader ---------------------------\n",
    "\n",
    "def load_prices(symbols: Sequence[str], start: str) -> pd.DataFrame:\n",
    "    \"\"\"Download adjusted close prices for a list of symbols from yfinance.\"\"\"\n",
    "    frames = []\n",
    "    for s in symbols:\n",
    "        px = yf.download(s, start=start, auto_adjust=True, progress=False)\n",
    "        if px.empty:\n",
    "            raise RuntimeError(f\"No price data downloaded for {s}.\")\n",
    "        if \"Close\" not in px.columns:\n",
    "            raise RuntimeError(f\"'Close' column missing for {s}.\")\n",
    "\n",
    "        close = px[\"Close\"].copy()\n",
    "        close.name = s\n",
    "        frames.append(close)\n",
    "\n",
    "    prices = pd.concat(frames, axis=1).sort_index()\n",
    "    prices = prices.dropna(how=\"all\")\n",
    "    prices = prices.ffill().dropna(how=\"any\")\n",
    "    return prices\n",
    "\n",
    "\n",
    "def compute_returns(prices: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Log returns of price series.\"\"\"\n",
    "    rets = np.log(prices).diff()\n",
    "    rets = rets.dropna(how=\"all\")\n",
    "    return rets\n",
    "\n",
    "\n",
    "# --------------------------- Portfolio Math ---------------------------\n",
    "\n",
    "def cov_to_corr(cov: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Convert covariance matrix to correlation matrix.\"\"\"\n",
    "    if not isinstance(cov, pd.DataFrame):\n",
    "        cov = pd.DataFrame(cov)\n",
    "    diag = np.diag(cov.values)\n",
    "    diag = np.where(diag <= 0, 1e-12, diag)\n",
    "    std = np.sqrt(diag)\n",
    "    denom = np.outer(std, std)\n",
    "    corr = cov.values / denom\n",
    "    corr[~np.isfinite(corr)] = 0.0\n",
    "    np.fill_diagonal(corr, 1.0)\n",
    "    return pd.DataFrame(corr, index=cov.index, columns=cov.columns)\n",
    "\n",
    "\n",
    "def risk_parity_weights(cov: pd.DataFrame, max_iter: int = 1000,\n",
    "                        tol: float = 1e-8) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Simple equal risk-contribution (risk-parity) solver.\n",
    "    Long-only, fully invested.\n",
    "    \"\"\"\n",
    "    if not isinstance(cov, pd.DataFrame):\n",
    "        cov = pd.DataFrame(cov)\n",
    "\n",
    "    cols = cov.columns\n",
    "    C = cov.values.astype(float)\n",
    "    n = C.shape[0]\n",
    "\n",
    "    diag = np.diag(C)\n",
    "    diag = np.where(diag <= 0, 1e-6, diag)\n",
    "    w = 1.0 / diag\n",
    "    w /= w.sum()\n",
    "\n",
    "    for _ in range(max_iter):\n",
    "        port_var = float(w @ C @ w)\n",
    "        if port_var <= 0:\n",
    "            break\n",
    "\n",
    "        mrc = C @ w\n",
    "        rc = w * mrc\n",
    "        target = port_var / n\n",
    "\n",
    "        rc_safe = np.where(rc == 0.0, target, rc)\n",
    "        w_new = w * target / rc_safe\n",
    "\n",
    "        w_new = np.maximum(w_new, 0.0)\n",
    "        s = w_new.sum()\n",
    "        if s <= 0:\n",
    "            w_new = np.ones(n) / n\n",
    "        else:\n",
    "            w_new /= s\n",
    "\n",
    "        if np.max(np.abs(w_new - w)) < tol:\n",
    "            w = w_new\n",
    "            break\n",
    "        w = w_new\n",
    "\n",
    "    return pd.Series(w, index=cols)\n",
    "\n",
    "\n",
    "def equal_weight(symbols: Sequence[str]) -> pd.Series:\n",
    "    n = len(symbols)\n",
    "    w = np.ones(n) / n\n",
    "    return pd.Series(w, index=list(symbols))\n",
    "\n",
    "\n",
    "# --------------------------- HRP Helpers ---------------------------\n",
    "\n",
    "def correl_to_dist(corr: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Distance matrix from correlation: d_ij = sqrt(0.5 * (1 - rho_ij)).\"\"\"\n",
    "    if not isinstance(corr, pd.DataFrame):\n",
    "        corr = pd.DataFrame(corr)\n",
    "    d = np.sqrt(0.5 * (1.0 - corr.values))\n",
    "    np.fill_diagonal(d, 0.0)\n",
    "    return pd.DataFrame(d, index=corr.index, columns=corr.columns)\n",
    "\n",
    "\n",
    "def single_linkage_order(dist: pd.DataFrame) -> List[int]:\n",
    "    \"\"\"\n",
    "    Naive single-linkage hierarchical clustering to produce a leaf order\n",
    "    (quasi-diagonalization) without external libraries.\n",
    "    \"\"\"\n",
    "    n = dist.shape[0]\n",
    "    # Distances between leaves (0..n-1)\n",
    "    D = dist.values.astype(float)\n",
    "    np.fill_diagonal(D, np.inf)\n",
    "\n",
    "    # Each cluster id maps to list of leaf indices\n",
    "    clusters: Dict[int, List[int]] = {i: [i] for i in range(n)}\n",
    "    next_id = n\n",
    "\n",
    "    while len(clusters) > 1:\n",
    "        ids = list(clusters.keys())\n",
    "        best = None\n",
    "        best_pair = None\n",
    "\n",
    "        for i in range(len(ids)):\n",
    "            for j in range(i + 1, len(ids)):\n",
    "                ci = ids[i]\n",
    "                cj = ids[j]\n",
    "                members_i = clusters[ci]\n",
    "                members_j = clusters[cj]\n",
    "                # Single-linkage: min distance between any pair\n",
    "                sub = D[np.ix_(members_i, members_j)]\n",
    "                d_ij = float(sub.min())\n",
    "                if (best is None) or (d_ij < best):\n",
    "                    best = d_ij\n",
    "                    best_pair = (ci, cj)\n",
    "\n",
    "        if best_pair is None:\n",
    "            break\n",
    "\n",
    "        a, b = best_pair\n",
    "        new_members = clusters[a] + clusters[b]\n",
    "        del clusters[a]\n",
    "        del clusters[b]\n",
    "        clusters[next_id] = new_members\n",
    "        next_id += 1\n",
    "\n",
    "    # Remaining cluster contains the quasi-diagonal order\n",
    "    final_members = list(clusters.values())[0]\n",
    "    return final_members\n",
    "\n",
    "\n",
    "def hrp_weights(cov: pd.DataFrame) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Hierarchical Risk Parity weights (Lopez de Prado style), long-only.\n",
    "    \"\"\"\n",
    "    if not isinstance(cov, pd.DataFrame):\n",
    "        cov = pd.DataFrame(cov)\n",
    "    cols = list(cov.columns)\n",
    "    n = len(cols)\n",
    "\n",
    "    # Correlation and distance\n",
    "    corr = cov_to_corr(cov)\n",
    "    dist = correl_to_dist(corr)\n",
    "\n",
    "    # Quasi-diagonalization via naive single-linkage\n",
    "    order = single_linkage_order(dist)\n",
    "    cov_reordered = cov.values[order][:, order]\n",
    "\n",
    "    # Recursive bisection\n",
    "    diag = np.diag(cov_reordered)\n",
    "    diag = np.where(diag <= 0, 1e-8, diag)\n",
    "    inv_var = 1.0 / diag\n",
    "\n",
    "    weights = np.ones(n)\n",
    "\n",
    "    clusters = [np.arange(n)]\n",
    "\n",
    "    while clusters:\n",
    "        cluster = clusters.pop(0)\n",
    "        if len(cluster) <= 1:\n",
    "            continue\n",
    "\n",
    "        split = len(cluster) // 2\n",
    "        left = cluster[:split]\n",
    "        right = cluster[split:]\n",
    "\n",
    "        inv_var_left = inv_var[left]\n",
    "        inv_var_right = inv_var[right]\n",
    "\n",
    "        w_left = inv_var_left / inv_var_left.sum()\n",
    "        w_right = inv_var_right / inv_var_right.sum()\n",
    "\n",
    "        cov_left = cov_reordered[np.ix_(left, left)]\n",
    "        cov_right = cov_reordered[np.ix_(right, right)]\n",
    "\n",
    "        var_left = float(w_left @ cov_left @ w_left)\n",
    "        var_right = float(w_right @ cov_right @ w_right)\n",
    "        if var_left + var_right == 0:\n",
    "            alpha = 0.5\n",
    "        else:\n",
    "            alpha = 1.0 - var_left / (var_left + var_right)\n",
    "\n",
    "        weights[left] *= alpha\n",
    "        weights[right] *= (1.0 - alpha)\n",
    "\n",
    "        clusters.append(left)\n",
    "        clusters.append(right)\n",
    "\n",
    "    # Map back to original order\n",
    "    w_final = np.zeros(n)\n",
    "    for pos, asset_idx in enumerate(order):\n",
    "        w_final[asset_idx] = weights[pos]\n",
    "\n",
    "    return pd.Series(w_final, index=cols)\n",
    "\n",
    "\n",
    "# --------------------------- Backtest Engine ---------------------------\n",
    "\n",
    "def compute_rebalance_dates(rets: pd.DataFrame, freq: str) -> pd.DatetimeIndex:\n",
    "    \"\"\"Compute rebalance dates as month-end using 'ME'.\"\"\"\n",
    "    if freq != \"ME\":\n",
    "        raise ValueError(\"This script expects rebalance_freq 'ME' (month-end).\")\n",
    "    return rets.resample(\"ME\").last().index\n",
    "\n",
    "\n",
    "def run_backtest(prices: pd.DataFrame, cfg: Config) -> Tuple[pd.DataFrame, dict]:\n",
    "    rets = compute_returns(prices)\n",
    "\n",
    "    rebal_dates = compute_rebalance_dates(rets, cfg.rebalance_freq)\n",
    "    symbols = list(cfg.symbols)\n",
    "    idx = rets.index\n",
    "\n",
    "    w_ew = pd.DataFrame(index=idx, columns=symbols, dtype=float)\n",
    "    w_rp = pd.DataFrame(index=idx, columns=symbols, dtype=float)\n",
    "    w_hrp = pd.DataFrame(index=idx, columns=symbols, dtype=float)\n",
    "\n",
    "    for d in rebal_dates:\n",
    "        window = rets.loc[:d].tail(cfg.cov_lookback)\n",
    "        if window.shape[0] < cfg.min_lookback:\n",
    "            continue\n",
    "\n",
    "        cov = window.cov()\n",
    "        if cov.isnull().any().any():\n",
    "            continue\n",
    "\n",
    "        w_ew_d = equal_weight(symbols)\n",
    "        w_rp_d = risk_parity_weights(cov)\n",
    "        w_hrp_d = hrp_weights(cov)\n",
    "\n",
    "        w_ew.loc[d] = w_ew_d\n",
    "        w_rp.loc[d] = w_rp_d\n",
    "        w_hrp.loc[d] = w_hrp_d\n",
    "\n",
    "    w_ew = w_ew.ffill().dropna()\n",
    "    w_rp = w_rp.ffill().dropna()\n",
    "    w_hrp = w_hrp.ffill().dropna()\n",
    "\n",
    "    common_idx = w_ew.index.intersection(w_rp.index).intersection(w_hrp.index)\n",
    "    rets = rets.reindex(common_idx).dropna(how=\"any\")\n",
    "    prices = prices.reindex(common_idx)\n",
    "    w_ew = w_ew.reindex(common_idx)\n",
    "    w_rp = w_rp.reindex(common_idx)\n",
    "    w_hrp = w_hrp.reindex(common_idx)\n",
    "\n",
    "    port_ew = (w_ew * rets).sum(axis=1).rename(\"ret_ew\")\n",
    "    port_rp = (w_rp * rets).sum(axis=1).rename(\"ret_rp\")\n",
    "    port_hrp = (w_hrp * rets).sum(axis=1).rename(\"ret_hrp\")\n",
    "\n",
    "    out = pd.DataFrame(index=common_idx)\n",
    "    out[symbols] = prices\n",
    "    out[[f\"ret_{s}\" for s in symbols]] = rets.add_prefix(\"ret_\")\n",
    "    out[[f\"w_ew_{s}\" for s in symbols]] = w_ew.add_prefix(\"w_ew_\")\n",
    "    out[[f\"w_rp_{s}\" for s in symbols]] = w_rp.add_prefix(\"w_rp_\")\n",
    "    out[[f\"w_hrp_{s}\" for s in symbols]] = w_hrp.add_prefix(\"w_hrp_\")\n",
    "    out[port_ew.name] = port_ew\n",
    "    out[port_rp.name] = port_rp\n",
    "    out[port_hrp.name] = port_hrp\n",
    "\n",
    "    out[\"eq_ew\"] = (1.0 + port_ew).cumprod()\n",
    "    out[\"eq_rp\"] = (1.0 + port_rp).cumprod()\n",
    "    out[\"eq_hrp\"] = (1.0 + port_hrp).cumprod()\n",
    "\n",
    "    def stats_from_returns(r: pd.Series) -> dict:\n",
    "        if r.empty:\n",
    "            return dict(ann_ret=np.nan, ann_vol=np.nan,\n",
    "                        sharpe=np.nan, max_dd=np.nan)\n",
    "        daily_ret = r\n",
    "        ann_ret = (1.0 + daily_ret.mean()) ** 252 - 1.0\n",
    "        ann_vol = float(daily_ret.std() * np.sqrt(252))\n",
    "        sharpe = ann_ret / ann_vol if ann_vol > 0 else np.nan\n",
    "\n",
    "        eq = (1.0 + daily_ret).cumprod()\n",
    "        peak = eq.cummax()\n",
    "        dd = eq / peak - 1.0\n",
    "        max_dd = float(dd.min()) if not dd.empty else np.nan\n",
    "\n",
    "        return dict(\n",
    "            ann_ret=float(ann_ret),\n",
    "            ann_vol=float(ann_vol),\n",
    "            sharpe=float(sharpe),\n",
    "            max_dd=float(max_dd),\n",
    "        )\n",
    "\n",
    "    summary = {\n",
    "        \"config\": asdict(cfg),\n",
    "        \"start_date\": str(common_idx.min().date()) if len(common_idx) else None,\n",
    "        \"end_date\": str(common_idx.max().date()) if len(common_idx) else None,\n",
    "        \"n_days\": int(len(common_idx)),\n",
    "        \"EW\": stats_from_returns(port_ew),\n",
    "        \"RiskParity\": stats_from_returns(port_rp),\n",
    "        \"HRP\": stats_from_returns(port_hrp),\n",
    "    }\n",
    "\n",
    "    return out, summary\n",
    "\n",
    "\n",
    "# --------------------------- I/O ---------------------------\n",
    "\n",
    "def save_outputs(out: pd.DataFrame, summary: dict, cfg: Config) -> None:\n",
    "    out.to_csv(cfg.out_csv, index=True, date_format=\"%Y-%m-%d\")\n",
    "    with open(cfg.out_json, \"w\") as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "\n",
    "    print(f\"[OK] Saved daily series → {cfg.out_csv}\")\n",
    "    print(f\"[OK] Saved summary → {cfg.out_json}\")\n",
    "    if summary[\"start_date\"] and summary[\"end_date\"]:\n",
    "        print(\n",
    "            f\"Period {summary['start_date']} → {summary['end_date']}, \"\n",
    "            f\"n_days={summary['n_days']}\"\n",
    "        )\n",
    "\n",
    "    for name in [\"EW\", \"RiskParity\", \"HRP\"]:\n",
    "        s = summary[name]\n",
    "        print(\n",
    "            f\"{name}: AnnRet={s['ann_ret']*100:.2f}%, \"\n",
    "            f\"AnnVol={s['ann_vol']*100:.2f}%, \"\n",
    "            f\"Sharpe={s['sharpe']:.2f}, \"\n",
    "            f\"MaxDD={s['max_dd']*100:.2f}%\"\n",
    "        )\n",
    "\n",
    "\n",
    "# --------------------------- CLI ---------------------------\n",
    "\n",
    "def parse_args() -> Config:\n",
    "    p = argparse.ArgumentParser(\n",
    "        description=\"Level-68: Hierarchical Risk Parity vs Risk-Parity vs Equal-Weight\"\n",
    "    )\n",
    "    p.add_argument(\n",
    "        \"--symbols\",\n",
    "        type=str,\n",
    "        default=\"SPY,QQQ,IWM,EFA,EEM,TLT,LQD,GLD\",\n",
    "        help=\"Comma-separated tickers (default: SPY,QQQ,IWM,EFA,EEM,TLT,LQD,GLD)\",\n",
    "    )\n",
    "    p.add_argument(\"--start\", type=str, default=\"2010-01-01\")\n",
    "    p.add_argument(\"--cov-lookback\", type=int, default=252)\n",
    "    p.add_argument(\"--min-lookback\", type=int, default=126)\n",
    "    p.add_argument(\n",
    "        \"--rebalance-freq\",\n",
    "        type=str,\n",
    "        default=\"ME\",\n",
    "        help=\"Rebalance frequency (use 'ME' for month-end).\",\n",
    "    )\n",
    "    p.add_argument(\"--csv\", type=str, default=\"level68_hrp_portfolio.csv\")\n",
    "    p.add_argument(\"--json\", type=str, default=\"level68_hrp_summary.json\")\n",
    "    p.add_argument(\"--seed\", type=int, default=42)\n",
    "\n",
    "    a = p.parse_args()\n",
    "    symbols = tuple(s.strip() for s in a.symbols.split(\",\") if s.strip())\n",
    "    return Config(\n",
    "        symbols=symbols,\n",
    "        start=a.start,\n",
    "        cov_lookback=a.cov_lookback,\n",
    "        min_lookback=a.min_lookback,\n",
    "        rebalance_freq=a.rebalance_freq,\n",
    "        out_csv=a.csv,\n",
    "        out_json=a.json,\n",
    "        seed=a.seed,\n",
    "    )\n",
    "\n",
    "\n",
    "# --------------------------- Main ---------------------------\n",
    "\n",
    "def main() -> None:\n",
    "    cfg = parse_args()\n",
    "    np.random.seed(cfg.seed)\n",
    "\n",
    "    print(f\"[INFO] Downloading prices for {cfg.symbols} from {cfg.start} ...\")\n",
    "    prices = load_prices(cfg.symbols, cfg.start)\n",
    "    print(f\"[INFO] Got {len(prices)} price rows.\")\n",
    "\n",
    "    out, summary = run_backtest(prices, cfg)\n",
    "    save_outputs(out, summary, cfg)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Jupyter/PyCharm shim to ignore kernel-related args\n",
    "    import sys\n",
    "\n",
    "    sys.argv = [sys.argv[0]] + [\n",
    "        arg\n",
    "        for arg in sys.argv[1:]\n",
    "        if arg != \"-f\" and not (arg.endswith(\".json\") and \"kernel\" in arg)\n",
    "    ]\n",
    "    main()\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Downloading prices for ('SPY', 'QQQ', 'IWM', 'EFA', 'EEM', 'TLT', 'LQD', 'GLD') from 2010-01-01 ...\n",
      "[INFO] Got 4006 price rows.\n",
      "[OK] Saved daily series → level68_hrp_portfolio.csv\n",
      "[OK] Saved summary → level68_hrp_summary.json\n",
      "Period 2010-07-31 → 2025-12-31, n_days=3895\n",
      "EW: AnnRet=8.64%, AnnVol=11.78%, Sharpe=0.73, MaxDD=-27.96%\n",
      "RiskParity: AnnRet=6.43%, AnnVol=13.27%, Sharpe=0.48, MaxDD=-35.42%\n",
      "HRP: AnnRet=5.79%, AnnVol=7.93%, Sharpe=0.73, MaxDD=-25.93%\n"
     ]
    }
   ],
   "execution_count": 1
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
