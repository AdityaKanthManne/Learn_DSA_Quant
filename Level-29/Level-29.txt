# Level-29 — Meta-Labeling with Triple-Barrier, Purged K-Fold, Prob-Driven Sizing (one cell)

import os, math, warnings
from pathlib import Path
import numpy as np, pandas as pd
import matplotlib.pyplot as plt

warnings.filterwarnings("ignore")

# ---------------- Config ----------------
TICKER = "AAPL"
YEARS  = 3
FEE    = 0.0005     # proportional transaction cost per unit turnover
SLIPP  = 0.0002     # extra slippage per trade
H      = 10         # vertical barrier horizon in bars (days)
UP_M   = 2.0        # upper barrier = UP_M * daily_vol
DN_M   = 2.0        # lower barrier = DN_M * daily_vol
CUSUM_THRESHOLD = 0.008  # event sensitivity on returns (0.8%)

N_SPLITS = 5
RANDOM_STATE = 42
OUT_DIR = Path(r"C:\Users\adity\Downloads\Learn_DSA_Quant\Level-29")
OUT_DIR.mkdir(parents=True, exist_ok=True)

# ---------------- Data ----------------
def load_daily_prices(ticker, years=3):
    try:
        import yfinance as yf
        df = yf.download(ticker, period=f"{years}y", interval="1d", auto_adjust=True, progress=False)
    except Exception:
        df = pd.DataFrame()
    if df is None or df.empty:
        # Synthetic fallback (geometric random walk)
        rng = np.random.default_rng(29)
        n = years*252
        r = rng.normal(0.0004, 0.015, n)
        px = 100*np.exp(np.cumsum(r))
        idx = pd.bdate_range(end=pd.Timestamp.today().normalize(), periods=n)
        df = pd.DataFrame({"Open":px, "High":px*(1+rng.uniform(0,0.01,n)),
                           "Low":px*(1-rng.uniform(0,0.01,n)), "Close":px, "Volume":rng.integers(1e6,5e6,n)}, index=idx)
        global TICKER
        TICKER = "SYNTH"
    df = df.dropna()
    df = df[~df.index.duplicated(keep="last")]
    return df

df = load_daily_prices(TICKER, YEARS)
px = df["Close"].astype(float).copy()
rets = px.pct_change().fillna(0.0)

# ---------------- Helpers ----------------
def ewma_vol(r, span=50):
    return r.ewm(span=span, adjust=False).std().fillna(method="bfill")

def cusum_filter(r, threshold):
    """Lopez de Prado CUSUM to pick event timestamps."""
    s_pos, s_neg = 0.0, 0.0
    t_events = []
    for t, x in r.items():
        s_pos = max(0.0, s_pos + x)
        s_neg = min(0.0, s_neg + x)
        if s_pos > threshold:
            s_pos = 0.0
            t_events.append(t)
        elif s_neg < -threshold:
            s_neg = 0.0
            t_events.append(t)
    return pd.Index(sorted(set(t_events)))

def get_vertical_barriers(t_events, h, index):
    """Return a Series mapping event time -> vertical barrier time (min(t+h, end))."""
    out = {}
    for t in t_events:
        loc = index.get_loc(t)
        end_loc = min(loc + h, len(index)-1)
        out[t] = index[end_loc]
    return pd.Series(out)

def get_triple_barrier_labels(close, t_events, vbar, up_m, dn_m, daily_vol):
    """
    Returns DataFrame with:
      't1' vertical barrier time,
      'label' in {+1, -1, 0},
      'ret' realized return at barrier hit (signed),
      'side' = +1 (we assume long meta-labeling; you can inject side model here).
    """
    events = pd.DataFrame(index=t_events)
    events["t1"] = vbar.reindex(t_events)
    events["trgt"] = daily_vol.reindex(t_events).fillna(method="bfill")
    events["side"] = 1.0  # if you have a primary model (long/short), put its sign here

    labels = []
    for t0, row in events.iterrows():
        t1 = row["t1"]
        if pd.isna(t1):
            continue
        c0 = float(close.loc[t0])
        up_lvl = c0 * (1 + up_m * row["trgt"])
        dn_lvl = c0 * (1 - dn_m * row["trgt"])

        path = close.loc[t0:t1]
        hit_up = (path >= up_lvl).idxmax() if (path >= up_lvl).any() else None
        hit_dn = (path <= dn_lvl).idxmax() if (path <= dn_lvl).any() else None

        # choose first barrier hit in time
        hit_time = None
        label = 0
        if hit_up is not None and hit_dn is not None:
            hit_time = hit_up if hit_up <= hit_dn else hit_dn
            label   = 1 if hit_time is hit_up else -1
        elif hit_up is not None:
            hit_time = hit_up; label = 1
        elif hit_dn is not None:
            hit_time = hit_dn; label = -1
        else:
            hit_time = t1
            label = 0 if float(close.loc[t1]) == c0 else (1 if close.loc[t1] > c0 else -1)

        ret = float(close.loc[hit_time] / c0 - 1.0)
        labels.append((t0, hit_time, label, ret, row["side"]))

    L = pd.DataFrame(labels, columns=["t0","t1","label","ret","side"]).set_index("t0")
    return L

# Purged K-Fold (time series aware)
def purged_kfold_splits(index, n_splits=5):
    n = len(index)
    fold_sizes = np.full(n_splits, n//n_splits, dtype=int)
    fold_sizes[: n % n_splits] += 1
    bounds = np.cumsum(fold_sizes)
    starts = np.concatenate([[0], bounds[:-1]])
    for i in range(n_splits):
        test_idx = np.arange(starts[i], bounds[i])
        train_idx = np.concatenate([np.arange(0, starts[i]), np.arange(bounds[i], n)])
        yield train_idx, test_idx

# Concurrency weights (simple version): inverse of number of overlapping labels at each time
def sample_weights(L: pd.DataFrame):
    counts = pd.Series(0.0, index=px.index)
    for t0, row in L.iterrows():
        counts.loc[t0:row["t1"]] += 1.0
    w = pd.Series(0.0, index=L.index)
    for t0, row in L.iterrows():
        c = counts.loc[t0:row["t1"]]
        w[t0] = (1.0 / c.replace(0, np.nan)).mean()
    w = w.fillna(w.median())
    return w / w.mean()

# ---------------- Feature Engineering ----------------
def build_features(df):
    c = df["Close"].astype(float)
    v = df["Volume"].astype(float)
    r1 = c.pct_change().fillna(0)
    r5 = c.pct_change(5).fillna(0)
    r10= c.pct_change(10).fillna(0)
    mom = (c / c.rolling(20).mean() - 1).fillna(0)
    vol = r1.rolling(20).std().fillna(0)
    rv  = (np.log(c).diff()**2).rolling(20).sum().fillna(0)
    vchg= v.pct_change().fillna(0)
    vma = (v / v.rolling(20).mean() - 1).fillna(0)
    rsi = 100 - (100 / (1 + (r1.clip(lower=0).rolling(14).mean() / (-r1.clip(upper=0).rolling(14).mean()).abs()).fillna(1)))
    rsi = rsi.fillna(50)

    X = pd.DataFrame({
        "r1":r1, "r5":r5, "r10":r10, "mom20":mom, "vol20":vol, "rv20":rv, "vchg":vchg, "vma20":vma, "rsi14":rsi
    }, index=df.index)
    return X.replace([np.inf, -np.inf], 0).fillna(0)

# ---------------- Pipeline ----------------
daily_vol = ewma_vol(rets, span=50).clip(lower=1e-6)
events_idx = cusum_filter(rets, CUSUM_THRESHOLD)
vbar = get_vertical_barriers(events_idx, H, px.index)
labels = get_triple_barrier_labels(px, events_idx, vbar, UP_M, DN_M, daily_vol)
labels = labels.dropna()

# Meta-label target is 1 if primary side would be profitable, else 0 (we'll predict prob of success)
y = (labels["label"]==1).astype(int)
t1 = labels["t1"]
weights = sample_weights(labels)

X = build_features(df)
# Align features at event start
X_evt = X.reindex(labels.index).fillna(0)

# ---------------- Model ----------------
try:
    import xgboost as xgb
    USE_XGB = True
except Exception:
    USE_XGB = False
from sklearn.metrics import roc_auc_score, precision_recall_fscore_support, confusion_matrix
from sklearn.ensemble import GradientBoostingClassifier

if USE_XGB:
    model = xgb.XGBClassifier(
        n_estimators=400, max_depth=3, learning_rate=0.05,
        subsample=0.8, colsample_bytree=0.8, reg_lambda=1.0,
        random_state=RANDOM_STATE, n_jobs=0
    )
else:
    model = GradientBoostingClassifier(
        n_estimators=400, learning_rate=0.05, max_depth=3, random_state=RANDOM_STATE
    )

# Purged K-Fold CV
idx = X_evt.index
auc_scores = []
proba = pd.Series(index=idx, dtype=float)

for tr, te in purged_kfold_splits(idx, N_SPLITS):
    tr_idx = idx[tr]; te_idx = idx[te]
    X_tr, y_tr, w_tr = X_evt.loc<tr_idx>, y.loc<tr_idx>, weights.loc<tr_idx>
    X_te, y_te = X_evt.loc[te_idx], y.loc[te_idx]

    model.fit(X_tr.values, y_tr.values, sample_weight=w_tr.values if USE_XGB else w_tr.values)
    p = model.predict_proba(X_te.values)[:,1]
    proba.loc[te_idx] = p
    if y_te.nunique() > 1:
        auc_scores.append(roc_auc_score(y_te.values, p))

cv_auc = float(np.mean(auc_scores)) if auc_scores else float("nan")
print(f"Purged {N_SPLITS}-Fold CV AUC: {cv_auc:.4f}")

# Refit on full data for deployment
model.fit(X_evt.values, y.values, sample_weight=weights.values if USE_XGB else weights.values)

# ---------------- Bet sizing & backtest ----------------
# Bet size b = max(0, 2p-1) in [0,1]; signal side is +1 from labels (can swap for primary model)
b = np.clip(2*proba - 1.0, 0.0, 1.0)
side = labels["side"].reindex(b.index).fillna(1.0)
target_pos = (b * side).clip(-1,1)  # meta-labeling often uses filter (enter on p>0.5); here continuous

# Convert discrete event targets to daily position by carrying forward between t0 and t1
pos = pd.Series(0.0, index=px.index, dtype=float)
for t0, row in labels.iterrows():
    t_end = row["t1"]
    val = float(target_pos.get(t0, 0.0))
    if pd.notna(val):
        pos.loc[t0:t_end] = val

pos = pos.fillna(method="ffill").fillna(0.0)
turnover = pos.diff().abs().fillna(0.0)

gross_ret = pos.shift(1).fillna(0.0) * rets  # apply at next bar open-close approx
costs = turnover * (FEE + SLIPP)
net_ret = gross_ret - costs

equity = (1 + net_ret).cumprod()
bh = (px / px.iloc[0]).rename("bh")

# ---------------- Metrics ----------------
def kpis(e):
    r = e.pct_change().dropna()
    ann = 252
    cagr = (e.iloc[-1] / e.iloc[0]) ** (ann/len(e)) - 1
    vol  = r.std() * np.sqrt(ann)
    sr   = r.mean()/r.std() * np.sqrt(ann) if r.std()>0 else np.nan
    mdd  = float(((e.cummax() - e)/e.cummax()).max())
    return {"CAGR":cagr, "Vol":vol, "Sharpe":sr, "MaxDD":mdd}

kpi_strat = kpis(equity)
kpi_bh    = kpis(bh.reindex(equity.index, method="nearest") * 1.0)

print("Strategy KPIs:", {k: round(v,4) if pd.notna(v) else v for k,v in kpi_strat.items()})
print("Buy&Hold  KPIs:", {k: round(v,4) if pd.notna(v) else v for k,v in kpi_bh.items()})

# ---------------- Plots ----------------
plt.figure(figsize=(11,6))
plt.plot(equity.index, equity.values, label="Meta-labeled Strategy")
plt.plot(bh.index, bh.values, label="Buy & Hold")
plt.title(f"Level-29: Meta-Labeling (Triple-Barrier) — {TICKER}\nCV AUC={cv_auc:.3f}  Fee={FEE}, Slipp={SLIPP}")
plt.legend(); plt.tight_layout(); plt.show()

plt.figure(figsize=(11,2.5))
plt.plot(pos.index, pos.values)
plt.title("Daily Position (after probability-driven sizing)")
plt.tight_layout(); plt.show()

# ---------------- Save artifacts ----------------
labels.assign(prob=proba, bet=b, weight=weights).to_csv(OUT_DIR / f"{TICKER}_labels_prob_bets.csv")
pd.DataFrame({"date":equity.index, "equity":equity.values}).to_csv(OUT_DIR / f"{TICKER}_equity.csv", index=False)
pd.DataFrame({"date":pos.index, "position":pos.values}).to_csv(OUT_DIR / f"{TICKER}_positions.csv", index=False)
X_evt.to_csv(OUT_DIR / f"{TICKER}_features_at_events.csv")

print(f"Saved outputs to {OUT_DIR}")
