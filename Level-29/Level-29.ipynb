{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-28T04:36:35.807983Z",
     "start_time": "2025-10-28T04:36:33.905868Z"
    }
   },
   "source": [
    "# ============================ Level-29 / Clean & Robust ============================\n",
    "# Triple-barrier labeling with robust price extraction (duplicates-safe), tz-safe, adaptive events\n",
    "# ==================================================================================\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# --------------------------- Config ---------------------------\n",
    "TICKER        = \"AAPL\"\n",
    "YEARS         = 3\n",
    "FREQ          = \"1d\"       # switch to \"1h\"/\"1m\" if needed (consider increasing YEARS for more labels)\n",
    "VOL_SPAN      = 50\n",
    "CUSUM_MIN_EVT = 50\n",
    "BASE_H        = 10\n",
    "UP_M, DN_M    = 1.0, 1.0\n",
    "\n",
    "# --------------------------- Utilities ---------------------------\n",
    "def ensure_series(x: pd.Series, name: Optional[str] = None) -> pd.Series:\n",
    "    if isinstance(x, pd.DataFrame):\n",
    "        # squeeze 1st column deterministically\n",
    "        x = x.iloc[:, 0]\n",
    "    if not isinstance(x, pd.Series):\n",
    "        raise TypeError(\"Expected a pandas Series.\")\n",
    "    if name:\n",
    "        x = x.rename(name)\n",
    "    x = pd.to_numeric(x, errors=\"coerce\")\n",
    "    x = x.dropna()\n",
    "    x = x[~x.index.duplicated(keep=\"last\")].sort_index()\n",
    "    return x\n",
    "\n",
    "def tz_to_naive_eastern(idx: pd.DatetimeIndex) -> pd.DatetimeIndex:\n",
    "    if idx.tz is None:\n",
    "        return idx\n",
    "    return idx.tz_convert(\"US/Eastern\").tz_localize(None)\n",
    "\n",
    "def extract_close_series(df: pd.DataFrame) -> pd.Series:\n",
    "    \"\"\"Return a clean Close series even if columns are duplicated.\"\"\"\n",
    "    candidates = []\n",
    "    if \"Adj Close\" in df.columns:\n",
    "        candidates.append(\"Adj Close\")\n",
    "    if \"Close\" in df.columns:\n",
    "        candidates.append(\"Close\")\n",
    "    if not candidates:\n",
    "        raise RuntimeError(\"No Close/Adj Close column in downloaded data.\")\n",
    "\n",
    "    # Prefer Adj Close; fall back to Close\n",
    "    for col in candidates:\n",
    "        s = df[col]\n",
    "        if isinstance(s, pd.DataFrame):  # duplicate columns case\n",
    "            s = s.iloc[:, 0]\n",
    "        s = ensure_series(s, \"Close\")\n",
    "        if not s.empty:\n",
    "            return s\n",
    "    raise RuntimeError(\"Unable to extract a non-empty Close series.\")\n",
    "\n",
    "def load_prices(ticker: str, years: int, freq: str = \"1d\") -> pd.Series:\n",
    "    start = (datetime.utcnow() - timedelta(days=int(365*years + 10))).date()\n",
    "    df = yf.download(ticker, start=str(start), interval=freq, auto_adjust=True, progress=False)\n",
    "    if df.empty:\n",
    "        raise RuntimeError(\"No data returned. Check ticker/frequency/network.\")\n",
    "\n",
    "    # Normalize index to US/Eastern tz-naive\n",
    "    if isinstance(df.index, pd.DatetimeIndex):\n",
    "        idx = df.index\n",
    "        try:\n",
    "            # if tz-aware, this will work; if tz-naive, it will raise -> handled by except\n",
    "            idx = tz_to_naive_eastern(idx.tz_convert(\"UTC\"))\n",
    "        except Exception:\n",
    "            if idx.tz is not None:\n",
    "                idx = tz_to_naive_eastern(idx)\n",
    "        df.index = idx\n",
    "\n",
    "    px = extract_close_series(df)  # duplicates-safe path\n",
    "    return px\n",
    "\n",
    "def ewma_vol(rets: pd.Series, span: int = 50) -> pd.Series:\n",
    "    r = ensure_series(rets, \"r\")\n",
    "    v = r.ewm(span=span, adjust=False).std()\n",
    "    return v.rename(\"vol\")\n",
    "\n",
    "def cusum_filter(r: pd.Series, threshold: float) -> pd.DatetimeIndex:\n",
    "    r = ensure_series(r, \"r\")\n",
    "    s_pos = 0.0\n",
    "    s_neg = 0.0\n",
    "    t_events = []\n",
    "    for t, x in r.items():\n",
    "        xf = float(x)\n",
    "        s_pos = max(0.0, s_pos + xf)\n",
    "        s_neg = min(0.0, s_neg + xf)\n",
    "        if s_pos > threshold:\n",
    "            s_pos = 0.0\n",
    "            t_events.append(t)\n",
    "        elif s_neg < -threshold:\n",
    "            s_neg = 0.0\n",
    "            t_events.append(t)\n",
    "    return pd.DatetimeIndex(t_events)\n",
    "\n",
    "def get_vertical_barriers(t_events: pd.DatetimeIndex, H: int, full_index: pd.DatetimeIndex) -> pd.Series:\n",
    "    full_index = pd.DatetimeIndex(full_index)\n",
    "    vbar = {}\n",
    "    pos_map = {ts: i for i, ts in enumerate(full_index)}\n",
    "    n = len(full_index)\n",
    "    for t0 in t_events:\n",
    "        if t0 in pos_map:\n",
    "            i = pos_map[t0]\n",
    "        else:\n",
    "            i = full_index.searchsorted(t0, side=\"left\")\n",
    "        j = min(n - 1, i + int(H))\n",
    "        vbar[t0] = full_index[j]\n",
    "    return pd.Series(vbar, name=\"t1\")\n",
    "\n",
    "def first_cross_idx(path_vals: np.ndarray, level: float, cmp: str) -> Optional[int]:\n",
    "    if cmp == \"ge\":\n",
    "        mask = path_vals >= level\n",
    "    elif cmp == \"le\":\n",
    "        mask = path_vals <= level\n",
    "    else:\n",
    "        raise ValueError(\"cmp must be 'ge' or 'le'\")\n",
    "    where = np.where(mask)[0]\n",
    "    return int(where[0]) if where.size else None\n",
    "\n",
    "def get_triple_barrier_labels(\n",
    "    close: pd.Series,\n",
    "    t_events: pd.DatetimeIndex,\n",
    "    vbar: pd.Series,\n",
    "    up_m: float,\n",
    "    dn_m: float,\n",
    "    daily_vol: pd.Series\n",
    ") -> pd.DataFrame:\n",
    "    close = ensure_series(close, \"Close\")\n",
    "    daily_vol = ensure_series(daily_vol, \"vol\")\n",
    "    vbar = vbar.dropna()\n",
    "    rows = []\n",
    "    for t0 in t_events:\n",
    "        if t0 not in close.index or t0 not in vbar.index:\n",
    "            continue\n",
    "        t1 = vbar.loc[t0]\n",
    "        if t1 not in close.index:\n",
    "            j = close.index.searchsorted(t1, side=\"left\")\n",
    "            j = max(0, min(j, len(close) - 1))\n",
    "            t1 = close.index[j]\n",
    "        c0 = float(close.loc[t0])\n",
    "        trgt = float(daily_vol.get(t0, np.nan))\n",
    "        if not np.isfinite(trgt) or trgt <= 0:\n",
    "            continue\n",
    "\n",
    "        up_lvl = c0 * (1 + up_m * trgt)\n",
    "        dn_lvl = c0 * (1 - dn_m * trgt)\n",
    "\n",
    "        # include t0 so a cross at the first bar can be detected consistently\n",
    "        path_idx = close.loc[t0:t1].index\n",
    "        path = close.loc[path_idx].to_numpy(dtype=float)\n",
    "\n",
    "        iu = first_cross_idx(path, up_lvl, \"ge\")\n",
    "        idn = first_cross_idx(path, dn_lvl, \"le\")\n",
    "\n",
    "        label = 0\n",
    "        t_hit = t1\n",
    "        if iu is not None and idn is not None:\n",
    "            if iu < idn:\n",
    "                label = 1\n",
    "                t_hit = path_idx[iu]\n",
    "            elif idn < iu:\n",
    "                label = -1\n",
    "                t_hit = path_idx[idn]\n",
    "            else:\n",
    "                label = 0\n",
    "                t_hit = path_idx[iu]\n",
    "        elif iu is not None:\n",
    "            label = 1\n",
    "            t_hit = path_idx[iu]\n",
    "        elif idn is not None:\n",
    "            label = -1\n",
    "            t_hit = path_idx[idn]\n",
    "        else:\n",
    "            rvt = float(close.loc[t1] / c0 - 1.0)\n",
    "            label = 1 if rvt > 0 else (-1 if rvt < 0 else 0)\n",
    "            t_hit = t1\n",
    "\n",
    "        rows.append({\"t0\": t0, \"t1\": t1, \"t_hit\": t_hit, \"label\": label, \"trgt\": trgt})\n",
    "\n",
    "    out = pd.DataFrame.from_records(rows).set_index(\"t0\").sort_index()\n",
    "    return out\n",
    "\n",
    "def build_features(px: pd.Series) -> pd.DataFrame:\n",
    "    px = ensure_series(px, \"Close\")\n",
    "    r1 = px.pct_change().fillna(0.0)\n",
    "    r5 = px.pct_change(5).fillna(0.0)\n",
    "    vol20 = r1.rolling(20).std().fillna(0.0)\n",
    "    mom10 = px.pct_change(10).fillna(0.0)\n",
    "    ma10 = px.rolling(10).mean()\n",
    "    ma20 = px.rolling(20).mean()\n",
    "    ma_ratio = (ma10 / ma20 - 1.0).replace([np.inf, -np.inf], 0.0).fillna(0.0)\n",
    "    feat = pd.DataFrame({\"r1\": r1, \"r5\": r5, \"vol20\": vol20, \"mom10\": mom10, \"ma_ratio\": ma_ratio}, index=px.index)\n",
    "    return feat\n",
    "\n",
    "def adaptive_events_and_labels(\n",
    "    close: pd.Series,\n",
    "    rets: pd.Series,\n",
    "    vol_span: int = 50,\n",
    "    min_events: int = 50,\n",
    "    base_H: int = 10,\n",
    "    up_m: float = 1.0,\n",
    "    dn_m: float = 1.0\n",
    ") -> Tuple[pd.DataFrame, pd.Series, pd.DatetimeIndex, int, float]:\n",
    "    close = ensure_series(close, \"Close\")\n",
    "    rets = ensure_series(rets, \"r\")\n",
    "    daily_vol = ewma_vol(rets, span=vol_span).clip(lower=1e-8).fillna(0.0)\n",
    "\n",
    "    vol_std = float(rets.std()) if np.isfinite(float(rets.std())) else 0.01\n",
    "    thr_fracs = [0.50, 0.35, 0.25, 0.18, 0.12, 0.08, 0.05, 0.03]\n",
    "    H_choices = [base_H, int(base_H*1.5), base_H*2, base_H*3]\n",
    "\n",
    "    labels = pd.DataFrame()\n",
    "    used_thr, used_H, events_idx = None, None, pd.DatetimeIndex([])\n",
    "\n",
    "    for H_try in H_choices:\n",
    "        for frac in thr_fracs:\n",
    "            thr = max(1e-4, frac * vol_std)\n",
    "            events_idx = cusum_filter(rets, thr)\n",
    "            if len(events_idx) == 0:\n",
    "                continue\n",
    "            vbar = get_vertical_barriers(events_idx, H_try, close.index)\n",
    "            lab = get_triple_barrier_labels(close, events_idx, vbar, up_m, dn_m, daily_vol).dropna()\n",
    "            if len(lab) >= min_events:\n",
    "                labels = lab\n",
    "                used_thr, used_H = thr, H_try\n",
    "                break\n",
    "        if len(labels) >= min_events:\n",
    "            break\n",
    "\n",
    "    if labels.empty:\n",
    "        for H_try in H_choices[max(1, len(H_choices)//2):]:\n",
    "            for frac in thr_fracs + [0.02, 0.015, 0.01]:\n",
    "                thr = max(5e-5, frac * vol_std)\n",
    "                events_idx = cusum_filter(rets, thr)\n",
    "                if len(events_idx) == 0:\n",
    "                    continue\n",
    "                vbar = get_vertical_barriers(events_idx, H_try, close.index)\n",
    "                lab = get_triple_barrier_labels(close, events_idx, vbar, up_m*0.5, dn_m*0.5, daily_vol).dropna()\n",
    "                if len(lab) >= max(10, min_events//2):\n",
    "                    labels = lab\n",
    "                    used_thr, used_H = thr, H_try\n",
    "                    break\n",
    "            if not labels.empty:\n",
    "                break\n",
    "\n",
    "    return labels, daily_vol, events_idx, (used_H or base_H), float(used_thr or 0.0)\n",
    "\n",
    "# --------------------------- Main ---------------------------\n",
    "px = load_prices(TICKER, YEARS, FREQ)\n",
    "rets = px.pct_change().replace([np.inf, -np.inf], 0.0).fillna(0.0)\n",
    "\n",
    "labels, daily_vol, events_idx, H_used, thr_used = adaptive_events_and_labels(\n",
    "    close=px,\n",
    "    rets=rets,\n",
    "    vol_span=VOL_SPAN,\n",
    "    min_events=CUSUM_MIN_EVT,\n",
    "    base_H=BASE_H,\n",
    "    up_m=UP_M,\n",
    "    dn_m=DN_M\n",
    ")\n",
    "\n",
    "print(f\"[Adaptive] events={len(events_idx)} labels={len(labels)}  H={H_used}  thr≈{thr_used:.6g}\")\n",
    "\n",
    "if labels.empty:\n",
    "    raise RuntimeError(\n",
    "        \"Adaptive labeling still produced no labels. \"\n",
    "        \"Increase YEARS, switch to higher frequency, or lower CUSUM_MIN_EVT.\"\n",
    "    )\n",
    "\n",
    "# Features & target (align on t0)\n",
    "X_all = build_features(px)\n",
    "X_evt = X_all.reindex(labels.index).dropna()\n",
    "y = (labels[\"label\"] == 1).astype(int).reindex(X_evt.index)\n",
    "\n",
    "# Time split\n",
    "cut = int(len(X_evt) * 0.7)\n",
    "X_tr, X_te = X_evt.iloc[:cut], X_evt.iloc[cut:]\n",
    "y_tr, y_te = y.iloc[:cut], y.iloc[cut:]\n",
    "\n",
    "# Scale & model\n",
    "scaler = StandardScaler(with_mean=True, with_std=True)\n",
    "X_tr_sc = pd.DataFrame(scaler.fit_transform(X_tr), index=X_tr.index, columns=X_tr.columns)\n",
    "X_te_sc = pd.DataFrame(scaler.transform(X_te), index=X_te.index, columns=X_te.columns)\n",
    "\n",
    "clf = RandomForestClassifier(\n",
    "    n_estimators=300,\n",
    "    max_depth=6,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    class_weight=\"balanced_subsample\"\n",
    ")\n",
    "clf.fit(X_tr_sc, y_tr)\n",
    "pred = clf.predict(X_te_sc)\n",
    "proba = clf.predict_proba(X_te_sc)[:, 1]\n",
    "\n",
    "print(\"\\n=== Classification Report (long = 1) ===\")\n",
    "print(classification_report(y_te, pred, digits=3))\n",
    "\n",
    "out = pd.DataFrame({\"y_true\": y_te, \"y_pred\": pred, \"proba\": proba}, index=X_te_sc.index)\n",
    "out.to_csv(f\"{TICKER}_labels_predictions.csv\", index_label=\"date\")\n",
    "print(f\"Saved predictions to {TICKER}_labels_predictions.csv\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Adaptive] events=410 labels=410  H=10  thr≈0.00867296\n",
      "\n",
      "=== Classification Report (long = 1) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.417     0.328     0.367        61\n",
      "           1      0.453     0.548     0.496        62\n",
      "\n",
      "    accuracy                          0.439       123\n",
      "   macro avg      0.435     0.438     0.432       123\n",
      "weighted avg      0.435     0.439     0.432       123\n",
      "\n",
      "Saved predictions to AAPL_labels_predictions.csv\n"
     ]
    }
   ],
   "execution_count": 13
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
