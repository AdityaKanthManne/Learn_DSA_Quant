{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-27T16:16:46.211079Z",
     "start_time": "2025-12-27T16:16:44.375386Z"
    }
   },
   "source": [
    "# level42_vpin_flow_toxicity.py\n",
    "# Free-only VPIN using Binance public 1m klines (no API key).\n",
    "# Outputs:\n",
    "#   - CSV : level42_vpin_buckets.csv  (bucket_time, buy, sell, vol, imb, vpin, toxic_flag)\n",
    "#   - JSON: level42_vpin_metrics.json (means/quantiles/config)\n",
    "# Usage:\n",
    "#   python level42_vpin_flow_toxicity.py --symbol BTCUSDT --days 3 --bucket-mult 3 --m 50 --toxic-q 0.9\n",
    "# Notes:\n",
    "#   - This uses a bar-sign proxy for buy/sell splits. If you have trade prints, replace split_volume_signed().\n",
    "\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import math\n",
    "import argparse\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import Optional, Dict\n",
    "from collections import deque\n",
    "\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter, Retry\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# ----------------------------- Config -----------------------------\n",
    "@dataclass\n",
    "class Config:\n",
    "    symbol: str = \"BTCUSDT\"        # Binance spot symbol\n",
    "    interval: str = \"1m\"           # 1-minute klines\n",
    "    days: int = 3                  # how many days back to pull\n",
    "    bucket_mult: float = 3.0       # bucket size = bucket_mult * median(1m volume)\n",
    "    m: int = 50                    # VPIN smoothing window (in buckets)\n",
    "    toxic_q: float = 0.90          # toxic when VPIN >= this quantile (computed from result)\n",
    "    base_url: str = \"https://api.binance.com\"\n",
    "    csv_path: str = \"level42_vpin_buckets.csv\"\n",
    "    json_path: str = \"level42_vpin_metrics.json\"\n",
    "    timeout: float = 10.0          # HTTP timeout\n",
    "    backoff_sleep: float = 0.15    # sleep between paged requests\n",
    "    verify_ssl: bool = True        # set False only if your env has broken certs\n",
    "\n",
    "\n",
    "# ----------------------------- HTTP Session (Retry/Backoff) -----------------------------\n",
    "def build_http_session(cfg: Config) -> requests.Session:\n",
    "    session = requests.Session()\n",
    "    retries = Retry(\n",
    "        total=5,\n",
    "        backoff_factor=0.5,\n",
    "        status_forcelist=(429, 500, 502, 503, 504),\n",
    "        allowed_methods=(\"GET\",),\n",
    "        raise_on_status=False,\n",
    "    )\n",
    "    adapter = HTTPAdapter(max_retries=retries)\n",
    "    session.mount(\"https://\", adapter)\n",
    "    session.mount(\"http://\", adapter)\n",
    "    session.verify = cfg.verify_ssl\n",
    "    return session\n",
    "\n",
    "\n",
    "# ----------------------------- Loader (Binance public klines) -----------------------------\n",
    "def fetch_klines(session: requests.Session, cfg: Config,\n",
    "                 start_ms: Optional[int] = None,\n",
    "                 end_ms: Optional[int] = None,\n",
    "                 limit: int = 1000):\n",
    "    url = f\"{cfg.base_url}/api/v3/klines\"\n",
    "    params = {\"symbol\": cfg.symbol, \"interval\": cfg.interval, \"limit\": limit}\n",
    "    if start_ms is not None:\n",
    "        params[\"startTime\"] = int(start_ms)\n",
    "    if end_ms is not None:\n",
    "        params[\"endTime\"] = int(end_ms)\n",
    "\n",
    "    resp = session.get(url, params=params, timeout=cfg.timeout)\n",
    "    resp.raise_for_status()\n",
    "    return resp.json()\n",
    "\n",
    "\n",
    "def load_minutes(session: requests.Session, cfg: Config) -> pd.DataFrame:\n",
    "    # Pull last `days` of klines with forward pagination from start -> end\n",
    "    end = int(time.time() * 1000)\n",
    "    start = end - cfg.days * 24 * 60 * 60 * 1000\n",
    "    frames = []\n",
    "    cur_start = start\n",
    "    last_end_guard = -1\n",
    "\n",
    "    while True:\n",
    "        data = fetch_klines(session, cfg, start_ms=cur_start, end_ms=end, limit=1000)\n",
    "        if not data:\n",
    "            break\n",
    "\n",
    "        df = pd.DataFrame(data, columns=[\n",
    "            \"open_time\", \"open\", \"high\", \"low\", \"close\", \"volume\",\n",
    "            \"close_time\", \"qav\", \"num_trades\", \"taker_buy_vol\", \"taker_buy_qav\", \"ignore\"\n",
    "        ])\n",
    "\n",
    "        # Basic numeric casting & timezone to UTC\n",
    "        for col in [\"open\", \"high\", \"low\", \"close\", \"volume\"]:\n",
    "            df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "        df[\"open_time\"] = pd.to_datetime(df[\"open_time\"], unit=\"ms\", utc=True)\n",
    "        df[\"close_time\"] = pd.to_datetime(df[\"close_time\"], unit=\"ms\", utc=True)\n",
    "\n",
    "        frames.append(df[[\"open_time\", \"close_time\", \"open\", \"high\", \"low\", \"close\", \"volume\"]])\n",
    "\n",
    "        # pagination: next start = last open_time + 1 minute\n",
    "        last_open = int(df[\"open_time\"].iloc[-1].value / 1e6)  # ms\n",
    "        if last_open == last_end_guard:  # safety: prevent infinite loops\n",
    "            break\n",
    "        last_end_guard = last_open\n",
    "        cur_start = last_open + 60_000\n",
    "\n",
    "        if df.shape[0] < 1000 or cur_start >= end:\n",
    "            break\n",
    "\n",
    "        time.sleep(cfg.backoff_sleep)\n",
    "\n",
    "    if not frames:\n",
    "        raise RuntimeError(\"No klines returned; check symbol/interval or network.\")\n",
    "\n",
    "    out = pd.concat(frames).drop_duplicates(subset=[\"open_time\"]).set_index(\"open_time\").sort_index()\n",
    "    # Drop rows with missing essential fields\n",
    "    out = out.dropna(subset=[\"open\", \"close\", \"volume\"])\n",
    "    # Remove zero/negative volumes (shouldn't happen, but guard)\n",
    "    out = out[out[\"volume\"] > 0]\n",
    "    if out.empty:\n",
    "        raise RuntimeError(\"Klines loaded, but all rows invalid after cleaning.\")\n",
    "    return out\n",
    "\n",
    "\n",
    "# ----------------------------- Signed Volume (bar-sign proxy) -----------------------------\n",
    "def split_volume_signed(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Approximate buy/sell volume using the sign of (close - open).\"\"\"\n",
    "    ret = df[\"close\"] - df[\"open\"]\n",
    "    sign = np.sign(ret.to_numpy(dtype=float))\n",
    "    vol = df[\"volume\"].to_numpy(dtype=float)\n",
    "\n",
    "    buy = np.where(sign > 0, vol, np.where(sign < 0, 0.0, 0.5 * vol))\n",
    "    sell = np.where(sign < 0, vol, np.where(sign > 0, 0.0, 0.5 * vol))\n",
    "\n",
    "    out = df.copy()\n",
    "    out[\"buy_vol\"] = buy\n",
    "    out[\"sell_vol\"] = sell\n",
    "    return out\n",
    "\n",
    "\n",
    "# ----------------------------- Volume-Time Bucketing -----------------------------\n",
    "def build_volume_buckets(bars: pd.DataFrame, bucket_vol: float, m: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Convert 1m bars with (buy_vol, sell_vol, volume) into equal-volume buckets.\n",
    "    - Proportionally split a bar if it straddles a bucket boundary.\n",
    "    - Maintain a deque for O(1) VPIN smoothing over last m buckets.\n",
    "    \"\"\"\n",
    "    if not np.isfinite(bucket_vol) or bucket_vol <= 0:\n",
    "        raise ValueError(\"bucket_vol must be positive and finite.\")\n",
    "\n",
    "    buy_cum = sell_cum = vol_cum = 0.0\n",
    "    vpin_window = deque(maxlen=m)\n",
    "\n",
    "    rows = []\n",
    "    for ts, row in bars.iterrows():\n",
    "        v = float(row[\"volume\"])\n",
    "        if v <= 0 or not np.isfinite(v):\n",
    "            continue\n",
    "        buy = float(row[\"buy_vol\"])\n",
    "        sell = float(row[\"sell_vol\"])\n",
    "\n",
    "        remaining = v\n",
    "        while remaining > 1e-12:\n",
    "            need = bucket_vol - vol_cum\n",
    "            take = remaining if remaining <= need + 1e-12 else need\n",
    "            frac = (take / v) if v > 0 else 0.0\n",
    "\n",
    "            buy_cum += buy * frac\n",
    "            sell_cum += sell * frac\n",
    "            vol_cum += take\n",
    "            remaining -= take\n",
    "\n",
    "            if vol_cum >= bucket_vol - 1e-12:  # close bucket\n",
    "                imb = abs(buy_cum - sell_cum) / bucket_vol  # 0..1+ (rare >1 if data dirty)\n",
    "                vpin_window.append(imb)\n",
    "                vpin = float(sum(vpin_window) / len(vpin_window))\n",
    "\n",
    "                rows.append({\n",
    "                    \"bucket_time\": ts,   # timestamp of bar that closed the bucket\n",
    "                    \"buy\": buy_cum,\n",
    "                    \"sell\": sell_cum,\n",
    "                    \"vol\": vol_cum,\n",
    "                    \"imb\": imb,\n",
    "                    \"vpin\": vpin\n",
    "                })\n",
    "                # reset bucket accumulators for next bucket\n",
    "                buy_cum = sell_cum = vol_cum = 0.0\n",
    "\n",
    "    if not rows:\n",
    "        raise RuntimeError(\"No buckets formed (bucket too large?). Try lowering --bucket-mult or increase --days.\")\n",
    "\n",
    "    out = pd.DataFrame(rows).set_index(\"bucket_time\")\n",
    "    # basic sanity: remove impossible values\n",
    "    out = out[(out[\"vol\"] > 0) & out[\"imb\"].notna() & out[\"vpin\"].notna()]\n",
    "    return out\n",
    "\n",
    "\n",
    "# ----------------------------- Pipeline -----------------------------\n",
    "def compute_vpin(cfg: Config) -> (pd.DataFrame, Dict):\n",
    "    session = build_http_session(cfg)\n",
    "\n",
    "    # 1) Load 1m bars\n",
    "    bars = load_minutes(session, cfg)\n",
    "\n",
    "    # 2) Signed volume proxy\n",
    "    bars = split_volume_signed(bars)\n",
    "\n",
    "    # 3) Bucket size from median 1m volume\n",
    "    med_vol = float(bars[\"volume\"].median())\n",
    "    if not np.isfinite(med_vol) or med_vol <= 0:\n",
    "        raise RuntimeError(\"Median volume is invalid. Data may be too sparse.\")\n",
    "    bucket_vol = cfg.bucket_mult * med_vol\n",
    "\n",
    "    # 4) Build buckets + VPIN\n",
    "    buckets = build_volume_buckets(bars, bucket_vol, cfg.m)\n",
    "\n",
    "    # 5) Toxic flag via quantile threshold\n",
    "    q_cut = float(buckets[\"vpin\"].quantile(cfg.toxic_q))\n",
    "    buckets[\"toxic_flag\"] = (buckets[\"vpin\"] >= q_cut).astype(int)\n",
    "\n",
    "    # 6) Metrics\n",
    "    metrics = {\n",
    "        \"config\": asdict(cfg),\n",
    "        \"symbol\": cfg.symbol,\n",
    "        \"interval\": cfg.interval,\n",
    "        \"median_1m_vol\": med_vol,\n",
    "        \"bucket_vol\": bucket_vol,\n",
    "        \"buckets\": int(buckets.shape[0]),\n",
    "        \"vpin_mean\": float(buckets[\"vpin\"].mean()),\n",
    "        \"vpin_p50\": float(buckets[\"vpin\"].quantile(0.50)),\n",
    "        \"vpin_p80\": float(buckets[\"vpin\"].quantile(0.80)),\n",
    "        \"vpin_p90\": float(buckets[\"vpin\"].quantile(0.90)),\n",
    "        \"toxic_quantile_cut\": q_cut,\n",
    "        \"toxic_share_pct\": float(100.0 * buckets[\"toxic_flag\"].mean()),\n",
    "    }\n",
    "    return buckets, metrics\n",
    "\n",
    "\n",
    "# ----------------------------- I/O -----------------------------\n",
    "def save_outputs(buckets: pd.DataFrame, metrics: Dict, cfg: Config):\n",
    "    # Ensure directory exists\n",
    "    os.makedirs(os.path.dirname(cfg.csv_path) or \".\", exist_ok=True)\n",
    "    os.makedirs(os.path.dirname(cfg.json_path) or \".\", exist_ok=True)\n",
    "\n",
    "    # CSV (append-safe: dedupe on index)\n",
    "    if os.path.exists(cfg.csv_path):\n",
    "        prev = pd.read_csv(cfg.csv_path, parse_dates=[\"bucket_time\"]).set_index(\"bucket_time\")\n",
    "        out = pd.concat([prev, buckets], axis=0)\n",
    "        out = out[~out.index.duplicated(keep=\"last\")].sort_index()\n",
    "    else:\n",
    "        out = buckets.sort_index()\n",
    "    out.to_csv(cfg.csv_path, index=True)\n",
    "\n",
    "    # JSON metrics\n",
    "    with open(cfg.json_path, \"w\") as f:\n",
    "        json.dump(metrics, f, indent=2)\n",
    "\n",
    "    print(f\"[OK] Saved buckets → {cfg.csv_path}\")\n",
    "    print(f\"[OK] Saved metrics → {cfg.json_path}\")\n",
    "    print(\"Metrics summary:\", {k: (round(v, 4) if isinstance(v, (int, float)) else v) for k, v in metrics.items()})\n",
    "\n",
    "\n",
    "# ----------------------------- CLI -----------------------------\n",
    "def parse_args() -> Config:\n",
    "    p = argparse.ArgumentParser(description=\"Level-42 VPIN (Flow Toxicity) — Binance 1m klines\")\n",
    "    p.add_argument(\"--symbol\", type=str, default=\"BTCUSDT\", help=\"Binance spot symbol (e.g., BTCUSDT)\")\n",
    "    p.add_argument(\"--days\", type=int, default=3, help=\"How many recent days of 1m bars to fetch\")\n",
    "    p.add_argument(\"--bucket-mult\", type=float, default=3.0, help=\"Bucket size multiplier on median 1m volume\")\n",
    "    p.add_argument(\"--m\", type=int, default=50, help=\"Smoothing window in buckets for VPIN\")\n",
    "    p.add_argument(\"--toxic-q\", type=float, default=0.90, help=\"Quantile for toxic flag (0-1)\")\n",
    "    p.add_argument(\"--csv\", type=str, default=\"level42_vpin_buckets.csv\", help=\"Output CSV path\")\n",
    "    p.add_argument(\"--json\", type=str, default=\"level42_vpin_metrics.json\", help=\"Output JSON path\")\n",
    "    p.add_argument(\"--interval\", type=str, default=\"1m\", help=\"Binance kline interval (default 1m)\")\n",
    "    p.add_argument(\"--base-url\", type=str, default=\"https://api.binance.com\", help=\"Binance REST base URL\")\n",
    "    p.add_argument(\"--timeout\", type=float, default=10.0, help=\"HTTP timeout seconds\")\n",
    "    p.add_argument(\"--no-verify-ssl\", action=\"store_true\", help=\"Disable SSL cert verification (not recommended)\")\n",
    "\n",
    "    args = p.parse_args()\n",
    "    cfg = Config(\n",
    "        symbol=args.symbol,\n",
    "        interval=args.interval,\n",
    "        days=args.days,\n",
    "        bucket_mult=args.bucket_mult,\n",
    "        m=args.m,\n",
    "        toxic_q=args.toxic_q,\n",
    "        base_url=args.base_url,\n",
    "        csv_path=args.csv,\n",
    "        json_path=args.json,\n",
    "        timeout=args.timeout,\n",
    "        verify_ssl=not args.no_verify_ssl,\n",
    "    )\n",
    "    return cfg\n",
    "\n",
    "\n",
    "# ----------------------------- Main -----------------------------\n",
    "def main():\n",
    "    cfg = parse_args()\n",
    "    buckets, metrics = compute_vpin(cfg)\n",
    "    save_outputs(buckets, metrics, cfg)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--symbol SYMBOL] [--days DAYS]\n",
      "                             [--bucket-mult BUCKET_MULT] [--m M]\n",
      "                             [--toxic-q TOXIC_Q] [--csv CSV] [--json JSON]\n",
      "                             [--interval INTERVAL] [--base-url BASE_URL]\n",
      "                             [--timeout TIMEOUT] [--no-verify-ssl]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f C:\\Users\\adity\\AppData\\Roaming\\jupyter\\runtime\\kernel-0d441352-f13f-4320-bfd8-3e818416b5b1.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001B[31mSystemExit\u001B[39m\u001B[31m:\u001B[39m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\PyCharmMiscProject\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3678: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "execution_count": 1
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
