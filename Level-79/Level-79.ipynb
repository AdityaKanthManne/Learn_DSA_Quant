{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-12-17T02:58:12.688482Z"
    }
   },
   "source": [
    "# level79_tcopula_var_es.py\n",
    "# Level-79: t-Copula Tail-Risk Simulator (VaR/ES) using ONLY free data + numpy/pandas/yfinance.\n",
    "#\n",
    "# What this script does:\n",
    "# 1) Downloads daily adjusted close prices for a basket of ETFs.\n",
    "# 2) Computes daily log returns.\n",
    "# 3) Fits (calibrates) a t-copula degrees-of-freedom (nu) over a grid using pseudo log-likelihood.\n",
    "# 4) Simulates joint return scenarios via the fitted t-copula + empirical marginals.\n",
    "# 5) Computes portfolio PnL distribution and VaR/ES at chosen alpha(s).\n",
    "# 6) Saves a daily panel CSV + summary JSON.\n",
    "#\n",
    "# Key Fix Included:\n",
    "# - Uses np.trapezoid if available; otherwise falls back to np.trapz.\n",
    "#   (Prevents: AttributeError: module 'numpy' has no attribute 'trapzoid')\n",
    "#\n",
    "# Usage:\n",
    "#   python level79_tcopula_var_es.py\n",
    "#   python level79_tcopula_var_es.py --start 2010-01-01 --sims 50000 --alphas 0.95 0.99\n",
    "#   python level79_tcopula_var_es.py --weights 0.25 0.25 0.25 0.25 0 0 0 0\n",
    "#\n",
    "# Notes:\n",
    "# - This is intentionally scipy-free. Numerical integration + root find are implemented here.\n",
    "# - It will run slower than scipy-based code (especially PPF), but it is portable.\n",
    "\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "import argparse\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "\n",
    "\n",
    "# ----------------------------- Config -----------------------------\n",
    "@dataclass\n",
    "class Config:\n",
    "    symbols: Tuple[str, ...] = (\"SPY\", \"QQQ\", \"IWM\", \"EFA\", \"EEM\", \"TLT\", \"LQD\", \"GLD\")\n",
    "    start: str = \"2010-01-01\"\n",
    "    nu_grid: Tuple[int, ...] = (4, 6, 8, 10, 15, 20)\n",
    "    corr_shrink: float = 0.05  # shrink corr toward identity to ensure PD\n",
    "    sims: int = 50000\n",
    "    seed: int = 42\n",
    "\n",
    "    # VaR/ES settings (confidence levels)\n",
    "    alphas: Tuple[float, ...] = (0.95, 0.99)\n",
    "\n",
    "    # Output\n",
    "    out_csv: str = \"level79_tcopula_var_es.csv\"\n",
    "    out_json: str = \"level79_tcopula_var_es_summary.json\"\n",
    "\n",
    "    # Portfolio settings\n",
    "    weights: Optional[List[float]] = None  # if None -> equal weight\n",
    "    notional: float = 1_000_000.0          # portfolio notional for PnL in $\n",
    "    horizon_days: int = 1                  # 1-day VaR/ES\n",
    "\n",
    "\n",
    "# ----------------------------- Utilities -----------------------------\n",
    "def trapz_compat(y: np.ndarray, x: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    NumPy >= 2.0 prefers trapezoid; older versions have trapz.\n",
    "    Also avoids the user's typo `trapzoid`.\n",
    "    \"\"\"\n",
    "    if hasattr(np, \"trapezoid\"):\n",
    "        return float(np.trapezoid(y, x))\n",
    "    return float(np.trapz(y, x))\n",
    "\n",
    "\n",
    "def ensure_pos_def_corr(corr: np.ndarray, shrink: float = 0.05) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Shrink correlation matrix toward identity and ensure positive definiteness\n",
    "    via eigenvalue clipping.\n",
    "    \"\"\"\n",
    "    n = corr.shape[0]\n",
    "    corr = (1.0 - shrink) * corr + shrink * np.eye(n)\n",
    "    corr = 0.5 * (corr + corr.T)\n",
    "\n",
    "    # Eigenvalue clip\n",
    "    vals, vecs = np.linalg.eigh(corr)\n",
    "    vals = np.clip(vals, 1e-8, None)\n",
    "    corr_pd = vecs @ np.diag(vals) @ vecs.T\n",
    "\n",
    "    # Renormalize to correlation\n",
    "    d = np.sqrt(np.diag(corr_pd))\n",
    "    corr_pd = corr_pd / np.outer(d, d)\n",
    "    corr_pd = np.clip(corr_pd, -0.9999, 0.9999)\n",
    "    np.fill_diagonal(corr_pd, 1.0)\n",
    "    return corr_pd\n",
    "\n",
    "\n",
    "def rank_to_uniform(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Convert data to pseudo-uniforms using rank/(n+1) per column.\n",
    "    \"\"\"\n",
    "    n = x.shape[0]\n",
    "    u = np.empty_like(x, dtype=float)\n",
    "    for j in range(x.shape[1]):\n",
    "        order = np.argsort(x[:, j])\n",
    "        ranks = np.empty(n, dtype=float)\n",
    "        ranks[order] = np.arange(1, n + 1, dtype=float)\n",
    "        u[:, j] = ranks / (n + 1.0)\n",
    "    # keep away from exact 0/1\n",
    "    eps = 1e-12\n",
    "    return np.clip(u, eps, 1.0 - eps)\n",
    "\n",
    "\n",
    "# ----------------------------- Data -----------------------------\n",
    "def load_prices(symbols: Tuple[str, ...], start: str) -> pd.DataFrame:\n",
    "    frames = []\n",
    "    for s in symbols:\n",
    "        px = yf.download(s, start=start, auto_adjust=True, progress=False)\n",
    "        if px.empty:\n",
    "            raise RuntimeError(f\"No data returned for symbol: {s}\")\n",
    "        if \"Close\" not in px.columns:\n",
    "            raise RuntimeError(f\"'Close' column missing for {s}\")\n",
    "        close = px[\"Close\"].copy()\n",
    "        close.name = s\n",
    "        frames.append(close)\n",
    "    prices = pd.concat(frames, axis=1).sort_index()\n",
    "    prices = prices.dropna(how=\"any\")\n",
    "    return prices\n",
    "\n",
    "\n",
    "def compute_log_returns(prices: pd.DataFrame) -> pd.DataFrame:\n",
    "    rets = np.log(prices).diff().dropna()\n",
    "    rets = rets.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "    return rets\n",
    "\n",
    "\n",
    "# ----------------------------- Student-t PDF/CDF/PPF (scipy-free) -----------------------------\n",
    "def t_log_pdf(x: np.ndarray, nu: float) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    log pdf of Student-t with df=nu, location 0, scale 1.\n",
    "    \"\"\"\n",
    "    # log Γ((nu+1)/2) - log Γ(nu/2) - 0.5 log(nuπ) - ((nu+1)/2) log(1 + x^2/nu)\n",
    "    a = math.lgamma((nu + 1.0) / 2.0) - math.lgamma(nu / 2.0)\n",
    "    b = -0.5 * math.log(nu * math.pi)\n",
    "    c = -((nu + 1.0) / 2.0) * np.log1p((x * x) / nu)\n",
    "    return a + b + c\n",
    "\n",
    "\n",
    "def t_cdf_scalar(x: float, nu: float, n_steps: int = 4001) -> float:\n",
    "    \"\"\"\n",
    "    Numerically integrate t pdf from 0 to |x| and use symmetry:\n",
    "      CDF(x) = 0.5 + sign(x) * ∫_0^{|x|} f(t) dt\n",
    "    \"\"\"\n",
    "    if x == 0.0:\n",
    "        return 0.5\n",
    "    sign = 1.0 if x > 0 else -1.0\n",
    "    ax = abs(x)\n",
    "\n",
    "    # integrate over [0, ax] with an odd number of steps for smoother trapezoid\n",
    "    xs = np.linspace(0.0, ax, int(n_steps))\n",
    "    logf = t_log_pdf(xs, nu)\n",
    "    f = np.exp(logf)\n",
    "\n",
    "    area = trapz_compat(f, xs)\n",
    "    cdf = 0.5 + sign * area\n",
    "    # numerical safety\n",
    "    return float(np.clip(cdf, 1e-12, 1.0 - 1e-12))\n",
    "\n",
    "\n",
    "def t_ppf_scalar(u: float, nu: float) -> float:\n",
    "    \"\"\"\n",
    "    Inverse CDF via bisection using the numeric CDF above.\n",
    "    Works for u in (0,1).\n",
    "    \"\"\"\n",
    "    u = float(np.clip(u, 1e-12, 1.0 - 1e-12))\n",
    "    if u == 0.5:\n",
    "        return 0.0\n",
    "\n",
    "    # symmetry\n",
    "    if u < 0.5:\n",
    "        return -t_ppf_scalar(1.0 - u, nu)\n",
    "\n",
    "    target = u\n",
    "    lo, hi = 0.0, 10.0\n",
    "\n",
    "    # widen hi until cdf(hi) >= target\n",
    "    while t_cdf_scalar(hi, nu) < target:\n",
    "        hi *= 2.0\n",
    "        if hi > 200.0:\n",
    "            break\n",
    "\n",
    "    # bisection\n",
    "    for _ in range(80):\n",
    "        mid = 0.5 * (lo + hi)\n",
    "        cmid = t_cdf_scalar(mid, nu)\n",
    "        if cmid < target:\n",
    "            lo = mid\n",
    "        else:\n",
    "            hi = mid\n",
    "    return 0.5 * (lo + hi)\n",
    "\n",
    "\n",
    "def t_ppf(U: np.ndarray, nu: float) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Elementwise t-ppf. (Slow but portable; avoids scipy dependency.)\n",
    "    \"\"\"\n",
    "    out = np.empty_like(U, dtype=float)\n",
    "    it = np.nditer(U, flags=[\"multi_index\"])\n",
    "    while not it.finished:\n",
    "        out[it.multi_index] = t_ppf_scalar(float(it[0]), nu)\n",
    "        it.iternext()\n",
    "    return out\n",
    "\n",
    "\n",
    "# ----------------------------- t-Copula Calibration -----------------------------\n",
    "def pseudo_log_likelihood_tcopula(U: np.ndarray, corr: np.ndarray, nu: float) -> float:\n",
    "    \"\"\"\n",
    "    Pseudo log-likelihood for t-copula.\n",
    "    We transform U -> Z via t^{-1}_nu(U), then evaluate multivariate t copula density ratio.\n",
    "    For practicality, we compute:\n",
    "      pll = log c_t(Z; corr, nu) - sum log f_t(Z_i; nu)   (constants cancel in selection)\n",
    "    But we only need relative pll across nu grid.\n",
    "    \"\"\"\n",
    "    # Z: T x N\n",
    "    Z = t_ppf(U, nu)\n",
    "\n",
    "    # Cholesky for quadratic form\n",
    "    try:\n",
    "        L = np.linalg.cholesky(corr)\n",
    "    except np.linalg.LinAlgError:\n",
    "        corr = ensure_pos_def_corr(corr, shrink=0.10)\n",
    "        L = np.linalg.cholesky(corr)\n",
    "\n",
    "    # Solve y = L^{-1} z rowwise\n",
    "    # quadratic term: z^T corr^{-1} z = ||y||^2\n",
    "    Y = np.linalg.solve(L, Z.T).T  # T x N\n",
    "\n",
    "    q = np.sum(Y * Y, axis=1)  # length T\n",
    "    n = Z.shape[1]\n",
    "    # log multivariate t density up to constants:\n",
    "    # -0.5*log|corr| - (nu+n)/2 * log(1 + q/nu)\n",
    "    # minus sum of univariate t log-pdfs (which depend on each z_i)\n",
    "    logdet = 2.0 * np.sum(np.log(np.diag(L)))\n",
    "    mv_part = -0.5 * logdet - ((nu + n) / 2.0) * np.log1p(q / nu)\n",
    "\n",
    "    uni = t_log_pdf(Z, nu)  # T x N\n",
    "    uni_sum = np.sum(uni, axis=1)\n",
    "\n",
    "    pll = float(np.sum(mv_part - uni_sum))\n",
    "    return pll\n",
    "\n",
    "\n",
    "def calibrate_tcopula(rets: pd.DataFrame, nu_grid: Tuple[int, ...], corr_shrink: float) -> Dict:\n",
    "    X = rets.values\n",
    "    U = rank_to_uniform(X)\n",
    "\n",
    "    # Start with normal-score correlation proxy: use empirical corr of returns,\n",
    "    # then shrink & PD-fix. (We re-estimate corr each nu using Z; but this is initial baseline.)\n",
    "    base_corr = np.corrcoef(X, rowvar=False)\n",
    "    base_corr = ensure_pos_def_corr(base_corr, shrink=corr_shrink)\n",
    "\n",
    "    best = {\"nu\": None, \"pll\": -np.inf, \"corr\": None}\n",
    "\n",
    "    # For each nu:\n",
    "    for nu in nu_grid:\n",
    "        # Transform U to Z under this nu, then compute corr(Z) and PD-fix\n",
    "        Z = t_ppf(U, float(nu))\n",
    "        corr = np.corrcoef(Z, rowvar=False)\n",
    "        corr = ensure_pos_def_corr(corr, shrink=corr_shrink)\n",
    "\n",
    "        pll = pseudo_log_likelihood_tcopula(U, corr, float(nu))\n",
    "        if pll > best[\"pll\"]:\n",
    "            best = {\"nu\": int(nu), \"pll\": float(pll), \"corr\": corr}\n",
    "\n",
    "    return best\n",
    "\n",
    "\n",
    "# ----------------------------- Simulation -----------------------------\n",
    "def simulate_tcopula_returns(\n",
    "    hist_rets: pd.DataFrame,\n",
    "    corr: np.ndarray,\n",
    "    nu: float,\n",
    "    sims: int,\n",
    "    seed: int\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Simulate returns from fitted t-copula + empirical marginals.\n",
    "    Steps:\n",
    "    1) Sample multivariate t latent: Y = (L @ g) / sqrt(w/nu), g~N(0,I), w~ChiSq(nu)\n",
    "    2) Convert to uniforms via t-cdf (numeric) elementwise\n",
    "    3) Map uniforms to empirical marginal via quantiles of historical returns\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    n = hist_rets.shape[1]\n",
    "    L = np.linalg.cholesky(ensure_pos_def_corr(corr, shrink=0.0))\n",
    "\n",
    "    # Precompute sorted historical returns per asset for inverse-empirical-CDF\n",
    "    hist_sorted = np.sort(hist_rets.values, axis=0)\n",
    "    T = hist_sorted.shape[0]\n",
    "\n",
    "    # sample Gaussian\n",
    "    g = rng.standard_normal(size=(sims, n))\n",
    "    z = (g @ L.T)\n",
    "\n",
    "    # chi-square via gamma(k=nu/2, theta=2)\n",
    "    w = rng.gamma(shape=nu / 2.0, scale=2.0, size=(sims, 1))\n",
    "    y = z / np.sqrt(w / nu)\n",
    "\n",
    "    # convert each y_ij -> u_ij via t-cdf (numeric scalar, slow but deterministic)\n",
    "    U = np.empty_like(y, dtype=float)\n",
    "    it = np.nditer(y, flags=[\"multi_index\"])\n",
    "    while not it.finished:\n",
    "        U[it.multi_index] = t_cdf_scalar(float(it[0]), float(nu))\n",
    "        it.iternext()\n",
    "\n",
    "    # empirical inverse CDF: index = floor(u*(T-1))\n",
    "    idx = np.floor(U * (T - 1)).astype(int)\n",
    "    idx = np.clip(idx, 0, T - 1)\n",
    "\n",
    "    sim = np.empty_like(U, dtype=float)\n",
    "    for j in range(n):\n",
    "        sim[:, j] = hist_sorted[idx[:, j], j]\n",
    "\n",
    "    return sim  # sims x n\n",
    "\n",
    "\n",
    "# ----------------------------- Risk Metrics -----------------------------\n",
    "def var_es(pnl: np.ndarray, alpha: float) -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    pnl is array of PnL (positive = profit, negative = loss)\n",
    "    VaR_alpha as positive number (loss threshold) at confidence alpha\n",
    "    ES_alpha as positive number (expected loss beyond VaR)\n",
    "    \"\"\"\n",
    "    # Losses are -pnl\n",
    "    losses = -pnl\n",
    "    q = float(np.quantile(losses, alpha))\n",
    "    tail = losses[losses >= q]\n",
    "    es = float(np.mean(tail)) if tail.size else q\n",
    "    return q, es\n",
    "\n",
    "\n",
    "# ----------------------------- Pipeline -----------------------------\n",
    "def run_pipeline(cfg: Config) -> Tuple[pd.DataFrame, Dict]:\n",
    "    np.random.seed(cfg.seed)\n",
    "\n",
    "    print(f\"[INFO] Downloading prices for {cfg.symbols} from {cfg.start} ...\")\n",
    "    prices = load_prices(cfg.symbols, cfg.start)\n",
    "    rets = compute_log_returns(prices)\n",
    "    print(f\"[INFO] Got {len(prices)} price rows, {len(rets)} return rows, assets={rets.shape[1]}\")\n",
    "\n",
    "    # Calibrate nu + corr\n",
    "    print(f\"[INFO] Calibrating t-copula nu over grid: {cfg.nu_grid} ...\")\n",
    "    calib = calibrate_tcopula(rets, cfg.nu_grid, cfg.corr_shrink)\n",
    "    nu_hat = float(calib[\"nu\"])\n",
    "    corr_hat = calib[\"corr\"]\n",
    "    print(f\"[INFO] Best nu={int(nu_hat)} (pseudo-LL={calib['pll']:.2f})\")\n",
    "\n",
    "    # Portfolio weights\n",
    "    n = rets.shape[1]\n",
    "    if cfg.weights is None:\n",
    "        w = np.ones(n) / n\n",
    "    else:\n",
    "        w = np.asarray(cfg.weights, dtype=float)\n",
    "        if w.size != n:\n",
    "            raise ValueError(f\"--weights length must be {n}, got {w.size}\")\n",
    "        if not np.isfinite(w).all():\n",
    "            raise ValueError(\"weights contain non-finite values\")\n",
    "        s = float(np.sum(w))\n",
    "        if abs(s) < 1e-12:\n",
    "            raise ValueError(\"weights sum to zero\")\n",
    "        w = w / s\n",
    "\n",
    "    # Simulate horizon returns (1-day): use simulated daily returns\n",
    "    sim_rets = simulate_tcopula_returns(rets, corr_hat, nu_hat, cfg.sims, cfg.seed)\n",
    "\n",
    "    # Convert to portfolio PnL\n",
    "    # For small returns: pnl ≈ notional * (w · r)\n",
    "    port_r = sim_rets @ w\n",
    "    pnl = cfg.notional * port_r\n",
    "\n",
    "    # VaR/ES for each alpha\n",
    "    risk = {}\n",
    "    for a in cfg.alphas:\n",
    "        v, e = var_es(pnl, float(a))\n",
    "        risk[str(a)] = {\"VaR\": v, \"ES\": e}\n",
    "\n",
    "    # Output DataFrame: historical series + a “portfolio ret” column\n",
    "    out = pd.DataFrame(index=rets.index)\n",
    "    out[[f\"ret_{c}\" for c in rets.columns]] = rets.add_prefix(\"ret_\")\n",
    "    out[\"port_ret_hist\"] = rets.values @ w\n",
    "\n",
    "    summary = {\n",
    "        \"config\": asdict(cfg),\n",
    "        \"calibration\": {\n",
    "            \"nu\": int(nu_hat),\n",
    "            \"pseudo_ll\": float(calib[\"pll\"]),\n",
    "            \"corr_shrink\": float(cfg.corr_shrink),\n",
    "        },\n",
    "        \"portfolio\": {\n",
    "            \"symbols\": list(cfg.symbols),\n",
    "            \"weights\": [float(x) for x in w.tolist()],\n",
    "            \"notional\": float(cfg.notional),\n",
    "            \"horizon_days\": int(cfg.horizon_days),\n",
    "        },\n",
    "        \"risk\": risk,\n",
    "        \"data_window\": {\n",
    "            \"start\": str(rets.index.min().date()),\n",
    "            \"end\": str(rets.index.max().date()),\n",
    "            \"n_returns\": int(len(rets)),\n",
    "        },\n",
    "    }\n",
    "    return out, summary\n",
    "\n",
    "\n",
    "def save_outputs(out: pd.DataFrame, summary: Dict, cfg: Config) -> None:\n",
    "    os.makedirs(os.path.dirname(cfg.out_csv) or \".\", exist_ok=True)\n",
    "    os.makedirs(os.path.dirname(cfg.out_json) or \".\", exist_ok=True)\n",
    "\n",
    "    out.to_csv(cfg.out_csv)\n",
    "    with open(cfg.out_json, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "\n",
    "    print(f\"[OK] Saved daily panel → {cfg.out_csv}\")\n",
    "    print(f\"[OK] Saved summary → {cfg.out_json}\")\n",
    "    for a, d in summary[\"risk\"].items():\n",
    "        print(f\"alpha={a}: VaR=${d['VaR']:.2f}, ES=${d['ES']:.2f}\")\n",
    "\n",
    "\n",
    "# ----------------------------- CLI -----------------------------\n",
    "def parse_args() -> Config:\n",
    "    p = argparse.ArgumentParser(description=\"Level-79: t-Copula Tail Risk (VaR/ES) - SciPy-free\")\n",
    "\n",
    "    p.add_argument(\"--start\", type=str, default=\"2010-01-01\")\n",
    "    p.add_argument(\"--symbols\", nargs=\"+\", default=list(Config.symbols))\n",
    "    p.add_argument(\"--nu-grid\", nargs=\"+\", type=int, default=list(Config.nu_grid))\n",
    "    p.add_argument(\"--corr-shrink\", type=float, default=0.05)\n",
    "\n",
    "    p.add_argument(\"--sims\", type=int, default=50000)\n",
    "    p.add_argument(\"--seed\", type=int, default=42)\n",
    "\n",
    "    p.add_argument(\"--alphas\", nargs=\"+\", type=float, default=[0.95, 0.99])\n",
    "    p.add_argument(\"--weights\", nargs=\"+\", type=float, default=None)\n",
    "    p.add_argument(\"--notional\", type=float, default=1_000_000.0)\n",
    "\n",
    "    p.add_argument(\"--csv\", type=str, default=\"level79_tcopula_var_es.csv\")\n",
    "    p.add_argument(\"--json\", type=str, default=\"level79_tcopula_var_es_summary.json\")\n",
    "\n",
    "    a = p.parse_args()\n",
    "\n",
    "    return Config(\n",
    "        symbols=tuple(a.symbols),\n",
    "        start=a.start,\n",
    "        nu_grid=tuple(a.nu_grid),\n",
    "        corr_shrink=float(a.corr_shrink),\n",
    "        sims=int(a.sims),\n",
    "        seed=int(a.seed),\n",
    "        alphas=tuple(a.alphas),\n",
    "        weights=None if a.weights is None else list(a.weights),\n",
    "        notional=float(a.notional),\n",
    "        out_csv=a.csv,\n",
    "        out_json=a.json,\n",
    "    )\n",
    "\n",
    "\n",
    "def main() -> None:\n",
    "    cfg = parse_args()\n",
    "    out, summary = run_pipeline(cfg)\n",
    "    save_outputs(out, summary, cfg)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Jupyter/PyCharm cell shim: strip \"-f kernel.json\" etc.\n",
    "    import sys\n",
    "    sys.argv = [sys.argv[0]] + [\n",
    "        arg for arg in sys.argv[1:]\n",
    "        if arg != \"-f\" and not (arg.endswith(\".json\") and \"kernel\" in arg)\n",
    "    ]\n",
    "    main()\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Downloading prices for ('SPY', 'QQQ', 'IWM', 'EFA', 'EEM', 'TLT', 'LQD', 'GLD') from 2010-01-01 ...\n",
      "[INFO] Got 4013 price rows, 4012 return rows, assets=8\n",
      "[INFO] Calibrating t-copula nu over grid: (4, 6, 8, 10, 15, 20) ...\n"
     ]
    }
   ],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
