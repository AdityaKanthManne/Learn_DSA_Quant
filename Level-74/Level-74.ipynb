{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-05T03:57:51.277403Z",
     "start_time": "2025-12-05T03:57:46.196901Z"
    }
   },
   "source": [
    "# level74_hrp_allocation.py\n",
    "# Hierarchical Risk Parity (HRP) multi-asset allocation with monthly rebalancing.\n",
    "#\n",
    "# - Universe: SPY, QQQ, IWM, EFA, EEM, TLT, LQD, GLD (configurable)\n",
    "# - Download daily prices from yfinance (auto-adjusted).\n",
    "# - Compute daily log returns.\n",
    "# - Each rebalance date (monthly), estimate covariance on a rolling window\n",
    "#   and compute HRP weights.\n",
    "# - Forward-fill weights between rebalances, compute portfolio returns and equity curve.\n",
    "# - Compare vs equal-weight benchmark.\n",
    "#\n",
    "# Outputs:\n",
    "#   - level74_hrp_allocation.csv\n",
    "#   - level74_hrp_allocation_summary.json\n",
    "#\n",
    "# Usage:\n",
    "#   python level74_hrp_allocation.py --lookback 252 --rebalance-freq ME\n",
    "#\n",
    "# Notes:\n",
    "#   - Requires numpy, pandas, yfinance.\n",
    "#   - SciPy is optional; if not installed, falls back to simple inverse-variance weights.\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import Sequence, Tuple, Dict, List, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "\n",
    "try:\n",
    "    from scipy.cluster.hierarchy import linkage, leaves_list\n",
    "    SCIPY_AVAILABLE = True\n",
    "except ImportError:\n",
    "    linkage = None\n",
    "    leaves_list = None\n",
    "    SCIPY_AVAILABLE = False\n",
    "\n",
    "\n",
    "# --------------------------- Config ---------------------------\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    symbols: Tuple[str, ...] = (\n",
    "        \"SPY\", \"QQQ\", \"IWM\", \"EFA\", \"EEM\", \"TLT\", \"LQD\", \"GLD\"\n",
    "    )\n",
    "    start: str = \"2010-01-01\"\n",
    "\n",
    "    # HRP settings\n",
    "    lookback: int = 252            # days in covariance window\n",
    "    rebalance_freq: str = \"ME\"     # pandas offset alias (ME = month end, MS = month start)\n",
    "\n",
    "    # Outputs\n",
    "    out_csv: str = \"level74_hrp_allocation.csv\"\n",
    "    out_json: str = \"level74_hrp_allocation_summary.json\"\n",
    "\n",
    "    # Misc\n",
    "    seed: int = 42\n",
    "\n",
    "\n",
    "# --------------------------- Data Loader ---------------------------\n",
    "\n",
    "def _extract_close_series(px: pd.DataFrame, symbol: str) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Robustly extract a 1D close price Series for a symbol from a yfinance DataFrame.\n",
    "\n",
    "    Handles cases where:\n",
    "      - px[\"Close\"] is a Series\n",
    "      - px[\"Close\"] is a DataFrame with shape (n, 1)\n",
    "    \"\"\"\n",
    "    if \"Close\" not in px.columns:\n",
    "        raise RuntimeError(f\"'Close' column missing for {symbol}.\")\n",
    "\n",
    "    close_obj = px[\"Close\"]\n",
    "\n",
    "    if isinstance(close_obj, pd.Series):\n",
    "        close = pd.Series(close_obj.values, index=close_obj.index, name=symbol)\n",
    "    elif isinstance(close_obj, pd.DataFrame):\n",
    "        if close_obj.shape[1] < 1:\n",
    "            raise RuntimeError(f\"No close data columns for {symbol}.\")\n",
    "        col0 = close_obj.iloc[:, 0]\n",
    "        close = pd.Series(col0.values, index=col0.index, name=symbol)\n",
    "    else:\n",
    "        raise RuntimeError(\"Unexpected type for Close data.\")\n",
    "\n",
    "    close = close.astype(float)\n",
    "    return close\n",
    "\n",
    "\n",
    "def load_prices(symbols: Sequence[str], start: str) -> pd.DataFrame:\n",
    "    \"\"\"Download adjusted close prices for a list of symbols from yfinance.\"\"\"\n",
    "    frames = []\n",
    "    for s in symbols:\n",
    "        px = yf.download(s, start=start, auto_adjust=True, progress=False)\n",
    "        if px.empty:\n",
    "            raise RuntimeError(f\"No price data downloaded for {s}.\")\n",
    "        close = _extract_close_series(px, s)\n",
    "        frames.append(close)\n",
    "\n",
    "    prices = pd.concat(frames, axis=1).sort_index()\n",
    "    prices = prices.dropna(how=\"all\")\n",
    "    prices = prices.ffill().dropna(how=\"any\")\n",
    "    return prices\n",
    "\n",
    "\n",
    "def compute_returns(prices: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Daily log returns.\"\"\"\n",
    "    rets = np.log(prices).diff()\n",
    "    rets = rets.dropna(how=\"all\")\n",
    "    return rets\n",
    "\n",
    "\n",
    "# --------------------------- HRP Core ---------------------------\n",
    "\n",
    "def cov_to_corr(cov: pd.DataFrame) -> pd.DataFrame:\n",
    "    diag = np.sqrt(np.diag(cov.values))\n",
    "    diag[diag == 0] = 1.0\n",
    "    corr = cov.values / np.outer(diag, diag)\n",
    "    corr = np.clip(corr, -1.0, 1.0)\n",
    "    return pd.DataFrame(corr, index=cov.index, columns=cov.columns)\n",
    "\n",
    "\n",
    "def correlation_distance(corr: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Lopez de Prado's correlation distance:\n",
    "        d_ij = sqrt(0.5 * (1 - corr_ij))\n",
    "    \"\"\"\n",
    "    dist = np.sqrt(0.5 * (1.0 - corr.values))\n",
    "    return pd.DataFrame(dist, index=corr.index, columns=corr.columns)\n",
    "\n",
    "\n",
    "def _cluster_order(cov: pd.DataFrame) -> List[str]:\n",
    "    \"\"\"\n",
    "    Get the hierarchical clustering order of assets based on covariance.\n",
    "    If SciPy is unavailable, return the original order.\n",
    "    \"\"\"\n",
    "    if not SCIPY_AVAILABLE:\n",
    "        # Fallback: no clustering, just keep original order\n",
    "        return list(cov.index)\n",
    "\n",
    "    corr = cov_to_corr(cov)\n",
    "    dist = correlation_distance(corr)\n",
    "\n",
    "    # Use condensed distance matrix for linkage\n",
    "    tri_upper = dist.values[np.triu_indices(len(dist), k=1)]\n",
    "    Z = linkage(tri_upper, method=\"single\")\n",
    "    order_idx = leaves_list(Z)\n",
    "    assets = list(cov.index)\n",
    "    ordered = [assets[i] for i in order_idx]\n",
    "    return ordered\n",
    "\n",
    "\n",
    "def _cluster_variance(cov: pd.DataFrame, cluster: List[str]) -> float:\n",
    "    \"\"\"Compute the variance of a cluster using inverse-variance weights.\"\"\"\n",
    "    sub = cov.loc[cluster, cluster]\n",
    "    iv = 1.0 / np.diag(sub.values)\n",
    "    iv = iv / iv.sum()\n",
    "    w = iv.reshape(-1, 1)\n",
    "    # result is 1x1 array → extract scalar explicitly to avoid NumPy deprecation\n",
    "    var_mat = np.dot(np.dot(w.T, sub.values), w)\n",
    "    var = float(var_mat.item())\n",
    "    return var\n",
    "\n",
    "\n",
    "def hrp_weights(cov: pd.DataFrame) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Hierarchical Risk Parity weights from a covariance matrix.\n",
    "\n",
    "    If SciPy is not installed, we fall back to inverse-variance weights.\n",
    "    \"\"\"\n",
    "    assets = list(cov.index)\n",
    "\n",
    "    if len(assets) == 0:\n",
    "        raise ValueError(\"Empty covariance matrix.\")\n",
    "\n",
    "    # Fallback: no SciPy\n",
    "    if not SCIPY_AVAILABLE:\n",
    "        var = np.diag(cov.values)\n",
    "        var[var <= 0] = 1e-6\n",
    "        inv_var = 1.0 / var\n",
    "        w = inv_var / inv_var.sum()\n",
    "        return pd.Series(w, index=assets, name=\"hrp_weight\")\n",
    "\n",
    "    # 1) Get hierarchical order\n",
    "    ordered = _cluster_order(cov)\n",
    "    # 2) Recursive bisection\n",
    "    w = pd.Series(1.0, index=ordered)\n",
    "    clusters = [ordered]\n",
    "\n",
    "    while len(clusters) > 0:\n",
    "        cluster = clusters.pop(0)\n",
    "        if len(cluster) <= 1:\n",
    "            continue\n",
    "\n",
    "        # split cluster into two halves\n",
    "        split = len(cluster) // 2\n",
    "        left = cluster[:split]\n",
    "        right = cluster[split:]\n",
    "\n",
    "        var_left = _cluster_variance(cov, left)\n",
    "        var_right = _cluster_variance(cov, right)\n",
    "\n",
    "        if var_left + var_right == 0:\n",
    "            alpha = 0.5\n",
    "        else:\n",
    "            # Allocate more weight to lower-variance cluster\n",
    "            alpha = 1.0 - var_left / (var_left + var_right)\n",
    "\n",
    "        for i in left:\n",
    "            w[i] *= alpha\n",
    "        for j in right:\n",
    "            w[j] *= (1.0 - alpha)\n",
    "\n",
    "        clusters.append(left)\n",
    "        clusters.append(right)\n",
    "\n",
    "    w = w / w.sum()\n",
    "    w.name = \"hrp_weight\"\n",
    "    return w\n",
    "\n",
    "\n",
    "# --------------------------- Backtest Logic ---------------------------\n",
    "\n",
    "def build_hrp_backtest(rets: pd.DataFrame, cfg: Config) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    For each rebalance date:\n",
    "      - Use past `cfg.lookback` days of returns to estimate covariance.\n",
    "      - Compute HRP weights.\n",
    "      - Apply weights from next day until the next rebalance date.\n",
    "    \"\"\"\n",
    "    rets = rets.dropna(how=\"all\")\n",
    "    idx = rets.index\n",
    "    symbols = list(rets.columns)\n",
    "\n",
    "    # Rebalance dates (e.g., month-end)\n",
    "    rebal_dates = rets.resample(cfg.rebalance_freq).last().index\n",
    "    rebal_dates = rebal_dates[rebal_dates.isin(idx)]\n",
    "\n",
    "    weights_df = pd.DataFrame(index=idx, columns=symbols, dtype=float)\n",
    "\n",
    "    last_weights: Optional[pd.Series] = None\n",
    "    for d in rebal_dates:\n",
    "        window = rets.loc[:d].tail(cfg.lookback)\n",
    "        if window.shape[0] < cfg.lookback:\n",
    "            continue\n",
    "\n",
    "        cov = window.cov()\n",
    "        w_hrp = hrp_weights(cov)\n",
    "\n",
    "        weights_df.loc[d, w_hrp.index] = w_hrp.values\n",
    "        last_weights = w_hrp\n",
    "\n",
    "    # Forward-fill weights\n",
    "    if last_weights is not None:\n",
    "        weights_df = weights_df.ffill()\n",
    "\n",
    "    # Drop days before first actual weight\n",
    "    first_non_na = weights_df.dropna(how=\"all\").index.min()\n",
    "    if first_non_na is None:\n",
    "        raise RuntimeError(\"No HRP weights were computed (not enough history / bad data).\")\n",
    "\n",
    "    weights_df = weights_df.loc[first_non_na:]\n",
    "    rets = rets.loc[first_non_na:]\n",
    "\n",
    "    # Normalize weights daily (just in case of numerical drift)\n",
    "    weights_df = weights_df.div(weights_df.abs().sum(axis=1), axis=0).fillna(0.0)\n",
    "\n",
    "    # Portfolio returns\n",
    "    port_ret_hrp = (weights_df * rets).sum(axis=1)\n",
    "    port_ret_hrp.name = \"ret_hrp\"\n",
    "\n",
    "    # Equal-weight benchmark\n",
    "    eq_w = np.ones(len(symbols)) / len(symbols)\n",
    "    port_ret_eq = (rets * eq_w).sum(axis=1)\n",
    "    port_ret_eq.name = \"ret_eqw\"\n",
    "\n",
    "    out = pd.DataFrame(\n",
    "        {\n",
    "            \"ret_hrp\": port_ret_hrp,\n",
    "            \"ret_eqw\": port_ret_eq,\n",
    "        },\n",
    "        index=rets.index,\n",
    "    )\n",
    "\n",
    "    # Attach weights as separate columns\n",
    "    for s in symbols:\n",
    "        out[f\"w_hrp_{s}\"] = weights_df[s]\n",
    "\n",
    "    # Equity curves\n",
    "    out[\"eq_hrp\"] = (1.0 + out[\"ret_hrp\"]).cumprod()\n",
    "    out[\"eq_eqw\"] = (1.0 + out[\"ret_eqw\"]).cumprod()\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "# --------------------------- Performance Stats ---------------------------\n",
    "\n",
    "def stats_from_returns(r: pd.Series) -> dict:\n",
    "    r = r.dropna()\n",
    "    if r.empty:\n",
    "        return dict(ann_ret=np.nan, ann_vol=np.nan, sharpe=np.nan, max_dd=np.nan)\n",
    "\n",
    "    mu = float(r.mean())\n",
    "    sig = float(r.std())\n",
    "\n",
    "    ann_ret = (1.0 + mu) ** 252 - 1.0\n",
    "    ann_vol = sig * np.sqrt(252.0)\n",
    "    sharpe = ann_ret / ann_vol if ann_vol > 0 else np.nan\n",
    "\n",
    "    eq = (1.0 + r).cumprod()\n",
    "    peak = eq.cummax()\n",
    "    dd = eq / peak - 1.0\n",
    "    max_dd = float(dd.min()) if not dd.empty else np.nan\n",
    "\n",
    "    return dict(\n",
    "        ann_ret=float(ann_ret),\n",
    "        ann_vol=float(ann_vol),\n",
    "        sharpe=float(sharpe),\n",
    "        max_dd=float(max_dd),\n",
    "    )\n",
    "\n",
    "\n",
    "# --------------------------- Pipeline ---------------------------\n",
    "\n",
    "def run_pipeline(cfg: Config):\n",
    "    prices = load_prices(cfg.symbols, cfg.start)\n",
    "    rets = compute_returns(prices)\n",
    "\n",
    "    out = build_hrp_backtest(rets, cfg)\n",
    "\n",
    "    hrp_stats = stats_from_returns(out[\"ret_hrp\"])\n",
    "    eqw_stats = stats_from_returns(out[\"ret_eqw\"])\n",
    "\n",
    "    idx_all = out.index\n",
    "    summary = {\n",
    "        \"config\": asdict(cfg),\n",
    "        \"start_date\": str(idx_all.min().date()) if len(idx_all) else None,\n",
    "        \"end_date\": str(idx_all.max().date()) if len(idx_all) else None,\n",
    "        \"n_days\": int(len(idx_all)),\n",
    "        \"Performance_HRP\": hrp_stats,\n",
    "        \"Performance_EqualWeight\": eqw_stats,\n",
    "        \"scipy_available\": SCIPY_AVAILABLE,\n",
    "    }\n",
    "\n",
    "    return out, summary, prices\n",
    "\n",
    "\n",
    "# --------------------------- I/O ---------------------------\n",
    "\n",
    "def save_outputs(out: pd.DataFrame, summary: dict, prices: pd.DataFrame, cfg: Config) -> None:\n",
    "    # Merge prices in output for convenience\n",
    "    merged = out.copy()\n",
    "    p = prices.reindex(merged.index)\n",
    "    for s in cfg.symbols:\n",
    "        merged[s] = p[s]\n",
    "\n",
    "    merged.to_csv(cfg.out_csv, index=True, date_format=\"%Y-%m-%d\")\n",
    "    with open(cfg.out_json, \"w\") as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "\n",
    "    print(f\"[OK] Saved daily series → {cfg.out_csv}\")\n",
    "    print(f\"[OK] Saved summary      → {cfg.out_json}\")\n",
    "    if summary[\"start_date\"] and summary[\"end_date\"]:\n",
    "        print(\n",
    "            f\"Period {summary['start_date']} → {summary['end_date']}, \"\n",
    "            f\"n_days={summary['n_days']}\"\n",
    "        )\n",
    "\n",
    "    hrp = summary[\"Performance_HRP\"]\n",
    "    eqw = summary[\"Performance_EqualWeight\"]\n",
    "\n",
    "    print(\n",
    "        \"HRP portfolio:   \"\n",
    "        f\"AnnRet={hrp['ann_ret']*100:.2f}%, \"\n",
    "        f\"AnnVol={hrp['ann_vol']*100:.2f}%, \"\n",
    "        f\"Sharpe={hrp['sharpe']:.2f}, \"\n",
    "        f\"MaxDD={hrp['max_dd']*100:.2f}%\"\n",
    "    )\n",
    "    print(\n",
    "        \"Equal-weight:    \"\n",
    "        f\"AnnRet={eqw['ann_ret']*100:.2f}%, \"\n",
    "        f\"AnnVol={eqw['ann_vol']*100:.2f}%, \"\n",
    "        f\"Sharpe={eqw['sharpe']:.2f}, \"\n",
    "        f\"MaxDD={eqw['max_dd']*100:.2f}%\"\n",
    "    )\n",
    "    if not SCIPY_AVAILABLE:\n",
    "        print(\"NOTE: SciPy not available; HRP fell back to inverse-variance weights.\")\n",
    "\n",
    "\n",
    "# --------------------------- CLI ---------------------------\n",
    "\n",
    "def parse_args() -> Config:\n",
    "    p = argparse.ArgumentParser(\n",
    "        description=\"Level-74: Hierarchical Risk Parity Multi-Asset Allocation\"\n",
    "    )\n",
    "    p.add_argument(\n",
    "        \"--symbols\",\n",
    "        type=str,\n",
    "        default=\"SPY,QQQ,IWM,EFA,EEM,TLT,LQD,GLD\",\n",
    "        help=\"Comma-separated tickers.\",\n",
    "    )\n",
    "    p.add_argument(\"--start\", type=str, default=\"2010-01-01\")\n",
    "    p.add_argument(\"--lookback\", type=int, default=252)\n",
    "    p.add_argument(\n",
    "        \"--rebalance-freq\",\n",
    "        type=str,\n",
    "        default=\"ME\",\n",
    "        help=\"Pandas offset alias for rebalancing (e.g., ME=month end, MS=month start, W-FRI, etc.)\",\n",
    "    )\n",
    "    p.add_argument(\"--csv\", type=str, default=\"level74_hrp_allocation.csv\")\n",
    "    p.add_argument(\"--json\", type=str, default=\"level74_hrp_allocation_summary.json\")\n",
    "    p.add_argument(\"--seed\", type=int, default=42)\n",
    "\n",
    "    a = p.parse_args()\n",
    "    symbols = tuple(s.strip() for s in a.symbols.split(\",\") if s.strip())\n",
    "\n",
    "    return Config(\n",
    "        symbols=symbols,\n",
    "        start=a.start,\n",
    "        lookback=a.lookback,\n",
    "        rebalance_freq=a.rebalance_freq,\n",
    "        out_csv=a.csv,\n",
    "        out_json=a.json,\n",
    "        seed=a.seed,\n",
    "    )\n",
    "\n",
    "\n",
    "# --------------------------- Main ---------------------------\n",
    "\n",
    "def main() -> None:\n",
    "    cfg = parse_args()\n",
    "    np.random.seed(cfg.seed)\n",
    "\n",
    "    print(f\"[INFO] Downloading prices for {cfg.symbols} from {cfg.start} ...\")\n",
    "    out, summary, prices = run_pipeline(cfg)\n",
    "    save_outputs(out, summary, prices, cfg)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Jupyter / PyCharm shim: strip kernel args\n",
    "    import sys\n",
    "\n",
    "    sys.argv = [sys.argv[0]] + [\n",
    "        arg\n",
    "        for arg in sys.argv[1:]\n",
    "        if arg != \"-f\" and not (arg.endswith(\".json\") and \"kernel\" in arg)\n",
    "    ]\n",
    "    main()\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Downloading prices for ('SPY', 'QQQ', 'IWM', 'EFA', 'EEM', 'TLT', 'LQD', 'GLD') from 2010-01-01 ...\n",
      "[OK] Saved daily series → level74_hrp_allocation.csv\n",
      "[OK] Saved summary      → level74_hrp_allocation_summary.json\n",
      "Period 2011-01-31 → 2025-12-04, n_days=3735\n",
      "HRP portfolio:   AnnRet=6.02%, AnnVol=8.02%, Sharpe=0.75, MaxDD=-25.93%\n",
      "Equal-weight:    AnnRet=8.16%, AnnVol=11.90%, Sharpe=0.69, MaxDD=-27.96%\n"
     ]
    }
   ],
   "execution_count": 2
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
