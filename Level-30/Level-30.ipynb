{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-29T00:11:22.072368Z",
     "start_time": "2025-10-29T00:10:52.645486Z"
    }
   },
   "source": [
    "# ============================ Level-30 ============================\n",
    "# Purged, embargoed time-series CV + probability threshold search + walk-forward backtest\n",
    "# =================================================================\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple, Optional, Iterable\n",
    "\n",
    "import yfinance as yf\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# --------------------------- Config ---------------------------\n",
    "TICKER        = \"AAPL\"\n",
    "YEARS         = 5\n",
    "FREQ          = \"1d\"        # \"1h\" / \"1m\" also works (consider more history)\n",
    "VOL_SPAN      = 50\n",
    "MIN_EVENTS    = 80          # how many labels we want minimally\n",
    "BASE_H        = 10\n",
    "UP_M, DN_M    = 1.0, 1.0\n",
    "\n",
    "N_FOLDS       = 5           # Purged CV folds\n",
    "EMBARGO_FRAC  = 0.01        # 1% embargo of the whole sample length\n",
    "THRESH_GRID   = np.linspace(0.5, 0.9, 9)  # probability threshold candidates\n",
    "TC_BP         = 5           # transaction costs in basis points per trade side (0.0001 = 1bp)\n",
    "MAX_TURNOVER  = 0.5         # max daily change in position (to reduce churn)\n",
    "\n",
    "RANDOM_STATE  = 42\n",
    "\n",
    "# --------------------------- Helpers (robust price extraction etc.) ---------------------------\n",
    "def ensure_series(x: pd.Series, name: Optional[str] = None) -> pd.Series:\n",
    "    if isinstance(x, pd.DataFrame):\n",
    "        x = x.iloc[:, 0]\n",
    "    if not isinstance(x, pd.Series):\n",
    "        raise TypeError(\"Expected a pandas Series.\")\n",
    "    if name:\n",
    "        x = x.rename(name)\n",
    "    x = pd.to_numeric(x, errors=\"coerce\")\n",
    "    x = x.dropna()\n",
    "    x = x[~x.index.duplicated(keep=\"last\")].sort_index()\n",
    "    return x\n",
    "\n",
    "def tz_to_naive_eastern(idx: pd.DatetimeIndex) -> pd.DatetimeIndex:\n",
    "    if idx.tz is None:\n",
    "        return idx\n",
    "    return idx.tz_convert(\"US/Eastern\").tz_localize(None)\n",
    "\n",
    "def extract_close_series(df: pd.DataFrame) -> pd.Series:\n",
    "    candidates = []\n",
    "    if \"Adj Close\" in df.columns: candidates.append(\"Adj Close\")\n",
    "    if \"Close\" in df.columns:     candidates.append(\"Close\")\n",
    "    if not candidates:\n",
    "        raise RuntimeError(\"No Close/Adj Close in data.\")\n",
    "    for col in candidates:\n",
    "        s = df[col]\n",
    "        if isinstance(s, pd.DataFrame):  # duplicate columns\n",
    "            s = s.iloc[:, 0]\n",
    "        s = ensure_series(s, \"Close\")\n",
    "        if not s.empty:\n",
    "            return s\n",
    "    raise RuntimeError(\"Could not extract non-empty Close series.\")\n",
    "\n",
    "def load_prices(ticker: str, years: int, freq: str = \"1d\") -> pd.Series:\n",
    "    start = (datetime.utcnow() - timedelta(days=int(365*years + 20))).date()\n",
    "    df = yf.download(ticker, start=str(start), interval=freq, auto_adjust=True, progress=False)\n",
    "    if df.empty:\n",
    "        raise RuntimeError(\"No data returned. Check ticker/frequency/network.\")\n",
    "    if isinstance(df.index, pd.DatetimeIndex):\n",
    "        idx = df.index\n",
    "        try:\n",
    "            idx = tz_to_naive_eastern(idx.tz_convert(\"UTC\"))\n",
    "        except Exception:\n",
    "            if idx.tz is not None:\n",
    "                idx = tz_to_naive_eastern(idx)\n",
    "        df.index = idx\n",
    "    return extract_close_series(df)\n",
    "\n",
    "def ewma_vol(rets: pd.Series, span: int = 50) -> pd.Series:\n",
    "    r = ensure_series(rets, \"r\")\n",
    "    v = r.ewm(span=span, adjust=False).std()\n",
    "    return v.rename(\"vol\")\n",
    "\n",
    "def cusum_filter(r: pd.Series, threshold: float) -> pd.DatetimeIndex:\n",
    "    r = ensure_series(r, \"r\")\n",
    "    s_pos = 0.0\n",
    "    s_neg = 0.0\n",
    "    t_events = []\n",
    "    for t, x in r.items():\n",
    "        xf = float(x)\n",
    "        s_pos = max(0.0, s_pos + xf)\n",
    "        s_neg = min(0.0, s_neg + xf)\n",
    "        if s_pos > threshold:\n",
    "            s_pos = 0.0\n",
    "            t_events.append(t)\n",
    "        elif s_neg < -threshold:\n",
    "            s_neg = 0.0\n",
    "            t_events.append(t)\n",
    "    return pd.DatetimeIndex(t_events)\n",
    "\n",
    "def get_vertical_barriers(t_events: pd.DatetimeIndex, H: int, full_index: pd.DatetimeIndex) -> pd.Series:\n",
    "    full_index = pd.DatetimeIndex(full_index)\n",
    "    pos_map = {ts: i for i, ts in enumerate(full_index)}\n",
    "    n = len(full_index)\n",
    "    out = {}\n",
    "    for t0 in t_events:\n",
    "        if t0 in pos_map:\n",
    "            i = pos_map[t0]\n",
    "        else:\n",
    "            i = full_index.searchsorted(t0, side=\"left\")\n",
    "        j = min(n - 1, i + int(H))\n",
    "        out[t0] = full_index[j]\n",
    "    return pd.Series(out, name=\"t1\")\n",
    "\n",
    "def first_cross_idx(path_vals: np.ndarray, level: float, cmp: str) -> Optional[int]:\n",
    "    if cmp == \"ge\":\n",
    "        mask = path_vals >= level\n",
    "    elif cmp == \"le\":\n",
    "        mask = path_vals <= level\n",
    "    else:\n",
    "        raise ValueError(\"cmp must be 'ge' or 'le'\")\n",
    "    where = np.where(mask)[0]\n",
    "    return int(where[0]) if where.size else None\n",
    "\n",
    "def get_triple_barrier_labels(\n",
    "    close: pd.Series,\n",
    "    t_events: pd.DatetimeIndex,\n",
    "    vbar: pd.Series,\n",
    "    up_m: float,\n",
    "    dn_m: float,\n",
    "    daily_vol: pd.Series\n",
    ") -> pd.DataFrame:\n",
    "    close = ensure_series(close, \"Close\")\n",
    "    daily_vol = ensure_series(daily_vol, \"vol\")\n",
    "    vbar = vbar.dropna()\n",
    "    rows = []\n",
    "    for t0 in t_events:\n",
    "        if t0 not in close.index or t0 not in vbar.index:\n",
    "            continue\n",
    "        t1 = vbar.loc[t0]\n",
    "        if t1 not in close.index:\n",
    "            j = close.index.searchsorted(t1, side=\"left\")\n",
    "            j = max(0, min(j, len(close) - 1))\n",
    "            t1 = close.index[j]\n",
    "        c0 = float(close.loc[t0])\n",
    "        trgt = float(daily_vol.get(t0, np.nan))\n",
    "        if not np.isfinite(trgt) or trgt <= 0:\n",
    "            continue\n",
    "        up_lvl = c0 * (1 + up_m * trgt)\n",
    "        dn_lvl = c0 * (1 - dn_m * trgt)\n",
    "\n",
    "        path_idx = close.loc[t0:t1].index\n",
    "        path = close.loc[path_idx].to_numpy(dtype=float)\n",
    "\n",
    "        iu = first_cross_idx(path, up_lvl, \"ge\")\n",
    "        idn = first_cross_idx(path, dn_lvl, \"le\")\n",
    "\n",
    "        label = 0\n",
    "        t_hit = t1\n",
    "        if iu is not None and idn is not None:\n",
    "            if iu < idn:\n",
    "                label = 1;  t_hit = path_idx[iu]\n",
    "            elif idn < iu:\n",
    "                label = -1; t_hit = path_idx[idn]\n",
    "            else:\n",
    "                label = 0;  t_hit = path_idx[iu]\n",
    "        elif iu is not None:\n",
    "            label = 1;  t_hit = path_idx[iu]\n",
    "        elif idn is not None:\n",
    "            label = -1; t_hit = path_idx[idn]\n",
    "        else:\n",
    "            rvt = float(close.loc[t1] / c0 - 1.0)\n",
    "            label = 1 if rvt > 0 else (-1 if rvt < 0 else 0)\n",
    "            t_hit = t1\n",
    "\n",
    "        rows.append({\"t0\": t0, \"t1\": t1, \"t_hit\": t_hit, \"label\": label, \"trgt\": trgt})\n",
    "\n",
    "    out = pd.DataFrame.from_records(rows).set_index(\"t0\").sort_index()\n",
    "    return out\n",
    "\n",
    "def build_features(px: pd.Series) -> pd.DataFrame:\n",
    "    px = ensure_series(px, \"Close\")\n",
    "    r1 = px.pct_change().fillna(0.0)\n",
    "    r5 = px.pct_change(5).fillna(0.0)\n",
    "    vol20 = r1.rolling(20).std().fillna(0.0)\n",
    "    mom10 = px.pct_change(10).fillna(0.0)\n",
    "    ma10 = px.rolling(10).mean()\n",
    "    ma20 = px.rolling(20).mean()\n",
    "    ma_ratio = (ma10 / ma20 - 1.0).replace([np.inf, -np.inf], 0.0).fillna(0.0)\n",
    "    feat = pd.DataFrame({\"r1\": r1, \"r5\": r5, \"vol20\": vol20, \"mom10\": mom10, \"ma_ratio\": ma_ratio}, index=px.index)\n",
    "    return feat\n",
    "\n",
    "def adaptive_events_and_labels(\n",
    "    close: pd.Series,\n",
    "    rets: pd.Series,\n",
    "    vol_span: int = 50,\n",
    "    min_events: int = 80,\n",
    "    base_H: int = 10,\n",
    "    up_m: float = 1.0,\n",
    "    dn_m: float = 1.0\n",
    ") -> Tuple[pd.DataFrame, pd.Series, pd.DatetimeIndex, int, float]:\n",
    "    close = ensure_series(close, \"Close\")\n",
    "    rets  = ensure_series(rets, \"r\")\n",
    "    daily_vol = ewma_vol(rets, span=vol_span).clip(lower=1e-8).fillna(0.0)\n",
    "\n",
    "    vol_std = float(rets.std()) if np.isfinite(float(rets.std())) else 0.01\n",
    "    thr_fracs = [0.50, 0.35, 0.25, 0.18, 0.12, 0.08, 0.05, 0.03]\n",
    "    H_choices = [base_H, int(base_H*1.5), base_H*2, base_H*3]\n",
    "\n",
    "    labels = pd.DataFrame()\n",
    "    used_thr, used_H, events_idx = None, None, pd.DatetimeIndex([])\n",
    "\n",
    "    for H_try in H_choices:\n",
    "        for frac in thr_fracs:\n",
    "            thr = max(1e-4, frac * vol_std)\n",
    "            events_idx = cusum_filter(rets, thr)\n",
    "            if len(events_idx) == 0:\n",
    "                continue\n",
    "            vbar = get_vertical_barriers(events_idx, H_try, close.index)\n",
    "            lab = get_triple_barrier_labels(close, events_idx, vbar, up_m, dn_m, daily_vol).dropna()\n",
    "            if len(lab) >= min_events:\n",
    "                labels = lab\n",
    "                used_thr, used_H = thr, H_try\n",
    "                break\n",
    "        if len(labels) >= min_events:\n",
    "            break\n",
    "\n",
    "    if labels.empty:\n",
    "        for H_try in H_choices[max(1, len(H_choices)//2):]:\n",
    "            for frac in thr_fracs + [0.02, 0.015, 0.01]:\n",
    "                thr = max(5e-5, frac * vol_std)\n",
    "                events_idx = cusum_filter(rets, thr)\n",
    "                if len(events_idx) == 0:\n",
    "                    continue\n",
    "                vbar = get_vertical_barriers(events_idx, H_try, close.index)\n",
    "                lab = get_triple_barrier_labels(close, events_idx, vbar, up_m*0.5, dn_m*0.5, daily_vol).dropna()\n",
    "                if len(lab) >= max(10, min_events//2):\n",
    "                    labels = lab\n",
    "                    used_thr, used_H = thr, H_try\n",
    "                    break\n",
    "            if not labels.empty:\n",
    "                break\n",
    "\n",
    "    return labels, daily_vol, events_idx, (used_H or base_H), float(used_thr or 0.0)\n",
    "\n",
    "# --------------------------- Purged, embargoed CV ---------------------------\n",
    "@dataclass\n",
    "class PurgedFold:\n",
    "    tr_idx: np.ndarray\n",
    "    te_idx: np.ndarray\n",
    "\n",
    "def make_purged_folds(index: pd.DatetimeIndex,\n",
    "                      t1: pd.Series,\n",
    "                      n_folds: int,\n",
    "                      embargo_frac: float) -> List[PurgedFold]:\n",
    "    \"\"\"\n",
    "    Split the time index into ordered folds; purge train samples whose (t0,t1) overlap test,\n",
    "    and embargo a gap after each test fold.\n",
    "    \"\"\"\n",
    "    n = len(index)\n",
    "    fold_sizes = np.full(n_folds, n // n_folds, dtype=int)\n",
    "    fold_sizes[: n % n_folds] += 1\n",
    "    bounds = np.cumsum(fold_sizes)\n",
    "\n",
    "    folds = []\n",
    "    start = 0\n",
    "    embargo = int(np.ceil(n * embargo_frac))\n",
    "\n",
    "    for b in bounds:\n",
    "        te_slice = np.arange(start, b)\n",
    "        te_idx = te_slice\n",
    "\n",
    "        te_times = index[te_idx]\n",
    "        te_start, te_end = te_times[0], te_times[-1]\n",
    "\n",
    "        # Purge: remove any train event whose span intersects [te_start, te_end]\n",
    "        tr_mask = np.ones(n, dtype=bool)\n",
    "        if not t1.empty:\n",
    "            # events exist at subset of index; we use conservative rule:\n",
    "            # any sample whose timestamp is within [te_start, te_end] gets removed from train\n",
    "            tr_mask[(index >= te_start) & (index <= te_end)] = False\n",
    "\n",
    "        # Embargo: remove the next embargo samples after test block\n",
    "        emb_start = min(n, b)\n",
    "        emb_end = min(n, b + embargo)\n",
    "        tr_mask[emb_start:emb_end] = False\n",
    "\n",
    "        # Also remove the test indices from train\n",
    "        tr_mask[te_idx] = False\n",
    "\n",
    "        tr_idx = np.nonzero(tr_mask)[0]\n",
    "        folds.append(PurgedFold(tr_idx=tr_idx, te_idx=te_idx))\n",
    "\n",
    "        start = b\n",
    "    return folds\n",
    "\n",
    "# --------------------------- Backtest helpers ---------------------------\n",
    "def sharpe_ratio(rets: pd.Series) -> float:\n",
    "    if len(rets) < 2: return 0.0\n",
    "    mu = rets.mean()\n",
    "    sd = rets.std(ddof=0)\n",
    "    return 0.0 if sd == 0 else float(np.sqrt(252) * mu / sd)\n",
    "\n",
    "def apply_tc(positions: pd.Series, tc_bp: float) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Subtract transaction costs when position changes sign/size.\n",
    "    positions: target in [0,1] (long/flat).\n",
    "    TC applied on abs(delta_pos) * tc_rate.\n",
    "    \"\"\"\n",
    "    tc_rate = tc_bp / 10000.0\n",
    "    dpos = positions.diff().fillna(positions.iloc[0])\n",
    "    return dpos.abs() * tc_rate\n",
    "\n",
    "def cap_turnover(positions: pd.Series, max_turnover: float) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Limit per-step change in position to max_turnover.\n",
    "    \"\"\"\n",
    "    pos = positions.copy()\n",
    "    for i in range(1, len(pos)):\n",
    "        delta = pos.iloc[i] - pos.iloc[i-1]\n",
    "        if abs(delta) > max_turnover:\n",
    "            pos.iloc[i] = pos.iloc[i-1] + np.sign(delta) * max_turnover\n",
    "    return pos.clip(0.0, 1.0)\n",
    "\n",
    "# ============================ Main Flow ============================\n",
    "px = load_prices(TICKER, YEARS, FREQ)\n",
    "rets = px.pct_change().replace([np.inf, -np.inf], 0.0).fillna(0.0)\n",
    "\n",
    "labels, daily_vol, events_idx, H_used, thr_used = adaptive_events_and_labels(\n",
    "    close=px, rets=rets,\n",
    "    vol_span=VOL_SPAN, min_events=MIN_EVENTS,\n",
    "    base_H=BASE_H, up_m=UP_M, dn_m=DN_M\n",
    ")\n",
    "print(f\"[Adaptive] events={len(events_idx)} labels={len(labels)}  H={H_used}  thr≈{thr_used:.6g}\")\n",
    "if labels.empty:\n",
    "    raise RuntimeError(\"No labels. Increase YEARS or lower MIN_EVENTS / switch to higher frequency.\")\n",
    "\n",
    "# Features & target aligned on t0\n",
    "X_all = build_features(px)\n",
    "X = X_all.reindex(labels.index).dropna()\n",
    "y = (labels[\"label\"] == 1).astype(int).reindex(X.index)\n",
    "t1 = labels[\"t1\"].reindex(X.index)  # for purging logic; here we use simple conservative rule on timestamps\n",
    "\n",
    "# Common scaler & model\n",
    "scaler = StandardScaler()\n",
    "clf = RandomForestClassifier(\n",
    "    n_estimators=400, max_depth=7,\n",
    "    random_state=RANDOM_STATE, n_jobs=-1,\n",
    "    class_weight=\"balanced_subsample\"\n",
    ")\n",
    "\n",
    "# Build folds on the feature index (time-ordered)\n",
    "folds = make_purged_folds(X.index, t1, n_folds=N_FOLDS, embargo_frac=EMBARGO_FRAC)\n",
    "\n",
    "val_scores = []\n",
    "fold_reports = []\n",
    "per_fold_best = []\n",
    "\n",
    "for k, fold in enumerate(folds, 1):\n",
    "    tr_idx, te_idx = fold.tr_idx, fold.te_idx\n",
    "    X_tr, X_te = X.iloc[tr_idx], X.iloc[te_idx]\n",
    "    y_tr, y_te = y.iloc[tr_idx], y.iloc[te_idx]\n",
    "\n",
    "    if y_tr.nunique() < 2 or len(y_te) == 0:\n",
    "        continue\n",
    "\n",
    "    X_tr_sc = pd.DataFrame(scaler.fit_transform(X_tr), index=X_tr.index, columns=X_tr.columns)\n",
    "    X_te_sc = pd.DataFrame(scaler.transform(X_te), index=X_te.index, columns=X_te.columns)\n",
    "\n",
    "    clf.fit(X_tr_sc, y_tr)\n",
    "    proba = pd.Series(clf.predict_proba(X_te_sc)[:, 1], index=X_te_sc.index)\n",
    "\n",
    "    # Threshold search to maximize Sharpe on validation (using realized next-day returns)\n",
    "    # Strategy: position_t = 1 if proba_t >= thr else 0; pnl_t+1 = position_t * rets_{t+1}\n",
    "    # Align: use returns on the same universe dates:\n",
    "    r = rets.reindex(proba.index.union(rets.index)).reindex(proba.index).shift(-1).fillna(0.0)\n",
    "\n",
    "    best_thr, best_sh = 0.5, -np.inf\n",
    "    for thr in THRESH_GRID:\n",
    "        pos = (proba >= thr).astype(float)\n",
    "        pos = cap_turnover(pos, MAX_TURNOVER)\n",
    "        tc = apply_tc(pos, TC_BP)\n",
    "        pnl = pos * r - tc\n",
    "        sh = sharpe_ratio(pnl)\n",
    "        if sh > best_sh:\n",
    "            best_sh, best_thr = sh, thr\n",
    "\n",
    "    per_fold_best.append((best_thr, best_sh))\n",
    "    val_scores.append(best_sh)\n",
    "\n",
    "    y_pred = (proba >= best_thr).astype(int)\n",
    "    rep = classification_report(y_te, y_pred, digits=3, output_dict=True, zero_division=0)\n",
    "    fold_reports.append((k, rep))\n",
    "\n",
    "mean_val_sharpe = float(np.mean(val_scores)) if val_scores else 0.0\n",
    "best_thr_overall = float(np.median([t for t, _ in per_fold_best])) if per_fold_best else 0.6\n",
    "\n",
    "print(f\"\\nCV mean Sharpe (validation): {mean_val_sharpe:.3f}\")\n",
    "print(f\"Chosen probability threshold (median of per-fold best): {best_thr_overall:.2f}\")\n",
    "\n",
    "# --------------------------- Refit on full sample & Walk-forward backtest ---------------------------\n",
    "X_sc_all = pd.DataFrame(scaler.fit_transform(X), index=X.index, columns=X.columns)\n",
    "clf.fit(X_sc_all, y)\n",
    "\n",
    "proba_all = pd.Series(clf.predict_proba(X_sc_all)[:, 1], index=X_sc_all.index)\n",
    "pos_raw = (proba_all >= best_thr_overall).astype(float)\n",
    "pos = cap_turnover(pos_raw, MAX_TURNOVER)\n",
    "\n",
    "# Backtest on the same dates (out-of-sample equivalent is complex; here we show realized pnl using same-aligned returns)\n",
    "r_aligned = rets.reindex(pos.index).shift(-1).fillna(0.0)\n",
    "tc = apply_tc(pos, TC_BP)\n",
    "pnl = pos * r_aligned - tc\n",
    "\n",
    "eq = (1 + pnl).cumprod()\n",
    "sh = sharpe_ratio(pnl)\n",
    "cagr = (eq.iloc[-1]) ** (252/len(eq)) - 1 if len(eq) > 10 else np.nan\n",
    "dd = (eq / eq.cummax() - 1.0).min()\n",
    "hit_rate = (pnl > 0).mean()\n",
    "\n",
    "print(\"\\n=== Walk-forward Backtest (long/flat meta-signal) ===\")\n",
    "print(f\"Sharpe: {sh:.3f}  CAGR: {cagr:.2%}  MaxDD: {dd:.2%}  Hit%: {hit_rate:.2%}\")\n",
    "print(f\"Avg daily turnover: {pos.diff().abs().mean():.3f}  (Max cap {MAX_TURNOVER})\")\n",
    "print(f\"Avg daily TC paid (bp): {tc.mean()*10000:.2f}\")\n",
    "\n",
    "# Save artifacts\n",
    "out_cv = pd.DataFrame({\n",
    "    \"fold\": [i for i,_ in fold_reports],\n",
    "    \"val_sharpe\": [s for _, s in per_fold_best],\n",
    "    \"best_thr\":   [t for t, _ in per_fold_best]\n",
    "})\n",
    "out_cv.to_csv(f\"{TICKER}_level30_cv_summary.csv\", index=False)\n",
    "\n",
    "bt = pd.DataFrame({\n",
    "    \"proba\": proba_all,\n",
    "    \"pos_raw\": pos_raw,\n",
    "    \"pos_capped\": pos,\n",
    "    \"rets_next\": r_aligned,\n",
    "    \"tc\": tc,\n",
    "    \"pnl\": pnl,\n",
    "    \"equity\": eq\n",
    "})\n",
    "bt.to_csv(f\"{TICKER}_level30_backtest.csv\", index_label=\"date\")\n",
    "\n",
    "print(f\"\\nSaved: {TICKER}_level30_cv_summary.csv, {TICKER}_level30_backtest.csv\")\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\AppData\\Local\\Temp\\ipykernel_12488\\2997544063.py:72: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  start = (datetime.utcnow() - timedelta(days=int(365*years + 20))).date()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Adaptive] events=708 labels=708  H=10  thr≈0.00895125\n",
      "\n",
      "CV mean Sharpe (validation): 2.240\n",
      "Chosen probability threshold (median of per-fold best): 0.55\n",
      "\n",
      "=== Walk-forward Backtest (long/flat meta-signal) ===\n",
      "Sharpe: 6.686  CAGR: 279.59%  MaxDD: -5.65%  Hit%: 44.35%\n",
      "Avg daily turnover: 0.238  (Max cap 0.5)\n",
      "Avg daily TC paid (bp): 1.19\n",
      "\n",
      "Saved: AAPL_level30_cv_summary.csv, AAPL_level30_backtest.csv\n"
     ]
    }
   ],
   "execution_count": 1
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
