{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-05T03:41:46.924202Z",
     "start_time": "2025-12-05T03:41:41.072853Z"
    }
   },
   "source": [
    "# level70_vol_target_drawdown.py\n",
    "# Volatility-Targeted, Drawdown-Aware Overlay on a Base Portfolio (EW or HRP)\n",
    "# Universe: SPY, QQQ, IWM, EFA, EEM, TLT, LQD, GLD\n",
    "#\n",
    "# Steps:\n",
    "#   1) Build a base daily portfolio (Equal-Weight or static HRP, rebalanced monthly).\n",
    "#   2) Compute rolling realized vol of base portfolio.\n",
    "#   3) Apply volatility targeting with leverage bounds.\n",
    "#   4) Apply drawdown-aware scaling (reduce leverage as drawdown deepens).\n",
    "#   5) Compare base vs vol-targeted+DD-controlled performance.\n",
    "#\n",
    "# Outputs:\n",
    "#   - level70_vol_target_drawdown.csv\n",
    "#   - level70_vol_target_drawdown_summary.json\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import Sequence, Tuple, List, Dict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "\n",
    "\n",
    "# --------------------------- Config ---------------------------\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    symbols: Tuple[str, ...] = (\n",
    "        \"SPY\", \"QQQ\", \"IWM\", \"EFA\", \"EEM\", \"TLT\", \"LQD\", \"GLD\"\n",
    "    )\n",
    "    start: str = \"2010-01-01\"\n",
    "\n",
    "    # Base portfolio construction\n",
    "    base_mode: str = \"hrp\"        # \"hrp\" or \"ew\"\n",
    "    cov_lookback: int = 252       # days for covariance estimation\n",
    "    min_lookback: int = 126       # min days before we start using HRP\n",
    "    rebalance_freq: str = \"ME\"    # month-end\n",
    "\n",
    "    # Vol targeting overlay\n",
    "    vol_lookback: int = 60        # days of realized vol\n",
    "    vol_target: float = 0.10      # 10% annual target\n",
    "    lev_min: float = 0.0          # min leverage\n",
    "    lev_max: float = 2.0          # max leverage\n",
    "\n",
    "    # Drawdown-aware scaling\n",
    "    dd_thresh: float = 0.15       # 15% drawdown threshold\n",
    "    dd_max: float = 0.30          # 30% drawdown where scaling hits minimum\n",
    "    dd_min_scale: float = 0.20    # at max drawdown, leverage is scaled to 20% of base\n",
    "\n",
    "    # Outputs\n",
    "    out_csv: str = \"level70_vol_target_drawdown.csv\"\n",
    "    out_json: str = \"level70_vol_target_drawdown_summary.json\"\n",
    "\n",
    "    seed: int = 42\n",
    "\n",
    "\n",
    "# --------------------------- Data Loader ---------------------------\n",
    "\n",
    "def load_prices(symbols: Sequence[str], start: str) -> pd.DataFrame:\n",
    "    \"\"\"Download adjusted close prices for a list of symbols from yfinance.\"\"\"\n",
    "    frames = []\n",
    "    for s in symbols:\n",
    "        px = yf.download(s, start=start, auto_adjust=True, progress=False)\n",
    "        if px.empty:\n",
    "            raise RuntimeError(f\"No price data downloaded for {s}.\")\n",
    "        if \"Close\" not in px.columns:\n",
    "            raise RuntimeError(f\"'Close' column missing for {s}.\")\n",
    "\n",
    "        close = px[\"Close\"].copy()\n",
    "        close.name = s\n",
    "        frames.append(close)\n",
    "\n",
    "    prices = pd.concat(frames, axis=1).sort_index()\n",
    "    prices = prices.dropna(how=\"all\")\n",
    "    prices = prices.ffill().dropna(how=\"any\")\n",
    "    return prices\n",
    "\n",
    "\n",
    "def compute_returns(prices: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Daily log returns.\"\"\"\n",
    "    rets = np.log(prices).diff()\n",
    "    rets = rets.dropna(how=\"all\")\n",
    "    return rets\n",
    "\n",
    "\n",
    "# --------------------------- Base Portfolio Helpers ---------------------------\n",
    "\n",
    "def equal_weight(symbols: Sequence[str]) -> pd.Series:\n",
    "    n = len(symbols)\n",
    "    w = np.ones(n) / n\n",
    "    return pd.Series(w, index=list(symbols))\n",
    "\n",
    "\n",
    "def cov_to_corr(cov: pd.DataFrame) -> pd.DataFrame:\n",
    "    if not isinstance(cov, pd.DataFrame):\n",
    "        cov = pd.DataFrame(cov)\n",
    "    diag = np.diag(cov.values)\n",
    "    diag = np.where(diag <= 0, 1e-12, diag)\n",
    "    std = np.sqrt(diag)\n",
    "    denom = np.outer(std, std)\n",
    "    corr = cov.values / denom\n",
    "    corr[~np.isfinite(corr)] = 0.0\n",
    "    np.fill_diagonal(corr, 1.0)\n",
    "    return pd.DataFrame(corr, index=cov.index, columns=cov.columns)\n",
    "\n",
    "\n",
    "def correl_to_dist(corr: pd.DataFrame) -> pd.DataFrame:\n",
    "    if not isinstance(corr, pd.DataFrame):\n",
    "        corr = pd.DataFrame(corr)\n",
    "    d = np.sqrt(0.5 * (1.0 - corr.values))\n",
    "    np.fill_diagonal(d, 0.0)\n",
    "    return pd.DataFrame(d, index=corr.index, columns=corr.columns)\n",
    "\n",
    "\n",
    "def single_linkage_order(dist: pd.DataFrame) -> List[int]:\n",
    "    \"\"\"\n",
    "    Naive single-linkage clustering to get a leaf ordering\n",
    "    (no external libraries).\n",
    "    \"\"\"\n",
    "    n = dist.shape[0]\n",
    "    D = dist.values.astype(float)\n",
    "    np.fill_diagonal(D, np.inf)\n",
    "\n",
    "    clusters: Dict[int, List[int]] = {i: [i] for i in range(n)}\n",
    "    next_id = n\n",
    "\n",
    "    while len(clusters) > 1:\n",
    "        ids = list(clusters.keys())\n",
    "        best = None\n",
    "        best_pair = None\n",
    "\n",
    "        for i in range(len(ids)):\n",
    "            for j in range(i + 1, len(ids)):\n",
    "                ci = ids[i]\n",
    "                cj = ids[j]\n",
    "                members_i = clusters[ci]\n",
    "                members_j = clusters[cj]\n",
    "                sub = D[np.ix_(members_i, members_j)]\n",
    "                d_ij = float(np.min(sub))\n",
    "                if (best is None) or (d_ij < best):\n",
    "                    best = d_ij\n",
    "                    best_pair = (ci, cj)\n",
    "\n",
    "        if best_pair is None:\n",
    "            break\n",
    "\n",
    "        a, b = best_pair\n",
    "        new_members = clusters[a] + clusters[b]\n",
    "        del clusters[a]\n",
    "        del clusters[b]\n",
    "        clusters[next_id] = new_members\n",
    "        next_id += 1\n",
    "\n",
    "    final_members = list(clusters.values())[0]\n",
    "    return final_members\n",
    "\n",
    "\n",
    "def hrp_weights(cov: pd.DataFrame) -> pd.Series:\n",
    "    \"\"\"Hierarchical Risk Parity long-only weights.\"\"\"\n",
    "    if not isinstance(cov, pd.DataFrame):\n",
    "        cov = pd.DataFrame(cov)\n",
    "    cols = list(cov.columns)\n",
    "    n = len(cols)\n",
    "\n",
    "    corr = cov_to_corr(cov)\n",
    "    dist = correl_to_dist(corr)\n",
    "    order = single_linkage_order(dist)\n",
    "\n",
    "    cov_reordered = cov.values[order][:, order]\n",
    "    diag = np.diag(cov_reordered)\n",
    "    diag = np.where(diag <= 0, 1e-8, diag)\n",
    "    inv_var = 1.0 / diag\n",
    "\n",
    "    weights = np.ones(n)\n",
    "    clusters = [np.arange(n)]\n",
    "\n",
    "    while clusters:\n",
    "        cluster = clusters.pop(0)\n",
    "        if len(cluster) <= 1:\n",
    "            continue\n",
    "\n",
    "        split = len(cluster) // 2\n",
    "        left = cluster[:split]\n",
    "        right = cluster[split:]\n",
    "\n",
    "        inv_var_left = inv_var[left]\n",
    "        inv_var_right = inv_var[right]\n",
    "\n",
    "        w_left = inv_var_left / inv_var_left.sum()\n",
    "        w_right = inv_var_right / inv_var_right.sum()\n",
    "\n",
    "        cov_left = cov_reordered[np.ix_(left, left)]\n",
    "        cov_right = cov_reordered[np.ix_(right, right)]\n",
    "\n",
    "        var_left = float(w_left @ cov_left @ w_left)\n",
    "        var_right = float(w_right @ cov_right @ w_right)\n",
    "\n",
    "        if var_left + var_right == 0:\n",
    "            alpha = 0.5\n",
    "        else:\n",
    "            alpha = 1.0 - var_left / (var_left + var_right)\n",
    "\n",
    "        weights[left] *= alpha\n",
    "        weights[right] *= (1.0 - alpha)\n",
    "\n",
    "        clusters.append(left)\n",
    "        clusters.append(right)\n",
    "\n",
    "    w_final = np.zeros(n)\n",
    "    for pos, asset_idx in enumerate(order):\n",
    "        w_final[asset_idx] = weights[pos]\n",
    "\n",
    "    return pd.Series(w_final, index=cols)\n",
    "\n",
    "\n",
    "def compute_rebalance_dates(rets: pd.DataFrame, freq: str) -> pd.DatetimeIndex:\n",
    "    \"\"\"\n",
    "    Rebalance dates; here, we expect 'ME' for month-end.\n",
    "    \"\"\"\n",
    "    if freq != \"ME\":\n",
    "        raise ValueError(\"This script expects rebalance_freq 'ME' (month-end).\")\n",
    "    return rets.resample(\"ME\").last().index\n",
    "\n",
    "\n",
    "def build_base_weights(rets: pd.DataFrame, cfg: Config) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build base portfolio weights (EW or HRP) on monthly rebalancing.\n",
    "    \"\"\"\n",
    "    symbols = list(cfg.symbols)\n",
    "    idx = rets.index\n",
    "    w = pd.DataFrame(index=idx, columns=symbols, dtype=float)\n",
    "\n",
    "    rebal_dates = compute_rebalance_dates(rets, cfg.rebalance_freq)\n",
    "\n",
    "    for d in rebal_dates:\n",
    "        window = rets.loc[:d].tail(cfg.cov_lookback)\n",
    "        if window.shape[0] < cfg.min_lookback:\n",
    "            continue\n",
    "\n",
    "        if cfg.base_mode.lower() == \"ew\":\n",
    "            w_d = equal_weight(symbols)\n",
    "        elif cfg.base_mode.lower() == \"hrp\":\n",
    "            cov = window.cov()\n",
    "            if cov.isnull().any().any():\n",
    "                continue\n",
    "            w_d = hrp_weights(cov).reindex(symbols)\n",
    "        else:\n",
    "            raise ValueError(\"base_mode must be 'ew' or 'hrp'\")\n",
    "\n",
    "        w.loc[d] = w_d\n",
    "\n",
    "    w = w.ffill().dropna()\n",
    "    return w\n",
    "\n",
    "\n",
    "# --------------------------- Overlay (Vol Target + DD) ---------------------------\n",
    "\n",
    "def realized_vol_annual(r: pd.Series, lookback: int) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Rolling realized annualized volatility of returns.\n",
    "    \"\"\"\n",
    "    rolling_std = r.rolling(lookback).std()\n",
    "    return rolling_std * np.sqrt(252.0)\n",
    "\n",
    "\n",
    "def drawdown_series(ret: pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Compute drawdown series from daily returns.\n",
    "    Returns in [-1, 0], where 0 is no drawdown.\n",
    "    \"\"\"\n",
    "    eq = (1.0 + ret).cumprod()\n",
    "    peak = eq.cummax()\n",
    "    dd = eq / peak - 1.0\n",
    "    return dd\n",
    "\n",
    "\n",
    "def dd_scaling(\n",
    "    dd_mag: pd.Series, dd_thresh: float, dd_max: float, dd_min_scale: float\n",
    ") -> pd.Series:\n",
    "    \"\"\"\n",
    "    Piecewise linear scaling based on drawdown magnitude (0..1).\n",
    "    dd_mag = -drawdown (so 0 means no drawdown, 0.2 means -20%).\n",
    "    \"\"\"\n",
    "    scale = pd.Series(1.0, index=dd_mag.index, dtype=float)\n",
    "\n",
    "    # where drawdown <= threshold: scale = 1\n",
    "    # between threshold and max: linearly down to dd_min_scale\n",
    "    mask = (dd_mag > dd_thresh) & (dd_mag < dd_max)\n",
    "    mask_hi = dd_mag >= dd_max\n",
    "\n",
    "    denom = (dd_max - dd_thresh) if dd_max > dd_thresh else 1e-6\n",
    "    slope = (dd_min_scale - 1.0) / denom\n",
    "\n",
    "    scale.loc[mask] = 1.0 + slope * (dd_mag.loc[mask] - dd_thresh)\n",
    "    scale.loc[mask_hi] = dd_min_scale\n",
    "\n",
    "    return scale.clip(lower=dd_min_scale, upper=1.0)\n",
    "\n",
    "\n",
    "def apply_overlay(\n",
    "    base_ret: pd.Series,\n",
    "    cfg: Config\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute volatility-targeted + DD-aware leveraged returns.\n",
    "\n",
    "    Returns DataFrame with:\n",
    "        base_ret, lev_raw, lev_final, ret_targeted, eq_base, eq_targeted,\n",
    "        dd_base, dd_mag, dd_scale, vol_ann\n",
    "    \"\"\"\n",
    "    base_ret = base_ret.dropna()\n",
    "    idx = base_ret.index\n",
    "\n",
    "    vol_ann = realized_vol_annual(base_ret, cfg.vol_lookback)\n",
    "    lev_raw = cfg.vol_target / vol_ann\n",
    "    lev_raw = lev_raw.clip(lower=cfg.lev_min, upper=cfg.lev_max)\n",
    "\n",
    "    dd_base = drawdown_series(base_ret)\n",
    "    dd_mag = (-dd_base).clip(lower=0.0)\n",
    "\n",
    "    dd_scale = dd_scaling(dd_mag, cfg.dd_thresh, cfg.dd_max, cfg.dd_min_scale)\n",
    "\n",
    "    lev_final = lev_raw * dd_scale\n",
    "    lev_final = lev_final.fillna(0.0)\n",
    "\n",
    "    ret_targeted = lev_final * base_ret\n",
    "\n",
    "    eq_base = (1.0 + base_ret).cumprod()\n",
    "    eq_targeted = (1.0 + ret_targeted).cumprod()\n",
    "\n",
    "    out = pd.DataFrame(\n",
    "        {\n",
    "            \"ret_base\": base_ret,\n",
    "            \"vol_ann\": vol_ann,\n",
    "            \"lev_raw\": lev_raw,\n",
    "            \"dd_base\": dd_base,\n",
    "            \"dd_mag\": dd_mag,\n",
    "            \"dd_scale\": dd_scale,\n",
    "            \"lev_final\": lev_final,\n",
    "            \"ret_targeted\": ret_targeted,\n",
    "            \"eq_base\": eq_base,\n",
    "            \"eq_targeted\": eq_targeted,\n",
    "        }\n",
    "    )\n",
    "    return out\n",
    "\n",
    "\n",
    "# --------------------------- Backtest & Stats ---------------------------\n",
    "\n",
    "def stats_from_returns(r: pd.Series) -> dict:\n",
    "    if r.empty:\n",
    "        return dict(ann_ret=np.nan, ann_vol=np.nan, sharpe=np.nan, max_dd=np.nan)\n",
    "    r = r.dropna()\n",
    "    if r.empty:\n",
    "        return dict(ann_ret=np.nan, ann_vol=np.nan, sharpe=np.nan, max_dd=np.nan)\n",
    "\n",
    "    mu = float(r.mean())\n",
    "    sig = float(r.std())\n",
    "\n",
    "    ann_ret = (1.0 + mu) ** 252 - 1.0\n",
    "    ann_vol = sig * np.sqrt(252.0)\n",
    "    sharpe = ann_ret / ann_vol if ann_vol > 0 else np.nan\n",
    "\n",
    "    eq = (1.0 + r).cumprod()\n",
    "    peak = eq.cummax()\n",
    "    dd = eq / peak - 1.0\n",
    "    max_dd = float(dd.min()) if not dd.empty else np.nan\n",
    "\n",
    "    return dict(\n",
    "        ann_ret=float(ann_ret),\n",
    "        ann_vol=float(ann_vol),\n",
    "        sharpe=float(sharpe),\n",
    "        max_dd=float(max_dd),\n",
    "    )\n",
    "\n",
    "\n",
    "def run_pipeline(cfg: Config) -> Tuple[pd.DataFrame, dict]:\n",
    "    prices = load_prices(cfg.symbols, cfg.start)\n",
    "    rets = compute_returns(prices)\n",
    "\n",
    "    # Base portfolio weights and returns\n",
    "    w_base = build_base_weights(rets, cfg)\n",
    "    common_idx = rets.index.intersection(w_base.index)\n",
    "    rets = rets.reindex(common_idx)\n",
    "    w_base = w_base.reindex(common_idx)\n",
    "\n",
    "    base_ret = (w_base * rets).sum(axis=1).rename(\"ret_base\")\n",
    "\n",
    "    # Overlay\n",
    "    overlay = apply_overlay(base_ret, cfg)\n",
    "\n",
    "    # Align everything on overlay index\n",
    "    idx = overlay.index\n",
    "    prices = prices.reindex(idx)\n",
    "    rets = rets.reindex(idx)\n",
    "    w_base = w_base.reindex(idx)\n",
    "\n",
    "    out = pd.DataFrame(index=idx)\n",
    "    out[list(cfg.symbols)] = prices\n",
    "    out[[f\"ret_{s}\" for s in cfg.symbols]] = rets.add_prefix(\"ret_\")\n",
    "    out[[f\"w_base_{s}\" for s in cfg.symbols]] = w_base.add_prefix(\"w_base_\")\n",
    "\n",
    "    for col in overlay.columns:\n",
    "        out[col] = overlay[col]\n",
    "\n",
    "    summary = {\n",
    "        \"config\": asdict(cfg),\n",
    "        \"start_date\": str(idx.min().date()) if len(idx) else None,\n",
    "        \"end_date\": str(idx.max().date()) if len(idx) else None,\n",
    "        \"n_days\": int(len(idx)),\n",
    "        \"Base\": stats_from_returns(overlay[\"ret_base\"]),\n",
    "        \"VolTarget_DD\": stats_from_returns(overlay[\"ret_targeted\"]),\n",
    "    }\n",
    "\n",
    "    return out, summary\n",
    "\n",
    "\n",
    "# --------------------------- I/O ---------------------------\n",
    "\n",
    "def save_outputs(out: pd.DataFrame, summary: dict, cfg: Config) -> None:\n",
    "    out.to_csv(cfg.out_csv, index=True, date_format=\"%Y-%m-%d\")\n",
    "    with open(cfg.out_json, \"w\") as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "\n",
    "    print(f\"[OK] Saved daily series → {cfg.out_csv}\")\n",
    "    print(f\"[OK] Saved summary → {cfg.out_json}\")\n",
    "    if summary[\"start_date\"] and summary[\"end_date\"]:\n",
    "        print(\n",
    "            f\"Period {summary['start_date']} → {summary['end_date']}, \"\n",
    "            f\"n_days={summary['n_days']}\"\n",
    "        )\n",
    "\n",
    "    for name in [\"Base\", \"VolTarget_DD\"]:\n",
    "        s = summary[name]\n",
    "        print(\n",
    "            f\"{name}: AnnRet={s['ann_ret']*100:.2f}%, \"\n",
    "            f\"AnnVol={s['ann_vol']*100:.2f}%, \"\n",
    "            f\"Sharpe={s['sharpe']:.2f}, \"\n",
    "            f\"MaxDD={s['max_dd']*100:.2f}%\"\n",
    "        )\n",
    "\n",
    "\n",
    "# --------------------------- CLI ---------------------------\n",
    "\n",
    "def parse_args() -> Config:\n",
    "    p = argparse.ArgumentParser(\n",
    "        description=\"Level-70: Volatility-Targeted, Drawdown-Aware Overlay\"\n",
    "    )\n",
    "    p.add_argument(\n",
    "        \"--symbols\",\n",
    "        type=str,\n",
    "        default=\"SPY,QQQ,IWM,EFA,EEM,TLT,LQD,GLD\",\n",
    "        help=\"Comma-separated tickers.\",\n",
    "    )\n",
    "    p.add_argument(\"--start\", type=str, default=\"2010-01-01\")\n",
    "    p.add_argument(\n",
    "        \"--base-mode\",\n",
    "        type=str,\n",
    "        default=\"hrp\",\n",
    "        choices=[\"hrp\", \"ew\"],\n",
    "        help=\"Base portfolio type: 'hrp' or 'ew'.\",\n",
    "    )\n",
    "    p.add_argument(\"--cov-lookback\", type=int, default=252)\n",
    "    p.add_argument(\"--min-lookback\", type=int, default=126)\n",
    "    p.add_argument(\n",
    "        \"--rebalance-freq\",\n",
    "        type=str,\n",
    "        default=\"ME\",\n",
    "        help=\"Rebalance frequency (use 'ME' for month-end).\",\n",
    "    )\n",
    "\n",
    "    p.add_argument(\"--vol-lookback\", type=int, default=60)\n",
    "    p.add_argument(\"--vol-target\", type=float, default=0.10)\n",
    "    p.add_argument(\"--lev-min\", type=float, default=0.0)\n",
    "    p.add_argument(\"--lev-max\", type=float, default=2.0)\n",
    "\n",
    "    p.add_argument(\"--dd-thresh\", type=float, default=0.15)\n",
    "    p.add_argument(\"--dd-max\", type=float, default=0.30)\n",
    "    p.add_argument(\"--dd-min-scale\", type=float, default=0.20)\n",
    "\n",
    "    p.add_argument(\n",
    "        \"--csv\", type=str, default=\"level70_vol_target_drawdown.csv\"\n",
    "    )\n",
    "    p.add_argument(\n",
    "        \"--json\", type=str, default=\"level70_vol_target_drawdown_summary.json\"\n",
    "    )\n",
    "    p.add_argument(\"--seed\", type=int, default=42)\n",
    "\n",
    "    a = p.parse_args()\n",
    "    symbols = tuple(s.strip() for s in a.symbols.split(\",\") if s.strip())\n",
    "\n",
    "    return Config(\n",
    "        symbols=symbols,\n",
    "        start=a.start,\n",
    "        base_mode=a.base_mode,\n",
    "        cov_lookback=a.cov_lookback,\n",
    "        min_lookback=a.min_lookback,\n",
    "        rebalance_freq=a.rebalance_freq,\n",
    "        vol_lookback=a.vol_lookback,\n",
    "        vol_target=a.vol_target,\n",
    "        lev_min=a.lev_min,\n",
    "        lev_max=a.lev_max,\n",
    "        dd_thresh=a.dd_thresh,\n",
    "        dd_max=a.dd_max,\n",
    "        dd_min_scale=a.dd_min_scale,\n",
    "        out_csv=a.csv,\n",
    "        out_json=a.json,\n",
    "        seed=a.seed,\n",
    "    )\n",
    "\n",
    "\n",
    "# --------------------------- Main ---------------------------\n",
    "\n",
    "def main() -> None:\n",
    "    cfg = parse_args()\n",
    "    np.random.seed(cfg.seed)\n",
    "\n",
    "    print(f\"[INFO] Downloading prices for {cfg.symbols} from {cfg.start} ...\")\n",
    "    out, summary = run_pipeline(cfg)\n",
    "    save_outputs(out, summary, cfg)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Jupyter / PyCharm shim\n",
    "    import sys\n",
    "\n",
    "    sys.argv = [sys.argv[0]] + [\n",
    "        arg\n",
    "        for arg in sys.argv[1:]\n",
    "        if arg != \"-f\" and not (arg.endswith(\".json\") and \"kernel\" in arg)\n",
    "    ]\n",
    "    main()\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Downloading prices for ('SPY', 'QQQ', 'IWM', 'EFA', 'EEM', 'TLT', 'LQD', 'GLD') from 2010-01-01 ...\n",
      "[OK] Saved daily series → level70_vol_target_drawdown.csv\n",
      "[OK] Saved summary → level70_vol_target_drawdown_summary.json\n",
      "Period 2010-08-31 → 2025-12-04, n_days=3840\n",
      "Base: AnnRet=5.87%, AnnVol=7.99%, Sharpe=0.73, MaxDD=-25.93%\n",
      "VolTarget_DD: AnnRet=9.38%, AnnVol=9.54%, Sharpe=0.98, MaxDD=-22.85%\n"
     ]
    }
   ],
   "execution_count": 1
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
