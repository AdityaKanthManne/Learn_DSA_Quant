{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-05T03:12:15.773410Z",
     "start_time": "2025-11-05T03:06:16.204431Z"
    }
   },
   "source": [
    "# Level-37 — Meta-labeling + Fractional Differencing + Purged CV + Platt + Kelly + Vol Targeting (1-D hardened)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "from datetime import datetime, timedelta, timezone\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# ---------------- Config ----------------\n",
    "TICKER        = \"AAPL\"\n",
    "YEARS         = 3.0\n",
    "FREQ          = \"1D\"\n",
    "\n",
    "VOL_SPAN      = 50\n",
    "H_BARS        = 10\n",
    "CUSUM_GRID    = [0.003, 0.004, 0.006, 0.008, 0.010]\n",
    "UP_M, DN_M    = 3.0, 3.0\n",
    "MIN_EVENTS    = 400\n",
    "\n",
    "BASE_SEED     = 42\n",
    "N_SPLITS      = 5\n",
    "EMBARGO_DAYS  = 5                 # for purging leakage\n",
    "TC_BP         = 5                 # round-trip cost in bp\n",
    "BAND          = 0.05\n",
    "BET_CAP       = 0.5\n",
    "KELLY_EDGE_B  = 1.0\n",
    "\n",
    "# Vol targeting\n",
    "TARGET_VOL_ANNUAL = 0.12          # 12% annual target vol\n",
    "REALVOL_WIN       = 20            # realized vol lookback\n",
    "\n",
    "# Fractional differencing\n",
    "FRAC_D            = 0.5\n",
    "FRAC_THRESH       = 1e-4\n",
    "\n",
    "np.random.seed(BASE_SEED)\n",
    "\n",
    "# --------------- 1-D Hardeners ---------------\n",
    "def to1d(a):\n",
    "    if isinstance(a, pd.Series):\n",
    "        return a.to_numpy().ravel()\n",
    "    if isinstance(a, pd.DataFrame):\n",
    "        return a.iloc[:, 0].to_numpy().ravel()\n",
    "    return np.asarray(a).ravel()\n",
    "\n",
    "def series_1d(x, index=None, dtype=None):\n",
    "    if isinstance(x, pd.DataFrame):\n",
    "        vals = x.iloc[:, 0].to_numpy()\n",
    "        idx  = x.index\n",
    "    elif isinstance(x, pd.Series):\n",
    "        vals = x.to_numpy()\n",
    "        idx  = x.index\n",
    "    else:\n",
    "        vals = np.asarray(x)\n",
    "        idx  = None\n",
    "    vals = np.ravel(vals)\n",
    "    if dtype is not None:\n",
    "        vals = vals.astype(dtype, copy=False)\n",
    "    if index is None:\n",
    "        index = idx if (idx is not None and len(idx) == len(vals)) else None\n",
    "    return pd.Series(vals, index=index)\n",
    "\n",
    "# --------------- Data & Features ---------------\n",
    "def utc_today():\n",
    "    return datetime.now(timezone.utc).date()\n",
    "\n",
    "def load_prices(ticker, years, freq=\"1D\"):\n",
    "    end = utc_today()\n",
    "    start = (datetime.now(timezone.utc) - timedelta(days=int(365*years + 20))).date()\n",
    "    df = yf.download(ticker, start=start, end=end, auto_adjust=True, progress=False)\n",
    "    if df.empty:\n",
    "        raise SystemExit(\"No data downloaded.\")\n",
    "    s = df[\"Adj Close\"] if \"Adj Close\" in df.columns else df[\"Close\"]\n",
    "    s = s.asfreq(\"B\").ffill()\n",
    "    s.name = \"Close\"\n",
    "    return s\n",
    "\n",
    "def ewma_vol(r, span=50):\n",
    "    return r.ewm(span=span, adjust=False).std()\n",
    "\n",
    "def fracdiff_weights(d, size, thresh=1e-8):\n",
    "    w = [1.0]\n",
    "    k = 1\n",
    "    while k < size:\n",
    "        w_k = -w[-1] * (d - (k - 1)) / k\n",
    "        if abs(w_k) < thresh:\n",
    "            break\n",
    "        w.append(w_k)\n",
    "        k += 1\n",
    "    return np.array(w, dtype=float)\n",
    "\n",
    "def fracdiff(s, d=0.5, thresh=1e-4):\n",
    "    s = series_1d(s, index=None, dtype=float)\n",
    "    w = fracdiff_weights(d, size=len(s), thresh=thresh)\n",
    "    out = np.full(len(s), np.nan)\n",
    "    w_rev = w[::-1]\n",
    "    k = len(w_rev)\n",
    "    for i in range(k-1, len(s)):\n",
    "        out[i] = np.dot(w_rev, s[i-k+1:i+1])\n",
    "    return pd.Series(out, index=None)\n",
    "\n",
    "def make_features(close):\n",
    "    r = close.pct_change().fillna(0.0)\n",
    "    f = pd.DataFrame(index=close.index)\n",
    "    f[\"r1\"]    = r\n",
    "    f[\"r5\"]    = close.pct_change(5)\n",
    "    f[\"r10\"]   = close.pct_change(10)\n",
    "    f[\"mom5\"]  = close/close.shift(5)  - 1\n",
    "    f[\"mom10\"] = close/close.shift(10) - 1\n",
    "    f[\"vol10\"] = r.rolling(10).std()\n",
    "    f[\"vol20\"] = r.rolling(20).std()\n",
    "    f[\"z20\"]   = (close - close.rolling(20).mean()) / (1e-12 + close.rolling(20).std())\n",
    "    d = close.diff()\n",
    "    up = d.clip(lower=0).rolling(14).mean()\n",
    "    dn = (-d.clip(upper=0)).rolling(14).mean()\n",
    "    rs = up / (1e-12 + dn)\n",
    "    f[\"rsi14\"] = 100 - 100/(1+rs)\n",
    "    # Fractional diff of price (stationary-ish)\n",
    "    fd = fracdiff(close.pct_change().fillna(0.0), d=FRAC_D, thresh=FRAC_THRESH)\n",
    "    f[\"fracdiff\"] = pd.Series(fd.values, index=f.index).fillna(0.0)\n",
    "    return f.fillna(0.0)\n",
    "\n",
    "# --------------- Labeling (CUSUM + triple barrier) ---------------\n",
    "def cusum_filter(r, threshold, index=None):\n",
    "    s = series_1d(r, index=index, dtype=float).fillna(0.0)\n",
    "    idx = s.index\n",
    "    vals = s.to_numpy()\n",
    "    s_pos = s_neg = 0.0\n",
    "    t_events = []\n",
    "    for i, x in enumerate(vals):\n",
    "        s_pos = max(0.0, s_pos + x)\n",
    "        s_neg = min(0.0, s_neg + x)\n",
    "        if s_pos > threshold:\n",
    "            s_pos = 0.0\n",
    "            t_events.append(idx[i])\n",
    "        elif s_neg < -threshold:\n",
    "            s_neg = 0.0\n",
    "            t_events.append(idx[i])\n",
    "    return pd.DatetimeIndex(t_events).unique().sort_values()\n",
    "\n",
    "def get_vertical_barriers(t_events, h, index):\n",
    "    if len(t_events) == 0:\n",
    "        return pd.Series(dtype=\"datetime64[ns]\")\n",
    "    out = {}\n",
    "    for t0 in t_events:\n",
    "        pos = index.get_indexer([t0])[0]\n",
    "        t1_pos = min(pos + h, len(index) - 1)\n",
    "        out[t0] = index[t1_pos]\n",
    "    return pd.Series(out)\n",
    "\n",
    "def get_triple_barrier_labels(close, t_events, vbar, up_m, dn_m, daily_vol):\n",
    "    trgt_raw = daily_vol.reindex(t_events).fillna(method=\"bfill\").fillna(method=\"ffill\")\n",
    "    trgt = series_1d(trgt_raw, index=t_events, dtype=float)\n",
    "\n",
    "    t1_raw = vbar.reindex(t_events)\n",
    "    t1 = pd.Series(t1_raw.values, index=t_events)\n",
    "\n",
    "    rows = []\n",
    "    for t0 in t_events:\n",
    "        t1_i = t1.loc[t0]\n",
    "        if pd.isna(t1_i):\n",
    "            continue\n",
    "        try:\n",
    "            c0 = float(close.loc[t0])\n",
    "        except Exception:\n",
    "            continue\n",
    "        up_lvl = c0 * (1 + up_m * float(trgt.loc[t0]))\n",
    "        dn_lvl = c0 * (1 - dn_m * float(trgt.loc[t0]))\n",
    "\n",
    "        seg = close.loc[t0:t1_i]\n",
    "        if isinstance(seg, pd.DataFrame):\n",
    "            seg = seg.iloc[:, 0]\n",
    "        path = series_1d(seg, index=seg.index, dtype=float)\n",
    "        if path.empty:\n",
    "            continue\n",
    "\n",
    "        path_up = path >= up_lvl\n",
    "        path_dn = path <= dn_lvl\n",
    "        hit_up = path_up.idxmax() if path_up.to_numpy().any() else None\n",
    "        hit_dn = path_dn.idxmax() if path_dn.to_numpy().any() else None\n",
    "\n",
    "        if (hit_up is not None) and (hit_dn is not None):\n",
    "            lbl = 1 if hit_up <= hit_dn else 0\n",
    "            t_end = hit_up if lbl == 1 else hit_dn\n",
    "        elif hit_up is not None:\n",
    "            lbl, t_end = 1, hit_up\n",
    "        elif hit_dn is not None:\n",
    "            lbl, t_end = 0, hit_dn\n",
    "        else:\n",
    "            c1 = float(path.iloc[-1])\n",
    "            lbl, t_end = (1 if c1 > c0 else 0), t1_i\n",
    "\n",
    "        rows.append((t0, t_end, lbl, float(trgt.loc[t0])))\n",
    "\n",
    "    if not rows:\n",
    "        return pd.DataFrame(columns=[\"t1\", \"label\", \"trgt\"])\n",
    "    df = pd.DataFrame(rows, columns=[\"t0\", \"t1\", \"label\", \"trgt\"]).set_index(\"t0\")\n",
    "    return df\n",
    "\n",
    "def adaptive_events_and_labels(close, rets, base_H, cusum_grid, vol_span, up_m, dn_m, min_events):\n",
    "    daily_vol = ewma_vol(rets, span=vol_span).clip(lower=1e-8)\n",
    "\n",
    "    used_thr = None\n",
    "    events_idx = pd.DatetimeIndex([])\n",
    "    for thr in cusum_grid:\n",
    "        ev = cusum_filter(rets, thr, index=close.index)\n",
    "        if len(ev) >= min_events:\n",
    "            used_thr, events_idx = thr, ev\n",
    "            break\n",
    "    if used_thr is None:\n",
    "        used_thr = min(cusum_grid)\n",
    "        events_idx = cusum_filter(rets, used_thr, index=close.index)\n",
    "\n",
    "    vbar = get_vertical_barriers(events_idx, base_H, close.index)\n",
    "    labels = get_triple_barrier_labels(close, events_idx, vbar, up_m, dn_m, daily_vol).dropna()\n",
    "    if labels.empty:\n",
    "        raise SystemExit(\"No labeled events. Adjust thresholds.\")\n",
    "    print(f\"[Adaptive] events={len(labels)}  H={base_H} thr≈{used_thr}\")\n",
    "    return labels, daily_vol, events_idx, base_H, float(used_thr)\n",
    "\n",
    "# --------------- Purged Forward CV Splits ---------------\n",
    "def purged_forward_splits(index, n_splits=5, embargo_days=5):\n",
    "    dates = pd.DatetimeIndex(index)\n",
    "    n = len(dates)\n",
    "    fold_sizes = np.full(n_splits, n // n_splits, dtype=int)\n",
    "    fold_sizes[: n % n_splits] += 1\n",
    "    starts = np.cumsum(np.concatenate(([0], fold_sizes[:-1])))\n",
    "    ends   = np.cumsum(fold_sizes)\n",
    "    embargo = pd.Timedelta(days=embargo_days)\n",
    "    for k in range(n_splits):\n",
    "        va_start, va_end = starts[k], ends[k]\n",
    "        va_idx = np.arange(va_start, va_end)\n",
    "        va_dates = dates[va_idx]\n",
    "        cutoff = va_dates[0] - embargo\n",
    "        tr_idx = np.where(dates < cutoff)[0]\n",
    "        yield tr_idx, va_idx\n",
    "\n",
    "# --------------- Platt (logistic-on-logit) ---------------\n",
    "def platt_fit(y_true, raw_prob):\n",
    "    eps = 1e-6\n",
    "    z = np.clip(to1d(raw_prob), eps, 1 - eps)\n",
    "    logit = np.log(z / (1 - z)).reshape(-1, 1)\n",
    "    lr = LogisticRegression(solver=\"lbfgs\", max_iter=1000, random_state=BASE_SEED)\n",
    "    lr.fit(logit, to1d(y_true).astype(int))\n",
    "    return lr\n",
    "\n",
    "def platt_predict(lr, raw_prob):\n",
    "    eps = 1e-6\n",
    "    z = np.clip(to1d(raw_prob), eps, 1 - eps)\n",
    "    logit = np.log(z / (1 - z)).reshape(-1, 1)\n",
    "    return lr.predict_proba(logit)[:, 1]\n",
    "\n",
    "# --------------- Metrics ---------------\n",
    "def sharpe_ratio(x):\n",
    "    x = series_1d(x)\n",
    "    s = x.std()\n",
    "    return 0.0 if s == 0 or np.isnan(s) else float(np.sqrt(252) * x.mean() / s)\n",
    "\n",
    "def drawdown(x):\n",
    "    x = series_1d(x)\n",
    "    cum = (1 + x).cumprod()\n",
    "    return float((cum / cum.cummax() - 1).min())\n",
    "\n",
    "# --------------- Bet sizing ---------------\n",
    "def kelly_fraction(p, b=1.0):\n",
    "    p = to1d(p)\n",
    "    f = (p * (b + 1.0) - 1.0) / b  # b=1 -> f=2p-1\n",
    "    return np.clip(f, -BET_CAP, BET_CAP)\n",
    "\n",
    "def apply_rebalance_band(target, band=BAND):\n",
    "    t = to1d(target)\n",
    "    pos = 0.0\n",
    "    out = np.zeros_like(t, dtype=float)\n",
    "    for i, v in enumerate(t):\n",
    "        if abs(v - pos) > band:\n",
    "            pos = v\n",
    "        out[i] = pos\n",
    "    return out\n",
    "\n",
    "# --------------- Vol targeting helper ---------------\n",
    "def realized_vol_daily(returns, win=20):\n",
    "    r = series_1d(returns)\n",
    "    sig_d = r.rolling(win).std().fillna(method=\"bfill\").fillna(0.0)\n",
    "    return sig_d\n",
    "\n",
    "# --------------- Main ---------------\n",
    "if __name__ == \"__main__\":\n",
    "    # 1) Data\n",
    "    close = load_prices(TICKER, YEARS, FREQ)\n",
    "    rets  = close.pct_change().replace([np.inf, -np.inf], 0.0).fillna(0.0)\n",
    "\n",
    "    # 2) Events & Labels\n",
    "    labels, _, _, _, _ = adaptive_events_and_labels(\n",
    "        close, rets, H_BARS, CUSUM_GRID, VOL_SPAN, UP_M, DN_M, MIN_EVENTS\n",
    "    )\n",
    "\n",
    "    # 3) Features on event times\n",
    "    feats = make_features(close)\n",
    "    X = feats.reindex(labels.index).dropna()\n",
    "    labels = labels.loc[X.index]\n",
    "    y_side = labels[\"label\"].astype(int)  # primary side label (1 up / 0 down)\n",
    "\n",
    "    # 4) Scale\n",
    "    scaler = StandardScaler()\n",
    "    Xs = scaler.fit_transform(X)\n",
    "    feat_names = list(X.columns)\n",
    "\n",
    "    # 5) Primary model (side) — Purged CV, Platt, threshold by Sharpe on val\n",
    "    proba_side_cv = np.full(len(X), np.nan)\n",
    "    aucs, thrs = [], []\n",
    "    last_fold_primary = None\n",
    "    for tr_idx, va_idx in purged_forward_splits(X.index, n_splits=N_SPLITS, embargo_days=EMBARGO_DAYS):\n",
    "        if len(tr_idx) < 50 or len(va_idx) < 20:\n",
    "            continue\n",
    "        X_tr, y_tr = Xs[tr_idx], to1d(y_side.iloc[tr_idx])\n",
    "        X_va, y_va = Xs[va_idx], to1d(y_side.iloc[va_idx])\n",
    "\n",
    "        base = GradientBoostingClassifier(random_state=BASE_SEED)\n",
    "        base.fit(X_tr, y_tr)\n",
    "        p_va_raw = base.predict_proba(X_va)[:, 1]\n",
    "        pl = platt_fit(y_va, p_va_raw)\n",
    "        p_va = platt_predict(pl, p_va_raw)\n",
    "        proba_side_cv[va_idx] = p_va\n",
    "        aucs.append(roc_auc_score(y_va, p_va))\n",
    "\n",
    "        # pick threshold on Sharpe of val pnl (long if p>=thr else short)\n",
    "        r_next_va = to1d(close.pct_change().shift(-1).reindex(X.index[va_idx]).fillna(0.0))\n",
    "        grid = np.linspace(0.25, 0.75, 21)\n",
    "        best_thr, best_score = 0.5, -1e9\n",
    "        for thr in grid:\n",
    "            side = np.where(p_va >= thr, 1.0, -1.0)\n",
    "            pnl  = side * r_next_va\n",
    "            score = pnl.mean() / (pnl.std() + 1e-9)\n",
    "            if score > best_score:\n",
    "                best_score, best_thr = score, thr\n",
    "        thrs.append(best_thr)\n",
    "        last_fold_primary = (base, X_va, y_va)\n",
    "\n",
    "    cv_auc = float(np.nanmean(aucs)) if len(aucs) else np.nan\n",
    "    thr_side = float(np.nanmedian(thrs)) if len(thrs) else 0.5\n",
    "    print(f\"\\n[Primary] CV AUC: {cv_auc:.3f}  Side threshold: {thr_side:.2f}\")\n",
    "\n",
    "    # Rolling forward proba for primary\n",
    "    base_final = GradientBoostingClassifier(random_state=BASE_SEED)\n",
    "    proba_side_all = np.zeros(len(X), dtype=float)\n",
    "    min_fit = max(100, len(X)//N_SPLITS)\n",
    "    for i in range(min_fit, len(X)):\n",
    "        X_tr, y_tr = Xs[:i], to1d(y_side.iloc[:i])\n",
    "        base_final.fit(X_tr, y_tr)\n",
    "        raw = base_final.predict_proba(Xs[i:i+1])[:, 1]\n",
    "        # rolling Platt calibration\n",
    "        j0 = max(0, i - 250)\n",
    "        p_tr_raw = base_final.predict_proba(Xs[j0:i])[:, 1]\n",
    "        pl = platt_fit(y_side.iloc[j0:i], p_tr_raw)\n",
    "        proba_side_all[i] = platt_predict(pl, raw)[0]\n",
    "\n",
    "    # Primary side signal (+1 / -1)\n",
    "    side_signal = np.where(proba_side_all >= thr_side, 1.0, -1.0)\n",
    "\n",
    "    # 6) Meta-labeling: learn a filter on primary trades\n",
    "    # Meta label = 1 if primary side * next return > 0 (trade would make money), else 0\n",
    "    ret_next_full = close.pct_change().shift(-1)\n",
    "    r_next = to1d(ret_next_full.reindex(X.index).fillna(0.0))\n",
    "    side_series = series_1d(side_signal, index=X.index)\n",
    "    meta_y = (side_series.values * r_next > 0).astype(int)\n",
    "\n",
    "    # Train meta on same features; target = meta_y\n",
    "    proba_meta_cv = np.full(len(X), np.nan)\n",
    "    aucs_m, thrs_m = [], []\n",
    "    last_fold_meta = None\n",
    "    for tr_idx, va_idx in purged_forward_splits(X.index, n_splits=N_SPLITS, embargo_days=EMBARGO_DAYS):\n",
    "        if len(tr_idx) < 50 or len(va_idx) < 20:\n",
    "            continue\n",
    "        X_tr, y_tr = Xs[tr_idx], meta_y[tr_idx]\n",
    "        X_va, y_va = Xs[va_idx], meta_y[va_idx]\n",
    "\n",
    "        meta = GradientBoostingClassifier(random_state=BASE_SEED)\n",
    "        meta.fit(X_tr, y_tr)\n",
    "        p_va_raw = meta.predict_proba(X_va)[:, 1]\n",
    "        pl = platt_fit(y_va, p_va_raw)\n",
    "        p_va = platt_predict(pl, p_va_raw)\n",
    "        proba_meta_cv[va_idx] = p_va\n",
    "        try:\n",
    "            aucs_m.append(roc_auc_score(y_va, p_va))\n",
    "        except ValueError:\n",
    "            aucs_m.append(np.nan)\n",
    "\n",
    "        # threshold for meta filter — maximize Sharpe on val when *applied* to primary side\n",
    "        grid = np.linspace(0.3, 0.7, 17)\n",
    "        best_thr, best_score = 0.5, -1e9\n",
    "        # need side signal on val slice indices (use realized proba_side_all; if too early use proba_side_cv fallback)\n",
    "        p_side_slice = proba_side_cv[va_idx]\n",
    "        # backfill any NaN with 0.5\n",
    "        p_side_slice = np.where(np.isnan(p_side_slice), 0.5, p_side_slice)\n",
    "        side_slice = np.where(p_side_slice >= thr_side, 1.0, -1.0)\n",
    "        r_next_va = r_next[va_idx]\n",
    "        for thr in grid:\n",
    "            take = (p_va >= thr).astype(float)\n",
    "            pos  = side_slice * take\n",
    "            pnl  = pos * r_next_va\n",
    "            score = pnl.mean() / (pnl.std() + 1e-9)\n",
    "            if score > best_score:\n",
    "                best_score, best_thr = score, thr\n",
    "        thrs_m.append(best_thr)\n",
    "        last_fold_meta = (meta, X_va, y_va)\n",
    "\n",
    "    cv_auc_m = float(np.nanmean(aucs_m)) if len(aucs_m) else np.nan\n",
    "    thr_meta = float(np.nanmedian(thrs_m)) if len(thrs_m) else 0.5\n",
    "    print(f\"[Meta]    CV AUC: {cv_auc_m:.3f}  Meta threshold: {thr_meta:.2f}\")\n",
    "\n",
    "    # Rolling meta proba\n",
    "    meta_final = GradientBoostingClassifier(random_state=BASE_SEED)\n",
    "    proba_meta_all = np.zeros(len(X), dtype=float)\n",
    "    for i in range(min_fit, len(X)):\n",
    "        X_tr, y_tr = Xs[:i], meta_y[:i]\n",
    "        meta_final.fit(X_tr, y_tr)\n",
    "        raw = meta_final.predict_proba(Xs[i:i+1])[:, 1]\n",
    "        j0 = max(0, i - 250)\n",
    "        p_tr_raw = meta_final.predict_proba(Xs[j0:i])[:, 1]\n",
    "        pl = platt_fit(meta_y[j0:i], p_tr_raw)\n",
    "        proba_meta_all[i] = platt_predict(pl, raw)[0]\n",
    "\n",
    "    # 7) Position construction: primary side * Kelly(meta proba), with band + vol targeting\n",
    "    kelly_raw = kelly_fraction(proba_meta_all, b=KELLY_EDGE_B)  # signed later by side\n",
    "    kelly_pos_unsigned = apply_rebalance_band(kelly_raw, band=BAND)\n",
    "    # Apply meta gate: if meta proba < thr_meta, zero position\n",
    "    meta_gate = (proba_meta_all >= thr_meta).astype(float)\n",
    "    # Primary direction:\n",
    "    side_all = side_signal\n",
    "    # Combine:\n",
    "    target_pos = side_all * kelly_pos_unsigned * meta_gate\n",
    "\n",
    "    # Vol targeting on top\n",
    "    target_daily = TARGET_VOL_ANNUAL / np.sqrt(252.0)\n",
    "    realized_d = realized_vol_daily(ret_next_full, win=REALVOL_WIN).reindex(X.index).fillna(method=\"bfill\").fillna(0.0)\n",
    "    scale = (target_daily / (realized_d.replace(0.0, np.nan))).clip(upper=3.0).fillna(0.0)\n",
    "    pos_vol = series_1d(target_pos, index=X.index) * series_1d(scale, index=X.index)\n",
    "    pos_vol = np.clip(pos_vol, -BET_CAP, BET_CAP).to_numpy()\n",
    "\n",
    "    # 8) Backtest — net PnL with tc, stats\n",
    "    m = min(len(pos_vol), len(r_next))\n",
    "    pos = to1d(pos_vol[:m])\n",
    "    ret = to1d(r_next[:m])\n",
    "\n",
    "    pnl_gross = pos * ret\n",
    "    toggle    = np.abs(np.diff(pos, prepend=0.0))\n",
    "    tc        = toggle * (TC_BP / 1e4)\n",
    "    pnl_net   = pnl_gross - tc\n",
    "\n",
    "    pnl_series = series_1d(pnl_net, index=X.index[:m])\n",
    "    sharpe = sharpe_ratio(pnl_series)\n",
    "    cum = (1.0 + pnl_series).cumprod()\n",
    "    yrs = (cum.index[-1] - cum.index[0]).days / 365.25 if isinstance(cum.index, pd.DatetimeIndex) else len(cum)/252.0\n",
    "    cagr = float(cum.iloc[-1] ** (1.0 / max(1e-9, yrs)) - 1.0)\n",
    "    dd   = drawdown(pnl_series)\n",
    "\n",
    "    long_days = (pos > 0).astype(float)\n",
    "    hit = float(((long_days > 0) & (ret > 0)).sum() / max(1, (long_days > 0).sum()))\n",
    "\n",
    "    print(\"\\n=== Level-37 — Meta-labeling + FracDiff ===\")\n",
    "    print(f\"Sharpe: {sharpe:.3f}  CAGR: {100*cagr:.2f}%  MaxDD: {100*dd:.2f}%  Hit%: {100*hit:.2f}%\")\n",
    "    print(f\"Avg daily turnover: {toggle.mean():.3f}   Avg TC paid (bp): {(tc * 1e4).mean():.2f}\")\n",
    "\n",
    "    # --- Diagnostics: deciles of meta proba vs forward return (gated by meta)\n",
    "    df_diag = pd.DataFrame(\n",
    "        {\n",
    "            \"proba_meta\": proba_meta_all[:m],\n",
    "            \"ret_next\": ret[:m],\n",
    "            \"gate\": meta_gate[:m],\n",
    "            \"side\": side_all[:m]\n",
    "        },\n",
    "        index=X.index[:m],\n",
    "    ).dropna()\n",
    "    df_diag[\"bucket\"] = pd.qcut(df_diag[\"proba_meta\"].rank(method=\"first\"), 10, labels=False)\n",
    "    decile_stats = df_diag.groupby(\"bucket\")[\"ret_next\"].agg([\"mean\",\"std\",\"count\"])\n",
    "    decile_stats.index.name = \"meta_prob_decile\"\n",
    "\n",
    "    # --- Permutation importance (safe: on last CV fold validation)\n",
    "    perm_side = pd.DataFrame(columns=[\"feature\", \"mean_importance\", \"std_importance\"])\n",
    "    if last_fold_primary is not None:\n",
    "        base_last, X_va_last, y_va_last = last_fold_primary\n",
    "        from sklearn.metrics import roc_auc_score as _auc\n",
    "        class _AUCWrap:\n",
    "            def __init__(self, est): self.est = est\n",
    "            def fit(self, *a, **k): return self.est.fit(*a, **k)\n",
    "            def predict_proba(self, X): return self.est.predict_proba(X)\n",
    "            def score(self, X, y):\n",
    "                p = self.est.predict_proba(X)[:,1]\n",
    "                try: return _auc(y, p)\n",
    "                except ValueError: return 0.5\n",
    "        wrapped = _AUCWrap(base_last)\n",
    "        perm = permutation_importance(wrapped, X_va_last, y_va_last, n_repeats=20, random_state=BASE_SEED, n_jobs=1)\n",
    "        perm_side = pd.DataFrame({\n",
    "            \"feature\": feat_names,\n",
    "            \"mean_importance\": perm.importances_mean,\n",
    "            \"std_importance\": perm.importances_std\n",
    "        }).sort_values(\"mean_importance\", ascending=False)\n",
    "\n",
    "    perm_meta = pd.DataFrame(columns=[\"feature\", \"mean_importance\", \"std_importance\"])\n",
    "    if last_fold_meta is not None:\n",
    "        meta_last, X_va_last_m, y_va_last_m = last_fold_meta\n",
    "        from sklearn.metrics import roc_auc_score as _auc\n",
    "        class _AUCWrapM:\n",
    "            def __init__(self, est): self.est = est\n",
    "            def fit(self, *a, **k): return self.est.fit(*a, **k)\n",
    "            def predict_proba(self, X): return self.est.predict_proba(X)\n",
    "            def score(self, X, y):\n",
    "                p = self.est.predict_proba(X)[:,1]\n",
    "                try: return _auc(y, p)\n",
    "                except ValueError: return 0.5\n",
    "        wrapped_m = _AUCWrapM(meta_last)\n",
    "        perm_m = permutation_importance(wrapped_m, X_va_last_m, y_va_last_m, n_repeats=20, random_state=BASE_SEED, n_jobs=1)\n",
    "        perm_meta = pd.DataFrame({\n",
    "            \"feature\": feat_names,\n",
    "            \"mean_importance\": perm_m.importances_mean,\n",
    "            \"std_importance\": perm_m.importances_std\n",
    "        }).sort_values(\"mean_importance\", ascending=False)\n",
    "\n",
    "    # --- Save outputs\n",
    "    out_ts = pd.DataFrame(\n",
    "        {\n",
    "            \"proba_side\": proba_side_all[:m],\n",
    "            \"proba_meta\": proba_meta_all[:m],\n",
    "            \"side\": side_all[:m],\n",
    "            \"meta_gate\": meta_gate[:m],\n",
    "            \"kelly_unsigned\": kelly_fraction(proba_meta_all[:m], b=KELLY_EDGE_B),\n",
    "            \"pos_voltgt\": pos[:m],\n",
    "            \"ret_next\": ret[:m],\n",
    "            \"pnl_net\": pnl_net[:m],\n",
    "        },\n",
    "        index=X.index[:m],\n",
    "    )\n",
    "    ts_file = f\"{TICKER}_level37_timeseries.csv\"\n",
    "    out_ts.to_csv(ts_file)\n",
    "\n",
    "    dec_file = f\"{TICKER}_level37_deciles.csv\"\n",
    "    decile_stats.to_csv(dec_file)\n",
    "\n",
    "    equity = (1.0 + pnl_series).cumprod()\n",
    "    eq_df = pd.DataFrame({\"equity\": equity.values}, index=equity.index)\n",
    "    eq_file = f\"{TICKER}_level37_equity.csv\"\n",
    "    eq_df.to_csv(eq_file)\n",
    "\n",
    "    if not perm_side.empty:\n",
    "        pi_side_file = f\"{TICKER}_level37_perm_importance_primary.csv\"\n",
    "        perm_side.to_csv(pi_side_file, index=False)\n",
    "    if not perm_meta.empty:\n",
    "        pi_meta_file = f\"{TICKER}_level37_perm_importance_meta.csv\"\n",
    "        perm_meta.to_csv(pi_meta_file, index=False)\n",
    "\n",
    "    saved = [ts_file, dec_file, eq_file]\n",
    "    if not perm_side.empty: saved.append(pi_side_file)\n",
    "    if not perm_meta.empty: saved.append(pi_meta_file)\n",
    "    print(\"\\nSaved:\", \", \".join(saved))\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Adaptive] events=627  H=10 thr≈0.003\n",
      "\n",
      "[Primary] CV AUC: 0.593  Side threshold: 0.52\n",
      "[Meta]    CV AUC: 0.558  Meta threshold: 0.55\n",
      "\n",
      "=== Level-37 — Meta-labeling + FracDiff ===\n",
      "Sharpe: 0.568  CAGR: 1.73%  MaxDD: -4.61%  Hit%: 53.50%\n",
      "Avg daily turnover: 0.154   Avg TC paid (bp): 0.77\n",
      "\n",
      "Saved: AAPL_level37_timeseries.csv, AAPL_level37_deciles.csv, AAPL_level37_equity.csv, AAPL_level37_perm_importance_primary.csv, AAPL_level37_perm_importance_meta.csv\n"
     ]
    }
   ],
   "execution_count": 1
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
