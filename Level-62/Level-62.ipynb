{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-05T01:15:47.168202Z",
     "start_time": "2025-12-05T01:15:41.763395Z"
    }
   },
   "source": [
    "# level62_shrinkage_riskparity.py\n",
    "# Level-62: Shrinkage Min-Var + Risk-Parity Blended Portfolio (with vol targeting)\n",
    "#\n",
    "# Idea:\n",
    "#   - Multi-asset ETF universe (SPY, QQQ, IWM, EFA, EEM, TLT, LQD, GLD).\n",
    "#   - Use Ledoit–Wolf shrinkage covariance on a rolling window.\n",
    "#   - Build:\n",
    "#       * Risk-parity (inverse-vol) weights.\n",
    "#       * Minimum-variance weights from shrinkage covariance.\n",
    "#   - Blend the two based on average pairwise correlation:\n",
    "#       * Low correlation → tilt toward risk-parity.\n",
    "#       * High correlation → tilt toward min-var.\n",
    "#   - Scale the blended weights to reach a target annualized volatility\n",
    "#     with a leverage cap, and include a simple transaction cost model.\n",
    "#\n",
    "# Outputs:\n",
    "#   - level62_shrinkage_riskparity.csv\n",
    "#   - level62_shrinkage_riskparity_summary.json\n",
    "#\n",
    "# Requirements:\n",
    "#   - numpy, pandas, yfinance, scikit-learn\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "from sklearn.covariance import LedoitWolf\n",
    "\n",
    "\n",
    "# ----------------------------- Config -----------------------------\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    symbols: Tuple[str, ...] = (\n",
    "        \"SPY\", \"QQQ\", \"IWM\", \"EFA\", \"EEM\", \"TLT\", \"LQD\", \"GLD\"\n",
    "    )\n",
    "    start: str = \"2010-01-01\"\n",
    "    vol_lookback: int = 60       # days used for cov/vol estimation\n",
    "    rebalance_freq: str = \"M\"    # \"M\" (month-end), \"W-FRI\", etc.\n",
    "    vol_target: float = 0.10     # target annualized volatility\n",
    "    max_leverage: float = 2.0    # cap on leverage multiplier\n",
    "    corr_min: float = 0.1        # corr below this → almost all risk-parity\n",
    "    corr_max: float = 0.8        # corr above this → almost all min-var\n",
    "    tc_bps: float = 10.0         # round-trip transaction cost in bps\n",
    "    out_csv: str = \"level62_shrinkage_riskparity.csv\"\n",
    "    out_json: str = \"level62_shrinkage_riskparity_summary.json\"\n",
    "    seed: int = 42\n",
    "\n",
    "\n",
    "# ----------------------------- Data Loading -----------------------------\n",
    "\n",
    "\n",
    "def load_prices(cfg: Config) -> pd.DataFrame:\n",
    "    tickers = list(cfg.symbols)\n",
    "    px = yf.download(\n",
    "        tickers,\n",
    "        start=cfg.start,\n",
    "        auto_adjust=True,\n",
    "        progress=False,\n",
    "    )\n",
    "    if px.empty:\n",
    "        raise RuntimeError(\"No data downloaded. Check tickers or connection.\")\n",
    "\n",
    "    if isinstance(px.columns, pd.MultiIndex):\n",
    "        if \"Close\" not in px.columns.levels[0]:\n",
    "            raise RuntimeError(\"Expected 'Close' in yfinance output.\")\n",
    "        close = px[\"Close\"].copy()\n",
    "    else:\n",
    "        close = px[[\"Close\"]].copy()\n",
    "        close.columns = tickers[:1]\n",
    "\n",
    "    close = close.dropna(how=\"all\")\n",
    "    close = close[tickers].dropna(how=\"any\")\n",
    "    return close\n",
    "\n",
    "\n",
    "def compute_returns(close: pd.DataFrame) -> pd.DataFrame:\n",
    "    return close.pct_change().dropna()\n",
    "\n",
    "\n",
    "# ----------------------------- Helpers -----------------------------\n",
    "\n",
    "\n",
    "def annualize_vol(daily_vol: float) -> float:\n",
    "    return float(daily_vol * np.sqrt(252.0))\n",
    "\n",
    "\n",
    "def portfolio_vol(weights: np.ndarray, cov: np.ndarray) -> float:\n",
    "    return float(np.sqrt(weights @ cov @ weights))\n",
    "\n",
    "\n",
    "def inverse_vol_weights(vol_vec: pd.Series) -> pd.Series:\n",
    "    vol_vec = vol_vec.replace(0.0, np.nan)\n",
    "    inv = 1.0 / vol_vec\n",
    "    inv = inv.replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
    "    if inv.sum() <= 0:\n",
    "        return pd.Series(1.0 / len(inv), index=inv.index)\n",
    "    w = inv / inv.sum()\n",
    "    return w\n",
    "\n",
    "\n",
    "def min_var_weights_shrinkage(\n",
    "    window: pd.DataFrame,\n",
    ") -> pd.Series:\n",
    "    \"\"\"\n",
    "    Minimum-variance weights using Ledoit–Wolf covariance, with a simple\n",
    "    long-only heuristic: start from w ~ Σ^{-1} 1, clip negatives to zero,\n",
    "    renormalize. If all zero, fall back to equal weights.\n",
    "    \"\"\"\n",
    "    cols = window.columns\n",
    "    lw = LedoitWolf()\n",
    "    lw.fit(window.values)\n",
    "    cov = lw.covariance_\n",
    "    ones = np.ones(len(cols))\n",
    "\n",
    "    try:\n",
    "        inv_cov = np.linalg.inv(cov)\n",
    "    except np.linalg.LinAlgError:\n",
    "        # Fallback to diagonal if inversion fails\n",
    "        diag = np.diag(np.diag(cov))\n",
    "        inv_cov = np.linalg.inv(diag)\n",
    "\n",
    "    raw = inv_cov @ ones\n",
    "    if raw.sum() == 0:\n",
    "        w = np.ones_like(raw) / len(raw)\n",
    "    else:\n",
    "        w = raw / raw.sum()\n",
    "\n",
    "    # long-only heuristic\n",
    "    w = np.where(w < 0.0, 0.0, w)\n",
    "    s = w.sum()\n",
    "    if s <= 0.0:\n",
    "        w = np.ones_like(w) / len(w)\n",
    "    else:\n",
    "        w = w / s\n",
    "\n",
    "    return pd.Series(w, index=cols)\n",
    "\n",
    "\n",
    "def average_offdiag_corr(window: pd.DataFrame) -> float:\n",
    "    corr = window.corr()\n",
    "    n = corr.shape[0]\n",
    "    if n <= 1:\n",
    "        return 0.0\n",
    "    mask = ~np.eye(n, dtype=bool)\n",
    "    vals = corr.values[mask]\n",
    "    vals = vals[~np.isnan(vals)]\n",
    "    if vals.size == 0:\n",
    "        return 0.0\n",
    "    return float(vals.mean())\n",
    "\n",
    "\n",
    "def blend_weight(alpha: float, w_rp: pd.Series, w_mv: pd.Series) -> pd.Series:\n",
    "    w = (1.0 - alpha) * w_rp + alpha * w_mv\n",
    "    s = float(w.sum())\n",
    "    if s != 0.0:\n",
    "        w = w / s\n",
    "    return w\n",
    "\n",
    "\n",
    "def map_corr_to_alpha(corr: float, cmin: float, cmax: float) -> float:\n",
    "    \"\"\"\n",
    "    Map average correlation into [0, 1] for blending:\n",
    "      corr <= cmin → alpha = 0 (all risk-parity)\n",
    "      corr >= cmax → alpha = 1 (all min-var)\n",
    "      linear in-between\n",
    "    \"\"\"\n",
    "    if corr <= cmin:\n",
    "        return 0.0\n",
    "    if corr >= cmax:\n",
    "        return 1.0\n",
    "    return float((corr - cmin) / (cmax - cmin))\n",
    "\n",
    "\n",
    "# ----------------------------- Rebalance Schedule -----------------------------\n",
    "\n",
    "\n",
    "def build_rebalance_schedule(rets: pd.DataFrame, cfg: Config) -> pd.DatetimeIndex:\n",
    "    if cfg.rebalance_freq.upper().startswith(\"M\"):\n",
    "        idx = rets.resample(\"ME\").last().index\n",
    "    else:\n",
    "        idx = rets.resample(cfg.rebalance_freq).last().index\n",
    "    idx = idx[idx >= rets.index[0]]\n",
    "    return idx\n",
    "\n",
    "\n",
    "# ----------------------------- Weight Construction -----------------------------\n",
    "\n",
    "\n",
    "def compute_weights(rets: pd.DataFrame, cfg: Config):\n",
    "    rebal_dates = build_rebalance_schedule(rets, cfg)\n",
    "    all_dates = rets.index\n",
    "\n",
    "    weight_records = []\n",
    "\n",
    "    for dt in rebal_dates:\n",
    "        window = rets.loc[:dt].tail(cfg.vol_lookback)\n",
    "        if window.shape[0] < cfg.vol_lookback // 2:\n",
    "            continue\n",
    "\n",
    "        vol_daily = window.std()\n",
    "        vol_ann = vol_daily * np.sqrt(252.0)\n",
    "\n",
    "        w_rp = inverse_vol_weights(vol_ann)\n",
    "        w_rp = w_rp.reindex(rets.columns).fillna(0.0)\n",
    "\n",
    "        w_mv = min_var_weights_shrinkage(window)\n",
    "        w_mv = w_mv.reindex(rets.columns).fillna(0.0)\n",
    "\n",
    "        avg_corr = average_offdiag_corr(window)\n",
    "        alpha = map_corr_to_alpha(avg_corr, cfg.corr_min, cfg.corr_max)\n",
    "\n",
    "        w_blend = blend_weight(alpha, w_rp, w_mv)\n",
    "\n",
    "        # Vol targeting via leverage\n",
    "        lw = LedoitWolf()\n",
    "        lw.fit(window.values)\n",
    "        cov_ann = lw.covariance_ * 252.0\n",
    "        w_vec = w_blend.values\n",
    "        try:\n",
    "            port_vol = portfolio_vol(w_vec, cov_ann)\n",
    "        except Exception:\n",
    "            port_vol = 0.0\n",
    "\n",
    "        if port_vol > 0:\n",
    "            lev = cfg.vol_target / port_vol\n",
    "        else:\n",
    "            lev = 0.0\n",
    "        lev = float(min(cfg.max_leverage, max(0.0, lev)))\n",
    "\n",
    "        w_final = w_blend * lev\n",
    "\n",
    "        rec = {\n",
    "            \"date\": dt,\n",
    "            \"avg_corr\": float(avg_corr),\n",
    "            \"alpha_minvar\": float(alpha),\n",
    "            \"port_vol_ann_pre_scale\": float(port_vol),\n",
    "            \"leverage\": float(lev),\n",
    "        }\n",
    "        for col in rets.columns:\n",
    "            rec[f\"w_{col}\"] = float(w_final.get(col, 0.0))\n",
    "\n",
    "        weight_records.append(rec)\n",
    "\n",
    "    if not weight_records:\n",
    "        raise RuntimeError(\"No weights computed. Check start date or lookback.\")\n",
    "\n",
    "    weights_df = pd.DataFrame(weight_records).set_index(\"date\")\n",
    "\n",
    "    w_cols = [c for c in weights_df.columns if c.startswith(\"w_\")]\n",
    "    meta_cols = [c for c in weights_df.columns if c not in w_cols]\n",
    "\n",
    "    weights_daily = (\n",
    "        weights_df[w_cols]\n",
    "        .reindex(all_dates)\n",
    "        .ffill()\n",
    "        .fillna(0.0)\n",
    "    )\n",
    "    meta_daily = (\n",
    "        weights_df[meta_cols]\n",
    "        .reindex(all_dates)\n",
    "        .ffill()\n",
    "    )\n",
    "\n",
    "    return weights_daily, meta_daily\n",
    "\n",
    "\n",
    "# ----------------------------- Backtest -----------------------------\n",
    "\n",
    "\n",
    "def backtest(\n",
    "    rets: pd.DataFrame,\n",
    "    weights_daily: pd.DataFrame,\n",
    "    meta_daily: pd.DataFrame,\n",
    "    cfg: Config,\n",
    ") -> pd.DataFrame:\n",
    "    common_idx = rets.index.intersection(weights_daily.index)\n",
    "    rets = rets.loc[common_idx]\n",
    "    weights_daily = weights_daily.loc[common_idx]\n",
    "    meta_daily = meta_daily.loc[common_idx]\n",
    "\n",
    "    w_cols = [c for c in weights_daily.columns if c.startswith(\"w_\")]\n",
    "    asset_cols = [c.replace(\"w_\", \"\") for c in w_cols]\n",
    "\n",
    "    W = weights_daily[w_cols].copy()\n",
    "    W.columns = asset_cols\n",
    "    W = W.reindex(columns=rets.columns).fillna(0.0)\n",
    "\n",
    "    port_ret = (W * rets).sum(axis=1)\n",
    "\n",
    "    turnover = W.diff().abs().sum(axis=1).fillna(0.0)\n",
    "    tc_per_unit = cfg.tc_bps * 1e-4 / 2.0\n",
    "    cost = turnover * tc_per_unit\n",
    "\n",
    "    port_ret_tc = port_ret - cost\n",
    "\n",
    "    equity = (1.0 + port_ret).cumprod()\n",
    "    equity_tc = (1.0 + port_ret_tc).cumprod()\n",
    "\n",
    "    out = pd.DataFrame(index=common_idx)\n",
    "    out[\"port_ret\"] = port_ret\n",
    "    out[\"port_ret_tc\"] = port_ret_tc\n",
    "    out[\"equity\"] = equity\n",
    "    out[\"equity_tc\"] = equity_tc\n",
    "    out[\"turnover\"] = turnover\n",
    "    out[\"tc_cost\"] = cost\n",
    "\n",
    "    out[\"avg_corr\"] = meta_daily[\"avg_corr\"]\n",
    "    out[\"alpha_minvar\"] = meta_daily[\"alpha_minvar\"]\n",
    "    out[\"port_vol_ann_pre_scale\"] = meta_daily[\"port_vol_ann_pre_scale\"]\n",
    "    out[\"leverage\"] = meta_daily[\"leverage\"]\n",
    "\n",
    "    for col in rets.columns:\n",
    "        out[f\"ret_{col}\"] = rets[col]\n",
    "        out[f\"w_{col}\"] = W[col]\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "# ----------------------------- Metrics -----------------------------\n",
    "\n",
    "\n",
    "def max_drawdown(series: pd.Series) -> float:\n",
    "    run_max = series.cummax()\n",
    "    dd = series / run_max - 1.0\n",
    "    return float(dd.min())\n",
    "\n",
    "\n",
    "def summary_stats(df: pd.DataFrame, cfg: Config) -> dict:\n",
    "    port_ret = df[\"port_ret_tc\"]\n",
    "    if port_ret.empty:\n",
    "        raise RuntimeError(\"No returns for summary.\")\n",
    "\n",
    "    ann_ret = (1.0 + port_ret).prod() ** (252.0 / len(port_ret)) - 1.0\n",
    "    ann_vol = annualize_vol(port_ret.std())\n",
    "    sharpe = float(ann_ret / ann_vol) if ann_vol > 0 else 0.0\n",
    "\n",
    "    eq_tc = df[\"equity_tc\"]\n",
    "    mdd = max_drawdown(eq_tc)\n",
    "\n",
    "    stats = {\n",
    "        \"start\": str(df.index[0].date()),\n",
    "        \"end\": str(df.index[-1].date()),\n",
    "        \"n_days\": int(len(df)),\n",
    "        \"ann_return\": float(ann_ret),\n",
    "        \"ann_vol\": float(ann_vol),\n",
    "        \"sharpe\": sharpe,\n",
    "        \"max_drawdown\": float(mdd),\n",
    "        \"median_avg_corr\": float(df[\"avg_corr\"].median()),\n",
    "        \"median_alpha_minvar\": float(df[\"alpha_minvar\"].median()),\n",
    "        \"median_leverage\": float(df[\"leverage\"].median()),\n",
    "    }\n",
    "    return stats\n",
    "\n",
    "\n",
    "# ----------------------------- I/O -----------------------------\n",
    "\n",
    "\n",
    "def save_outputs(df: pd.DataFrame, stats: dict, cfg: Config) -> None:\n",
    "    df.to_csv(cfg.out_csv, index=True, date_format=\"%Y-%m-%d\")\n",
    "    summary = {\n",
    "        \"config\": asdict(cfg),\n",
    "        \"stats\": stats,\n",
    "    }\n",
    "    with open(cfg.out_json, \"w\") as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "\n",
    "    print(f\"[OK] Saved daily series → {cfg.out_csv}\")\n",
    "    print(f\"[OK] Saved summary → {cfg.out_json}\")\n",
    "    print(\n",
    "        f\"Period {stats['start']} → {stats['end']}, \"\n",
    "        f\"AnnRet={stats['ann_return']:.2%}, \"\n",
    "        f\"AnnVol={stats['ann_vol']:.2%}, \"\n",
    "        f\"Sharpe={stats['sharpe']:.2f}, \"\n",
    "        f\"MaxDD={stats['max_drawdown']:.2%}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Median avg_corr={stats['median_avg_corr']:.2f}, \"\n",
    "        f\"Median alpha_minvar={stats['median_alpha_minvar']:.2f}, \"\n",
    "        f\"Median leverage={stats['median_leverage']:.2f}\"\n",
    "    )\n",
    "\n",
    "\n",
    "# ----------------------------- CLI -----------------------------\n",
    "\n",
    "\n",
    "def parse_args() -> Config:\n",
    "    p = argparse.ArgumentParser(\n",
    "        description=\"Level-62: Shrinkage Min-Var + Risk-Parity Blended Portfolio\"\n",
    "    )\n",
    "    p.add_argument(\"--start\", type=str, default=\"2010-01-01\")\n",
    "    p.add_argument(\"--vol-lookback\", type=int, default=60)\n",
    "    p.add_argument(\"--rebalance-freq\", type=str, default=\"M\")\n",
    "    p.add_argument(\"--vol-target\", type=float, default=0.10)\n",
    "    p.add_argument(\"--max-leverage\", type=float, default=2.0)\n",
    "    p.add_argument(\"--corr-min\", type=float, default=0.1)\n",
    "    p.add_argument(\"--corr-max\", type=float, default=0.8)\n",
    "    p.add_argument(\"--tc-bps\", type=float, default=10.0)\n",
    "    p.add_argument(\"--csv\", type=str, default=\"level62_shrinkage_riskparity.csv\")\n",
    "    p.add_argument(\"--json\", type=str, default=\"level62_shrinkage_riskparity_summary.json\")\n",
    "    p.add_argument(\"--seed\", type=int, default=42)\n",
    "    a = p.parse_args()\n",
    "\n",
    "    return Config(\n",
    "        start=a.start,\n",
    "        vol_lookback=a.vol_lookback,\n",
    "        rebalance_freq=a.rebalance_freq,\n",
    "        vol_target=a.vol_target,\n",
    "        max_leverage=a.max_leverage,\n",
    "        corr_min=a.corr_min,\n",
    "        corr_max=a.corr_max,\n",
    "        tc_bps=a.tc_bps,\n",
    "        out_csv=a.csv,\n",
    "        out_json=a.json,\n",
    "        seed=a.seed,\n",
    "    )\n",
    "\n",
    "\n",
    "# ----------------------------- Main -----------------------------\n",
    "\n",
    "\n",
    "def main():\n",
    "    cfg = parse_args()\n",
    "    np.random.seed(cfg.seed)\n",
    "\n",
    "    print(f\"[INFO] Downloading prices for {cfg.symbols} from {cfg.start} ...\")\n",
    "    close = load_prices(cfg)\n",
    "    rets = compute_returns(close)\n",
    "    print(f\"[INFO] Got {len(close)} price rows, {len(rets)} return rows.\")\n",
    "\n",
    "    weights_daily, meta_daily = compute_weights(rets, cfg)\n",
    "    print(\n",
    "        f\"[INFO] Computed weights from {weights_daily.index[0].date()} \"\n",
    "        f\"to {weights_daily.index[-1].date()}\"\n",
    "    )\n",
    "\n",
    "    out_df = backtest(rets, weights_daily, meta_daily, cfg)\n",
    "    stats = summary_stats(out_df, cfg)\n",
    "    save_outputs(out_df, stats, cfg)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Jupyter / PyCharm shim to strip kernel args like \"-f kernel-xxxx.json\"\n",
    "    import sys\n",
    "\n",
    "    sys.argv = [sys.argv[0]] + [\n",
    "        arg\n",
    "        for arg in sys.argv[1:]\n",
    "        if arg != \"-f\" and not (arg.endswith(\".json\") and \"kernel\" in arg)\n",
    "    ]\n",
    "    main()\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Downloading prices for ('SPY', 'QQQ', 'IWM', 'EFA', 'EEM', 'TLT', 'LQD', 'GLD') from 2010-01-01 ...\n",
      "[INFO] Got 4006 price rows, 4005 return rows.\n",
      "[INFO] Computed weights from 2010-01-05 to 2025-12-04\n",
      "[OK] Saved daily series → level62_shrinkage_riskparity.csv\n",
      "[OK] Saved summary → level62_shrinkage_riskparity_summary.json\n",
      "Period 2010-01-05 → 2025-12-04, AnnRet=9.73%, AnnVol=13.40%, Sharpe=0.73, MaxDD=-41.05%\n",
      "Median avg_corr=0.29, Median alpha_minvar=0.28, Median leverage=1.43\n"
     ]
    }
   ],
   "execution_count": 2
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
