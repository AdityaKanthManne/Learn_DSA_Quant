{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-04T02:34:30.983480Z",
     "start_time": "2025-11-04T02:32:15.323831Z"
    }
   },
   "source": [
    "# Level-36 — Purged CV + Platt + Vol-Targeted Kelly + Permutation Importance + OOS Equity (1-D hardened)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "from datetime import datetime, timedelta, timezone\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# ---------------- Config ----------------\n",
    "TICKER        = \"AAPL\"\n",
    "YEARS         = 3.0\n",
    "FREQ          = \"1D\"\n",
    "\n",
    "VOL_SPAN      = 50\n",
    "H_BARS        = 10\n",
    "CUSUM_GRID    = [0.003, 0.004, 0.006, 0.008, 0.010]\n",
    "UP_M, DN_M    = 3.0, 3.0\n",
    "MIN_EVENTS    = 400\n",
    "\n",
    "BASE_SEED     = 42\n",
    "N_SPLITS      = 5\n",
    "EMBARGO_DAYS  = 5                 # for purging leakage\n",
    "TC_BP         = 5                 # round-trip cost in bp\n",
    "BAND          = 0.05\n",
    "BET_CAP       = 0.5\n",
    "KELLY_EDGE_B  = 1.0\n",
    "\n",
    "# Vol-targeting (annualized). Position scaled by ratio(target/realized)\n",
    "TARGET_VOL_ANNUAL = 0.12          # 12% annual target vol\n",
    "REALVOL_WIN       = 20            # lookback for realized vol\n",
    "\n",
    "np.random.seed(BASE_SEED)\n",
    "\n",
    "# --------------- 1-D Hardeners ---------------\n",
    "def to1d(a):\n",
    "    if isinstance(a, pd.Series):\n",
    "        return a.to_numpy().ravel()\n",
    "    if isinstance(a, pd.DataFrame):\n",
    "        return a.iloc[:, 0].to_numpy().ravel()\n",
    "    return np.asarray(a).ravel()\n",
    "\n",
    "def series_1d(x, index=None, dtype=None):\n",
    "    if isinstance(x, pd.DataFrame):\n",
    "        vals = x.iloc[:, 0].to_numpy()\n",
    "        idx  = x.index\n",
    "    elif isinstance(x, pd.Series):\n",
    "        vals = x.to_numpy()\n",
    "        idx  = x.index\n",
    "    else:\n",
    "        vals = np.asarray(x)\n",
    "        idx  = None\n",
    "    vals = np.ravel(vals)\n",
    "    if dtype is not None:\n",
    "        vals = vals.astype(dtype, copy=False)\n",
    "    if index is None:\n",
    "        index = idx if (idx is not None and len(idx) == len(vals)) else None\n",
    "    return pd.Series(vals, index=index)\n",
    "\n",
    "# --------------- Data & Features ---------------\n",
    "def utc_today():\n",
    "    return datetime.now(timezone.utc).date()\n",
    "\n",
    "def load_prices(ticker, years, freq=\"1D\"):\n",
    "    end = utc_today()\n",
    "    start = (datetime.now(timezone.utc) - timedelta(days=int(365*years + 20))).date()\n",
    "    df = yf.download(ticker, start=start, end=end, auto_adjust=True, progress=False)\n",
    "    if df.empty:\n",
    "        raise SystemExit(\"No data downloaded.\")\n",
    "    s = df[\"Adj Close\"] if \"Adj Close\" in df.columns else df[\"Close\"]\n",
    "    s = s.asfreq(\"B\").ffill()\n",
    "    s.name = \"Close\"\n",
    "    return s\n",
    "\n",
    "def ewma_vol(r, span=50):\n",
    "    return r.ewm(span=span, adjust=False).std()\n",
    "\n",
    "def make_features(close):\n",
    "    r = close.pct_change().fillna(0.0)\n",
    "    f = pd.DataFrame(index=close.index)\n",
    "    f[\"r1\"]    = r\n",
    "    f[\"r5\"]    = close.pct_change(5)\n",
    "    f[\"r10\"]   = close.pct_change(10)\n",
    "    f[\"mom5\"]  = close/close.shift(5)  - 1\n",
    "    f[\"mom10\"] = close/close.shift(10) - 1\n",
    "    f[\"vol10\"] = r.rolling(10).std()\n",
    "    f[\"vol20\"] = r.rolling(20).std()\n",
    "    f[\"z20\"]   = (close - close.rolling(20).mean()) / (1e-12 + close.rolling(20).std())\n",
    "    d = close.diff()\n",
    "    up = d.clip(lower=0).rolling(14).mean()\n",
    "    dn = (-d.clip(upper=0)).rolling(14).mean()\n",
    "    rs = up / (1e-12 + dn)\n",
    "    f[\"rsi14\"] = 100 - 100/(1+rs)\n",
    "    return f.fillna(0.0)\n",
    "\n",
    "# --------------- Labeling (CUSUM + triple barrier) ---------------\n",
    "def cusum_filter(r, threshold, index=None):\n",
    "    s = series_1d(r, index=index, dtype=float).fillna(0.0)\n",
    "    idx = s.index\n",
    "    vals = s.to_numpy()\n",
    "    s_pos = s_neg = 0.0\n",
    "    t_events = []\n",
    "    for i, x in enumerate(vals):\n",
    "        s_pos = max(0.0, s_pos + x)\n",
    "        s_neg = min(0.0, s_neg + x)\n",
    "        if s_pos > threshold:\n",
    "            s_pos = 0.0\n",
    "            t_events.append(idx[i])\n",
    "        elif s_neg < -threshold:\n",
    "            s_neg = 0.0\n",
    "            t_events.append(idx[i])\n",
    "    return pd.DatetimeIndex(t_events).unique().sort_values()\n",
    "\n",
    "def get_vertical_barriers(t_events, h, index):\n",
    "    if len(t_events) == 0:\n",
    "        return pd.Series(dtype=\"datetime64[ns]\")\n",
    "    out = {}\n",
    "    for t0 in t_events:\n",
    "        pos = index.get_indexer([t0])[0]\n",
    "        t1_pos = min(pos + h, len(index) - 1)\n",
    "        out[t0] = index[t1_pos]\n",
    "    return pd.Series(out)\n",
    "\n",
    "def get_triple_barrier_labels(close, t_events, vbar, up_m, dn_m, daily_vol):\n",
    "    trgt_raw = daily_vol.reindex(t_events).fillna(method=\"bfill\").fillna(method=\"ffill\")\n",
    "    trgt = series_1d(trgt_raw, index=t_events, dtype=float)\n",
    "\n",
    "    t1_raw = vbar.reindex(t_events)\n",
    "    t1 = pd.Series(t1_raw.values, index=t_events)\n",
    "\n",
    "    rows = []\n",
    "    for t0 in t_events:\n",
    "        t1_i = t1.loc[t0]\n",
    "        if pd.isna(t1_i):\n",
    "            continue\n",
    "        try:\n",
    "            c0 = float(close.loc[t0])\n",
    "        except Exception:\n",
    "            continue\n",
    "        up_lvl = c0 * (1 + up_m * float(trgt.loc[t0]))\n",
    "        dn_lvl = c0 * (1 - dn_m * float(trgt.loc[t0]))\n",
    "\n",
    "        seg = close.loc[t0:t1_i]\n",
    "        if isinstance(seg, pd.DataFrame):\n",
    "            seg = seg.iloc[:, 0]\n",
    "        path = series_1d(seg, index=seg.index, dtype=float)\n",
    "        if path.empty:\n",
    "            continue\n",
    "\n",
    "        path_up = path >= up_lvl\n",
    "        path_dn = path <= dn_lvl\n",
    "        hit_up = path_up.idxmax() if path_up.to_numpy().any() else None\n",
    "        hit_dn = path_dn.idxmax() if path_dn.to_numpy().any() else None\n",
    "\n",
    "        if (hit_up is not None) and (hit_dn is not None):\n",
    "            lbl = 1 if hit_up <= hit_dn else 0\n",
    "            t_end = hit_up if lbl == 1 else hit_dn\n",
    "        elif hit_up is not None:\n",
    "            lbl, t_end = 1, hit_up\n",
    "        elif hit_dn is not None:\n",
    "            lbl, t_end = 0, hit_dn\n",
    "        else:\n",
    "            c1 = float(path.iloc[-1])\n",
    "            lbl, t_end = (1 if c1 > c0 else 0), t1_i\n",
    "\n",
    "        rows.append((t0, t_end, lbl, float(trgt.loc[t0])))\n",
    "\n",
    "    if not rows:\n",
    "        return pd.DataFrame(columns=[\"t1\", \"label\", \"trgt\"])\n",
    "    df = pd.DataFrame(rows, columns=[\"t0\", \"t1\", \"label\", \"trgt\"]).set_index(\"t0\")\n",
    "    return df\n",
    "\n",
    "def adaptive_events_and_labels(close, rets, base_H, cusum_grid, vol_span, up_m, dn_m, min_events):\n",
    "    daily_vol = ewma_vol(rets, span=vol_span).clip(lower=1e-8)\n",
    "\n",
    "    used_thr = None\n",
    "    events_idx = pd.DatetimeIndex([])\n",
    "    for thr in cusum_grid:\n",
    "        ev = cusum_filter(rets, thr, index=close.index)\n",
    "        if len(ev) >= min_events:\n",
    "            used_thr, events_idx = thr, ev\n",
    "            break\n",
    "    if used_thr is None:\n",
    "        used_thr = min(cusum_grid)\n",
    "        events_idx = cusum_filter(rets, used_thr, index=close.index)\n",
    "\n",
    "    vbar = get_vertical_barriers(events_idx, base_H, close.index)\n",
    "    labels = get_triple_barrier_labels(close, events_idx, vbar, up_m, dn_m, daily_vol).dropna()\n",
    "    if labels.empty:\n",
    "        raise SystemExit(\"No labeled events. Adjust thresholds.\")\n",
    "    print(f\"[Adaptive] events={len(labels)}  H={base_H} thr≈{used_thr}\")\n",
    "    return labels, daily_vol, events_idx, base_H, float(used_thr)\n",
    "\n",
    "# --------------- Purged Forward CV Splits ---------------\n",
    "def purged_forward_splits(index, n_splits=5, embargo_days=5):\n",
    "    dates = pd.DatetimeIndex(index)\n",
    "    n = len(dates)\n",
    "    fold_sizes = np.full(n_splits, n // n_splits, dtype=int)\n",
    "    fold_sizes[: n % n_splits] += 1\n",
    "    starts = np.cumsum(np.concatenate(([0], fold_sizes[:-1])))\n",
    "    ends   = np.cumsum(fold_sizes)\n",
    "    embargo = pd.Timedelta(days=embargo_days)\n",
    "    for k in range(n_splits):\n",
    "        va_start, va_end = starts[k], ends[k]\n",
    "        va_idx = np.arange(va_start, va_end)\n",
    "        va_dates = dates[va_idx]\n",
    "        cutoff = va_dates[0] - embargo\n",
    "        tr_idx = np.where(dates < cutoff)[0]\n",
    "        yield tr_idx, va_idx\n",
    "\n",
    "# --------------- Platt (logistic-on-logit) ---------------\n",
    "def platt_fit(y_true, raw_prob):\n",
    "    eps = 1e-6\n",
    "    z = np.clip(to1d(raw_prob), eps, 1 - eps)\n",
    "    logit = np.log(z / (1 - z)).reshape(-1, 1)\n",
    "    lr = LogisticRegression(solver=\"lbfgs\", max_iter=1000, random_state=BASE_SEED)\n",
    "    lr.fit(logit, to1d(y_true).astype(int))\n",
    "    return lr\n",
    "\n",
    "def platt_predict(lr, raw_prob):\n",
    "    eps = 1e-6\n",
    "    z = np.clip(to1d(raw_prob), eps, 1 - eps)\n",
    "    logit = np.log(z / (1 - z)).reshape(-1, 1)\n",
    "    return lr.predict_proba(logit)[:, 1]\n",
    "\n",
    "# --------------- Metrics ---------------\n",
    "def sharpe_ratio(x):\n",
    "    x = series_1d(x)\n",
    "    s = x.std()\n",
    "    return 0.0 if s == 0 or np.isnan(s) else float(np.sqrt(252) * x.mean() / s)\n",
    "\n",
    "def drawdown(x):\n",
    "    x = series_1d(x)\n",
    "    cum = (1 + x).cumprod()\n",
    "    return float((cum / cum.cummax() - 1).min())\n",
    "\n",
    "# --------------- Bet sizing ---------------\n",
    "def kelly_fraction(p, b=1.0):\n",
    "    p = to1d(p)\n",
    "    f = (p * (b + 1.0) - 1.0) / b  # b=1 -> f=2p-1\n",
    "    return np.clip(f, -BET_CAP, BET_CAP)\n",
    "\n",
    "def apply_rebalance_band(target, band=BAND):\n",
    "    t = to1d(target)\n",
    "    pos = 0.0\n",
    "    out = np.zeros_like(t, dtype=float)\n",
    "    for i, v in enumerate(t):\n",
    "        if abs(v - pos) > band:\n",
    "            pos = v\n",
    "        out[i] = pos\n",
    "    return out\n",
    "\n",
    "# --------------- Vol targeting helper ---------------\n",
    "def realized_vol_daily(returns, win=20):\n",
    "    r = series_1d(returns)\n",
    "    sig_d = r.rolling(win).std().fillna(method=\"bfill\").fillna(0.0)\n",
    "    return sig_d\n",
    "\n",
    "# --------------- Main ---------------\n",
    "if __name__ == \"__main__\":\n",
    "    # 1) Data\n",
    "    close = load_prices(TICKER, YEARS, FREQ)\n",
    "    rets  = close.pct_change().replace([np.inf, -np.inf], 0.0).fillna(0.0)\n",
    "\n",
    "    # 2) Events & Labels\n",
    "    labels, _, _, _, _ = adaptive_events_and_labels(\n",
    "        close, rets, H_BARS, CUSUM_GRID, VOL_SPAN, UP_M, DN_M, MIN_EVENTS\n",
    "    )\n",
    "\n",
    "    # 3) Features on event times\n",
    "    feats = make_features(close)\n",
    "    X = feats.reindex(labels.index).dropna()\n",
    "    labels = labels.loc[X.index]\n",
    "    y = labels[\"label\"].astype(int)\n",
    "    feat_names = list(X.columns)\n",
    "\n",
    "    # 4) Scale\n",
    "    scaler = StandardScaler()\n",
    "    Xs = scaler.fit_transform(X)\n",
    "\n",
    "    # 5) Purged forward CV + Platt calibration\n",
    "    proba_cv = np.full(len(X), np.nan)\n",
    "    aucs, thrs = [], []\n",
    "    last_fold = None  # keep last fold data for permutation importance\n",
    "    for tr_idx, va_idx in purged_forward_splits(X.index, n_splits=N_SPLITS, embargo_days=EMBARGO_DAYS):\n",
    "        if len(tr_idx) < 50 or len(va_idx) < 20:\n",
    "            continue\n",
    "        X_tr, y_tr = Xs[tr_idx], to1d(y.iloc[tr_idx])\n",
    "        X_va, y_va = Xs[va_idx], to1d(y.iloc[va_idx])\n",
    "\n",
    "        base = GradientBoostingClassifier(random_state=BASE_SEED)\n",
    "        base.fit(X_tr, y_tr)\n",
    "        p_va_raw = base.predict_proba(X_va)[:, 1]\n",
    "\n",
    "        pl = platt_fit(y_va, p_va_raw)              # calibrate on val slice\n",
    "        p_va = platt_predict(pl, p_va_raw)\n",
    "        proba_cv[va_idx] = p_va\n",
    "\n",
    "        aucs.append(roc_auc_score(y_va, p_va))\n",
    "\n",
    "        # pick threshold on Sharpe of val pnl\n",
    "        r_next_va = to1d(close.pct_change().shift(-1).reindex(X.index[va_idx]).fillna(0.0))\n",
    "        grid = np.linspace(0.25, 0.75, 21)\n",
    "        best_thr, best_score = 0.5, -1e9\n",
    "        for thr in grid:\n",
    "            sig = (p_va >= thr).astype(float)\n",
    "            pnl = sig * r_next_va\n",
    "            score = pnl.mean() / (pnl.std() + 1e-9)\n",
    "            if score > best_score:\n",
    "                best_score, best_thr = score, thr\n",
    "        thrs.append(best_thr)\n",
    "\n",
    "        # keep last fold\n",
    "        last_fold = (base, X_va, y_va)\n",
    "\n",
    "    cv_auc = float(np.nanmean(aucs)) if len(aucs) else np.nan\n",
    "    chosen_thr = float(np.nanmedian(thrs)) if len(thrs) else 0.55\n",
    "    print(f\"\\nCV AUC (purged, {N_SPLITS} folds): {cv_auc:.3f}\")\n",
    "    print(f\"Chosen prob threshold (median across folds): {chosen_thr:.2f}\")\n",
    "\n",
    "    # 6) Rolling walk-forward proba (Platt recalibrated in a rolling window)\n",
    "    base_final = GradientBoostingClassifier(random_state=BASE_SEED)\n",
    "    proba_all  = np.zeros(len(X), dtype=float)\n",
    "    min_fit = max(100, len(X)//N_SPLITS)\n",
    "    for i in range(min_fit, len(X)):\n",
    "        X_tr, y_tr = Xs[:i], to1d(y.iloc[:i])\n",
    "        base_final.fit(X_tr, y_tr)\n",
    "        raw = base_final.predict_proba(Xs[i:i+1])[:, 1]\n",
    "\n",
    "        j0 = max(0, i - 250)\n",
    "        p_tr_raw = base_final.predict_proba(Xs[j0:i])[:, 1]\n",
    "        pl = platt_fit(y.iloc[j0:i], p_tr_raw)\n",
    "        proba_all[i] = platt_predict(pl, raw)[0]\n",
    "\n",
    "    # 7) Kelly targets + bands\n",
    "    kelly_raw = kelly_fraction(proba_all, b=KELLY_EDGE_B)\n",
    "    kelly_pos = apply_rebalance_band(kelly_raw, band=BAND)\n",
    "\n",
    "    # 8) Vol targeting on top of Kelly\n",
    "    target_daily = TARGET_VOL_ANNUAL / np.sqrt(252.0)\n",
    "    ret_next_full = close.pct_change().shift(-1)\n",
    "    realized_d = realized_vol_daily(ret_next_full, win=REALVOL_WIN).reindex(X.index).fillna(method=\"bfill\").fillna(0.0)\n",
    "    scale = (target_daily / (realized_d.replace(0.0, np.nan))).clip(upper=3.0).fillna(0.0)\n",
    "    kelly_vol = series_1d(kelly_pos, index=X.index) * series_1d(scale, index=X.index)\n",
    "    kelly_vol = np.clip(kelly_vol, -BET_CAP, BET_CAP).to_numpy()\n",
    "\n",
    "    # 9) Backtest — net PnL with tc, stats\n",
    "    ret_next = to1d(ret_next_full.reindex(X.index).fillna(0.0))\n",
    "    m = min(len(kelly_vol), len(ret_next))\n",
    "    pos = to1d(kelly_vol[:m])\n",
    "    ret = to1d(ret_next[:m])\n",
    "\n",
    "    pnl_gross = pos * ret\n",
    "    toggle    = np.abs(np.diff(pos, prepend=0.0))\n",
    "    tc        = toggle * (TC_BP / 1e4)\n",
    "    pnl_net   = pnl_gross - tc\n",
    "\n",
    "    pnl_series = series_1d(pnl_net, index=X.index[:m])\n",
    "    sharpe = sharpe_ratio(pnl_series)\n",
    "    cum = (1.0 + pnl_series).cumprod()\n",
    "    yrs = (cum.index[-1] - cum.index[0]).days / 365.25 if isinstance(cum.index, pd.DatetimeIndex) else len(cum)/252.0\n",
    "    cagr = float(cum.iloc[-1] ** (1.0 / max(1e-9, yrs)) - 1.0)\n",
    "    dd   = drawdown(pnl_series)\n",
    "\n",
    "    long_days = (pos > 0).astype(float)\n",
    "    hit = float(((long_days > 0) & (ret > 0)).sum() / max(1, (long_days > 0).sum()))\n",
    "\n",
    "    print(\"\\n=== Level-36 — Adds Permutation Importance + OOS Equity ===\")\n",
    "    print(f\"Sharpe: {sharpe:.3f}  CAGR: {100*cagr:.2f}%  MaxDD: {100*dd:.2f}%  Hit%: {100*hit:.2f}%\")\n",
    "    print(f\"Avg daily turnover: {toggle.mean():.3f}   Avg TC paid (bp): {(tc * 1e4).mean():.2f}\")\n",
    "\n",
    "    # --- Stability diagnostics: decile buckets of proba vs forward return\n",
    "    df_diag = pd.DataFrame(\n",
    "        {\n",
    "            \"proba\": proba_all[:m],\n",
    "            \"ret_next\": ret[:m]\n",
    "        },\n",
    "        index=X.index[:m],\n",
    "    ).dropna()\n",
    "    df_diag[\"bucket\"] = pd.qcut(df_diag[\"proba\"].rank(method=\"first\"), 10, labels=False)\n",
    "    decile_stats = df_diag.groupby(\"bucket\")[\"ret_next\"].agg([\"mean\",\"std\",\"count\"])\n",
    "    decile_stats.index.name = \"prob_decile\"\n",
    "\n",
    "    # --- Permutation importance (on last CV fold validation for leakage safety)\n",
    "    perm_df = pd.DataFrame(columns=[\"feature\", \"mean_importance\", \"std_importance\"])\n",
    "    if last_fold is not None:\n",
    "        base_last, X_va_last, y_va_last = last_fold\n",
    "        # We compute importance on calibrated probabilities by wrapping predict_proba[:,1]\n",
    "        # but permutation_importance expects a scorer or score; so we use AUC scorer behavior by recomputing proba each perm.\n",
    "        # Simpler: use default estimator.score (accuracy). Instead, define a small helper that returns AUC.\n",
    "        from sklearn.metrics import roc_auc_score as _auc\n",
    "\n",
    "        def auc_scorer(est, Xb, yb):\n",
    "            p = est.predict_proba(Xb)[:,1]\n",
    "            try:\n",
    "                return _auc(yb, p)\n",
    "            except ValueError:\n",
    "                # fallback if y has single class (rare)\n",
    "                return 0.5\n",
    "\n",
    "        # Monkey patch: wrap estimator so score uses AUC\n",
    "        class _AUCWrap:\n",
    "            def __init__(self, est): self.est = est\n",
    "            def fit(self, *a, **k): return self.est.fit(*a, **k)\n",
    "            def predict_proba(self, X): return self.est.predict_proba(X)\n",
    "            def score(self, X, y): return auc_scorer(self.est, X, y)\n",
    "\n",
    "        wrapped = _AUCWrap(base_last)\n",
    "        perm = permutation_importance(\n",
    "            wrapped, X_va_last, y_va_last, n_repeats=20, random_state=BASE_SEED, n_jobs=1\n",
    "        )\n",
    "        perm_df = pd.DataFrame({\n",
    "            \"feature\": feat_names,\n",
    "            \"mean_importance\": perm.importances_mean,\n",
    "            \"std_importance\": perm.importances_std\n",
    "        }).sort_values(\"mean_importance\", ascending=False)\n",
    "\n",
    "    # --- Save outputs\n",
    "    out_ts = pd.DataFrame(\n",
    "        {\n",
    "            \"proba\": proba_all[:m],\n",
    "            \"kelly_raw\": kelly_raw[:m],\n",
    "            \"kelly_pos_voltgt\": pos[:m],\n",
    "            \"ret_next\": ret[:m],\n",
    "            \"pnl_net\": pnl_net[:m],\n",
    "        },\n",
    "        index=X.index[:m],\n",
    "    )\n",
    "    ts_file = f\"{TICKER}_level36_timeseries.csv\"\n",
    "    out_ts.to_csv(ts_file)\n",
    "\n",
    "    dec_file = f\"{TICKER}_level36_deciles.csv\"\n",
    "    decile_stats.to_csv(dec_file)\n",
    "\n",
    "    # Rolling OOS equity curve\n",
    "    equity = (1.0 + pnl_series).cumprod()\n",
    "    eq_df = pd.DataFrame({\"equity\": equity.values}, index=equity.index)\n",
    "    eq_file = f\"{TICKER}_level36_equity.csv\"\n",
    "    eq_df.to_csv(eq_file)\n",
    "\n",
    "    # Permutation importance\n",
    "    if not perm_df.empty:\n",
    "        pi_file = f\"{TICKER}_level36_perm_importance.csv\"\n",
    "        perm_df.to_csv(pi_file, index=False)\n",
    "        print(f\"\\nSaved: {ts_file}, {dec_file}, {eq_file}, {pi_file}\")\n",
    "    else:\n",
    "        print(f\"\\nSaved: {ts_file}, {dec_file}, {eq_file} (no permutation importance if CV folds were too small)\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Adaptive] events=626  H=10 thr≈0.003\n",
      "\n",
      "CV AUC (purged, 5 folds): 0.575\n",
      "Chosen prob threshold (median across folds): 0.61\n",
      "\n",
      "=== Level-36 — Adds Permutation Importance + OOS Equity ===\n",
      "Sharpe: 0.842  CAGR: 3.86%  MaxDD: -9.74%  Hit%: 56.81%\n",
      "Avg daily turnover: 0.142   Avg TC paid (bp): 0.71\n",
      "\n",
      "Saved: AAPL_level36_timeseries.csv, AAPL_level36_deciles.csv, AAPL_level36_equity.csv, AAPL_level36_perm_importance.csv\n"
     ]
    }
   ],
   "execution_count": 1
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
