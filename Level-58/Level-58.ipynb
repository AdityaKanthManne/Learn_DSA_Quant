{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-03T01:27:46.322262Z",
     "start_time": "2025-12-03T01:27:44.685446Z"
    }
   },
   "source": [
    "\"\"\"\n",
    "Level-58 — Hierarchical Risk Parity (HRP) vs Equal-Weight vs Inverse-Variance\n",
    "\n",
    "Concept focus\n",
    "-------------\n",
    "This level combines:\n",
    "\n",
    "1) Quant model:\n",
    "   - Hierarchical Risk Parity (HRP) allocation on a small ETF universe\n",
    "     (default: SPY, QQQ, TLT, GLD).\n",
    "   - Benchmarks:\n",
    "       * Equal-weight (EW)\n",
    "       * Inverse-variance (IV) portfolio\n",
    "\n",
    "2) DSA concept:\n",
    "   - Hierarchical clustering as a tree of clusters built bottom-up.\n",
    "   - We implement a simple agglomerative clustering from scratch\n",
    "     (single-linkage) without SciPy, using:\n",
    "        * A distance matrix (graph on assets),\n",
    "        * Iterative merging of closest clusters,\n",
    "        * A tree structure built via nested lists,\n",
    "        * Recursive traversal of that tree to obtain an ordered leaf layout.\n",
    "   - This is essentially “build a tree by repeatedly joining the closest\n",
    "     nodes, then traverse it recursively” — a classic divide-and-conquer\n",
    "     pattern with an explicit tree data structure.\n",
    "\n",
    "What the script does\n",
    "--------------------\n",
    "- Downloads daily prices for ETFs (SPY, QQQ, TLT, GLD) using yfinance,\n",
    "  with a synthetic multi-asset GBM fallback if download fails.\n",
    "- Computes daily returns, covariance and correlation.\n",
    "- Builds a hierarchical cluster tree of assets using a custom\n",
    "  agglomerative single-linkage clustering (no SciPy required).\n",
    "- Derives an asset ordering from that tree (leaf ordering).\n",
    "- Computes three portfolios:\n",
    "    1) Equal-weight (EW)\n",
    "    2) Inverse-variance (IV)\n",
    "    3) Hierarchical Risk Parity (HRP)\n",
    "- Applies static weights over the full sample, computes daily portfolio\n",
    "  returns and performance statistics (CAGR, vol, Sharpe, max drawdown).\n",
    "- Saves:\n",
    "    - level58_hrp_portfolios.csv  (prices, returns, weights, portfolio returns)\n",
    "    - level58_hrp_summary.json    (weights + performance metrics)\n",
    "\n",
    "External references to learn more\n",
    "---------------------------------\n",
    "(This script is self-contained, but to study HRP in depth, see:)\n",
    "- López de Prado, “Building Diversified Portfolios that Perform Well\n",
    "  Out-of-Sample” (SSRN / arXiv).\n",
    "- Quantpedia overview of Hierarchical Risk Parity strategies.\n",
    "- Various HRP implementations and discussions:\n",
    "  - https://github.com/robertmartin8/PyPortfolioOpt\n",
    "  - https://www.quantconnect.com/tutorials/strategy-library/hierarchical-risk-parity\n",
    "\n",
    "Drop this into PyCharm or a Jupyter cell and run.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Tuple, Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "\n",
    "\n",
    "# ---------------------------- Config ---------------------------- #\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    symbols: Tuple[str, ...] = (\"SPY\", \"QQQ\", \"TLT\", \"GLD\")\n",
    "    start: str = \"2010-01-01\"\n",
    "\n",
    "    # Output\n",
    "    out_csv: str = \"level58_hrp_portfolios.csv\"\n",
    "    out_json: str = \"level58_hrp_summary.json\"\n",
    "\n",
    "\n",
    "# ---------------------- Data utilities -------------------------- #\n",
    "\n",
    "\n",
    "def build_synthetic_prices(cfg: Config) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Synthetic multi-asset GBM with a simple correlation structure.\n",
    "    Used if yfinance download fails.\n",
    "    \"\"\"\n",
    "    print(\"[WARN] Falling back to synthetic prices (Level-58).\")\n",
    "    rng = np.random.default_rng(58)\n",
    "    n_assets = len(cfg.symbols)\n",
    "    n_days = 4000\n",
    "\n",
    "    dates = pd.bdate_range(\"2010-01-04\", periods=n_days, freq=\"B\")\n",
    "\n",
    "    base_corr = np.array(\n",
    "        [\n",
    "            [1.0, 0.85, -0.2, 0.1],\n",
    "            [0.85, 1.0, -0.2, 0.1],\n",
    "            [-0.2, -0.2, 1.0, 0.0],\n",
    "            [0.1, 0.1, 0.0, 1.0],\n",
    "        ]\n",
    "    )\n",
    "    if n_assets != base_corr.shape[0]:\n",
    "        corr = np.eye(n_assets)\n",
    "    else:\n",
    "        corr = base_corr\n",
    "\n",
    "    chol = np.linalg.cholesky(corr)\n",
    "\n",
    "    vols = np.array([0.18, 0.22, 0.12, 0.15])[:n_assets]\n",
    "    mus = np.array([0.07, 0.09, 0.04, 0.05])[:n_assets]\n",
    "\n",
    "    dt = 1.0 / 252.0\n",
    "    z = rng.standard_normal((n_days, n_assets))\n",
    "    eps = z @ chol.T\n",
    "\n",
    "    rets = (mus - 0.5 * vols**2) * dt + vols * math.sqrt(dt) * eps\n",
    "    prices = 100.0 * np.exp(np.cumsum(rets, axis=0))\n",
    "\n",
    "    df = pd.DataFrame(prices, index=dates, columns=list(cfg.symbols))\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_price_series(cfg: Config) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Download daily adjusted close prices for the symbols from yfinance.\n",
    "    Handles MultiIndex columns and falls back to synthetic if needed.\n",
    "    Returns:\n",
    "        DataFrame with columns = cfg.symbols, index = dates, dtype=float.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        raw = yf.download(\n",
    "            list(cfg.symbols),\n",
    "            start=cfg.start,\n",
    "            auto_adjust=True,\n",
    "            progress=False,\n",
    "        )\n",
    "    except Exception:\n",
    "        raw = pd.DataFrame()\n",
    "\n",
    "    if raw is None or raw.empty:\n",
    "        return build_synthetic_prices(cfg)\n",
    "\n",
    "    if isinstance(raw.columns, pd.MultiIndex):\n",
    "        top = raw.columns.get_level_values(0)\n",
    "        if \"Adj Close\" in top and \"Close\" not in top:\n",
    "            px = raw[\"Adj Close\"].copy()\n",
    "        else:\n",
    "            px = raw[\"Close\"].copy()\n",
    "    else:\n",
    "        px = raw.copy()\n",
    "\n",
    "    cols = [c for c in px.columns if c in cfg.symbols]\n",
    "    if not cols:\n",
    "        return build_synthetic_prices(cfg)\n",
    "\n",
    "    px = px[cols].sort_index()\n",
    "    px = px.dropna(how=\"any\").copy()\n",
    "\n",
    "    for sym in cfg.symbols:\n",
    "        if sym not in px.columns:\n",
    "            px[sym] = 1.0\n",
    "\n",
    "    px = px[list(cfg.symbols)].astype(float)\n",
    "    return px\n",
    "\n",
    "\n",
    "# -------------------- Performance utilities --------------------- #\n",
    "\n",
    "\n",
    "def annualized_stats(ret: pd.Series) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Compute CAGR, vol, Sharpe (CAGR/vol), and max drawdown for daily returns.\n",
    "    \"\"\"\n",
    "    ret = ret.dropna()\n",
    "    if len(ret) == 0:\n",
    "        return {\n",
    "            \"cagr\": 0.0,\n",
    "            \"vol\": 0.0,\n",
    "            \"sharpe\": 0.0,\n",
    "            \"max_drawdown\": 0.0,\n",
    "        }\n",
    "\n",
    "    total_return = float((1.0 + ret).prod())\n",
    "    years = len(ret) / 252.0\n",
    "    cagr = total_return ** (1.0 / years) - 1.0 if years > 0 else 0.0\n",
    "\n",
    "    vol = float(ret.std() * math.sqrt(252.0))\n",
    "    sharpe = cagr / vol if vol > 0 else 0.0\n",
    "\n",
    "    equity = (1.0 + ret).cumprod()\n",
    "    roll_max = equity.cummax()\n",
    "    dd = equity / roll_max - 1.0\n",
    "    max_dd = float(dd.min())\n",
    "\n",
    "    return {\n",
    "        \"cagr\": float(cagr),\n",
    "        \"vol\": float(vol),\n",
    "        \"sharpe\": float(sharpe),\n",
    "        \"max_drawdown\": max_dd,\n",
    "    }\n",
    "\n",
    "\n",
    "# -------------------- Hierarchical clustering ------------------- #\n",
    "\n",
    "\n",
    "class ClusterNode:\n",
    "    \"\"\"\n",
    "    A simple cluster node for agglomerative clustering:\n",
    "    - members: list of asset indices used to compute distances.\n",
    "    - struct: nested representation of the cluster tree\n",
    "              (int or [left_struct, right_struct]).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, members: List[int], struct: Any):\n",
    "        self.members = members\n",
    "        self.struct = struct\n",
    "\n",
    "\n",
    "def corr_to_distance(corr: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Convert correlation matrix to a distance matrix using:\n",
    "        d_ij = sqrt(0.5 * (1 - corr_ij))\n",
    "\n",
    "    Ensures a proper distance for clustering.\n",
    "    \"\"\"\n",
    "    corr_clipped = np.clip(corr, -1.0, 1.0)\n",
    "    dist = np.sqrt(0.5 * (1.0 - corr_clipped))\n",
    "    np.fill_diagonal(dist, 0.0)\n",
    "    return dist\n",
    "\n",
    "\n",
    "def hierarchical_clustering_single_link(dist: np.ndarray) -> Any:\n",
    "    \"\"\"\n",
    "    Very simple agglomerative single-linkage clustering, implemented\n",
    "    from scratch to avoid SciPy.\n",
    "\n",
    "    Args:\n",
    "        dist: (n x n) distance matrix between assets.\n",
    "\n",
    "    Returns:\n",
    "        A nested list representation of the cluster tree. Leaves are\n",
    "        integers (asset indices). Internal nodes are [left, right].\n",
    "    \"\"\"\n",
    "    n = dist.shape[0]\n",
    "    nodes: List[ClusterNode] = [ClusterNode([i], i) for i in range(n)]\n",
    "\n",
    "    # Helper to compute distance between clusters via single-linkage\n",
    "    def cluster_distance(a: ClusterNode, b: ClusterNode) -> float:\n",
    "        vals = [\n",
    "            dist[i, j]\n",
    "            for i in a.members\n",
    "            for j in b.members\n",
    "            if i != j\n",
    "        ]\n",
    "        if not vals:\n",
    "            return float(\"inf\")\n",
    "        return float(min(vals))\n",
    "\n",
    "    # Agglomerative merging\n",
    "    while len(nodes) > 1:\n",
    "        best_i, best_j, best_d = 0, 1, float(\"inf\")\n",
    "        m = len(nodes)\n",
    "        for i in range(m):\n",
    "            for j in range(i + 1, m):\n",
    "                d = cluster_distance(nodes[i], nodes[j])\n",
    "                if d < best_d:\n",
    "                    best_d = d\n",
    "                    best_i, best_j = i, j\n",
    "\n",
    "        a = nodes[best_i]\n",
    "        b = nodes[best_j]\n",
    "        merged_members = sorted(a.members + b.members)\n",
    "        merged_struct = [a.struct, b.struct]\n",
    "\n",
    "        new_node = ClusterNode(merged_members, merged_struct)\n",
    "\n",
    "        # Remove j then i (higher index first) to keep indices valid\n",
    "        for idx in sorted([best_i, best_j], reverse=True):\n",
    "            del nodes[idx]\n",
    "        nodes.append(new_node)\n",
    "\n",
    "    return nodes[0].struct\n",
    "\n",
    "\n",
    "def tree_to_order(tree: Any) -> List[int]:\n",
    "    \"\"\"\n",
    "    Flatten nested cluster tree into a 1D ordering of leaf indices\n",
    "    (left-to-right traversal).\n",
    "    \"\"\"\n",
    "    if isinstance(tree, int):\n",
    "        return [tree]\n",
    "    left, right = tree\n",
    "    return tree_to_order(left) + tree_to_order(right)\n",
    "\n",
    "\n",
    "# --------------------- HRP weight construction ------------------ #\n",
    "\n",
    "\n",
    "def cluster_variance(cov: np.ndarray, cluster_idx: List[int]) -> float:\n",
    "    \"\"\"\n",
    "    Compute variance of a cluster using equal weights within the cluster.\n",
    "    \"\"\"\n",
    "    if not cluster_idx:\n",
    "        return 0.0\n",
    "    sub = cov[np.ix_(cluster_idx, cluster_idx)]\n",
    "    n = len(cluster_idx)\n",
    "    w = np.full(n, 1.0 / n, dtype=float)\n",
    "    var = float(w @ sub @ w)\n",
    "    return max(var, 1e-12)\n",
    "\n",
    "\n",
    "def hrp_allocation(cov: np.ndarray, order: List[int]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Hierarchical Risk Parity allocation given covariance matrix and\n",
    "    a leaf ordering.\n",
    "\n",
    "    Implementation follows the recursive bisection idea:\n",
    "    - Start with all assets in one cluster and weight 1.\n",
    "    - At each step, split the ordered list in two subclusters,\n",
    "      compute cluster variances, and allocate capital inversely\n",
    "      to variance, then recurse down the tree.\n",
    "    \"\"\"\n",
    "    n = cov.shape[0]\n",
    "    w = np.ones(n, dtype=float)\n",
    "\n",
    "    def _allocate(cluster: List[int]) -> None:\n",
    "        if len(cluster) <= 1:\n",
    "            return\n",
    "        mid = len(cluster) // 2\n",
    "        left = cluster[:mid]\n",
    "        right = cluster[mid:]\n",
    "\n",
    "        var_left = cluster_variance(cov, left)\n",
    "        var_right = cluster_variance(cov, right)\n",
    "        inv_sum = 1.0 / var_left + 1.0 / var_right\n",
    "\n",
    "        alloc_left = (1.0 / var_left) / inv_sum\n",
    "        alloc_right = (1.0 / var_right) / inv_sum\n",
    "\n",
    "        # Scale existing weights in each subcluster\n",
    "        w[left] *= alloc_left\n",
    "        w[right] *= alloc_right\n",
    "\n",
    "        _allocate(left)\n",
    "        _allocate(right)\n",
    "\n",
    "    _allocate(order)\n",
    "    # Normalize to sum to 1\n",
    "    total = float(w.sum())\n",
    "    if total <= 0:\n",
    "        return np.full(n, 1.0 / n, dtype=float)\n",
    "    return w / total\n",
    "\n",
    "\n",
    "def inverse_variance_weights(cov: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Inverse-variance portfolio: w_i ∝ 1 / cov_ii, then normalized.\n",
    "    \"\"\"\n",
    "    diag = np.diag(cov)\n",
    "    diag = np.where(diag <= 0.0, np.nan, diag)\n",
    "    inv = 1.0 / diag\n",
    "    inv = np.where(np.isfinite(inv), inv, 0.0)\n",
    "    s = float(inv.sum())\n",
    "    if s <= 0.0:\n",
    "        return np.full(len(diag), 1.0 / len(diag), dtype=float)\n",
    "    return inv / s\n",
    "\n",
    "\n",
    "def equal_weights(n_assets: int) -> np.ndarray:\n",
    "    return np.full(n_assets, 1.0 / n_assets, dtype=float)\n",
    "\n",
    "\n",
    "# --------------------- Portfolio construction ------------------- #\n",
    "\n",
    "\n",
    "def build_portfolios(cfg: Config, prices: pd.DataFrame) -> Tuple[pd.DataFrame, Dict[str, Dict]]:\n",
    "    \"\"\"\n",
    "    Given price series, build EW, inverse-variance, and HRP portfolios.\n",
    "\n",
    "    Returns:\n",
    "        df_out: DataFrame with prices, returns, weights, and portfolio returns.\n",
    "        stats:  Dict with weights and performance metrics.\n",
    "    \"\"\"\n",
    "    assets = list(cfg.symbols)\n",
    "    ret_assets = prices.pct_change().dropna()\n",
    "    idx = ret_assets.index\n",
    "    n_assets = len(assets)\n",
    "\n",
    "    # Covariance / correlation on entire sample\n",
    "    cov = ret_assets.cov().values\n",
    "    corr = ret_assets.corr().values\n",
    "\n",
    "    # Hierarchical clustering → order\n",
    "    dist = corr_to_distance(corr)\n",
    "    tree = hierarchical_clustering_single_link(dist)\n",
    "    order = tree_to_order(tree)\n",
    "\n",
    "    # Build portfolios\n",
    "    w_ew = equal_weights(n_assets)\n",
    "    w_iv = inverse_variance_weights(cov)\n",
    "    w_hrp = hrp_allocation(cov, order)\n",
    "\n",
    "    # Ensure numerical stability\n",
    "    def _normalize(w: np.ndarray) -> np.ndarray:\n",
    "        w = np.clip(w, 0.0, None)\n",
    "        s = float(w.sum())\n",
    "        if s <= 0.0:\n",
    "            return np.full_like(w, 1.0 / len(w))\n",
    "        return w / s\n",
    "\n",
    "    w_ew = _normalize(w_ew)\n",
    "    w_iv = _normalize(w_iv)\n",
    "    w_hrp = _normalize(w_hrp)\n",
    "\n",
    "    # Portfolio returns (static weights)\n",
    "    rets_ew = ret_assets.values @ w_ew\n",
    "    rets_iv = ret_assets.values @ w_iv\n",
    "    rets_hrp = ret_assets.values @ w_hrp\n",
    "\n",
    "    port_rets = pd.DataFrame(\n",
    "        {\n",
    "            \"ret_ew\": rets_ew,\n",
    "            \"ret_iv\": rets_iv,\n",
    "            \"ret_hrp\": rets_hrp,\n",
    "        },\n",
    "        index=idx,\n",
    "    )\n",
    "\n",
    "    # Build weights DataFrame (constant through time)\n",
    "    weights_df = pd.DataFrame(\n",
    "        {\n",
    "            **{f\"w_ew_{sym}\": w_ew[i] for i, sym in enumerate(assets)},\n",
    "            **{f\"w_iv_{sym}\": w_iv[i] for i, sym in enumerate(assets)},\n",
    "            **{f\"w_hrp_{sym}\": w_hrp[i] for i, sym in enumerate(assets)},\n",
    "        },\n",
    "        index=[idx[0]],\n",
    "    )\n",
    "\n",
    "    # Broadcast weights to all dates\n",
    "    weights_full = weights_df.reindex(idx, method=\"ffill\")\n",
    "\n",
    "    # Combine everything\n",
    "    prices_aligned = prices.reindex(idx)\n",
    "    out_df = pd.concat(\n",
    "        [\n",
    "            prices_aligned.add_prefix(\"px_\"),\n",
    "            ret_assets.add_prefix(\"ret_\"),\n",
    "            weights_full,\n",
    "            port_rets,\n",
    "        ],\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    # Performance stats\n",
    "    stats = {\n",
    "        \"weights\": {\n",
    "            \"assets\": assets,\n",
    "            \"order_indices\": [int(i) for i in order],\n",
    "            \"order_symbols\": [assets[i] for i in order],\n",
    "            \"w_ew\": w_ew.tolist(),\n",
    "            \"w_iv\": w_iv.tolist(),\n",
    "            \"w_hrp\": w_hrp.tolist(),\n",
    "        },\n",
    "        \"performance\": {\n",
    "            \"ret_ew\": annualized_stats(port_rets[\"ret_ew\"]),\n",
    "            \"ret_iv\": annualized_stats(port_rets[\"ret_iv\"]),\n",
    "            \"ret_hrp\": annualized_stats(port_rets[\"ret_hrp\"]),\n",
    "        },\n",
    "    }\n",
    "\n",
    "    return out_df, stats\n",
    "\n",
    "\n",
    "# ----------------------------- Main ----------------------------- #\n",
    "\n",
    "\n",
    "def run_pipeline(cfg: Config) -> None:\n",
    "    # 1) Load prices\n",
    "    prices = load_price_series(cfg)\n",
    "    print(\n",
    "        f\"[INFO] Loaded prices for {cfg.symbols} from \"\n",
    "        f\"{prices.index.min().date()} to {prices.index.max().date()} \"\n",
    "        f\"(n={len(prices)})\"\n",
    "    )\n",
    "\n",
    "    # 2) Build portfolios\n",
    "    out_df, stats = build_portfolios(cfg, prices)\n",
    "\n",
    "    # 3) Save CSV\n",
    "    out_df.to_csv(cfg.out_csv)\n",
    "    print(f\"[OK] Saved daily portfolios → {cfg.out_csv}\")\n",
    "\n",
    "    # 4) Save JSON summary\n",
    "    summary = {\n",
    "        \"symbols\": list(cfg.symbols),\n",
    "        \"start\": str(prices.index.min().date()),\n",
    "        \"end\": str(prices.index.max().date()),\n",
    "        \"stats\": stats,\n",
    "    }\n",
    "    with open(cfg.out_json, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "    print(f\"[OK] Saved summary → {cfg.out_json}\")\n",
    "\n",
    "    # 5) Print quick stats\n",
    "    print(\"\\n[SUMMARY — Annualized stats]\")\n",
    "    for key, s in stats[\"performance\"].items():\n",
    "        print(\n",
    "            f\"  {key:8s}: \"\n",
    "            f\"CAGR={s['cagr']:.2%}, \"\n",
    "            f\"Vol={s['vol']:.2%}, \"\n",
    "            f\"Sharpe={s['sharpe']:.2f}, \"\n",
    "            f\"MaxDD={s['max_drawdown']:.2%}\"\n",
    "        )\n",
    "\n",
    "    print(\"\\n[WEIGHTS]\")\n",
    "    for name, w in [\n",
    "        (\"EW\", stats[\"weights\"][\"w_ew\"]),\n",
    "        (\"IV\", stats[\"weights\"][\"w_iv\"]),\n",
    "        (\"HRP\", stats[\"weights\"][\"w_hrp\"]),\n",
    "    ]:\n",
    "        line = \", \".join(\n",
    "            f\"{sym}={weight:.2%}\"\n",
    "            for sym, weight in zip(stats[\"weights\"][\"assets\"], w)\n",
    "        )\n",
    "        print(f\"  {name:3s}: {line}\")\n",
    "\n",
    "\n",
    "def main() -> None:\n",
    "    cfg = Config()\n",
    "    run_pipeline(cfg)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Jupyter-safe: strip any unwanted args like \"-f kernel-xxxx.json\"\n",
    "    import sys\n",
    "\n",
    "    sys.argv = [sys.argv[0]]\n",
    "    main()\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loaded prices for ('SPY', 'QQQ', 'TLT', 'GLD') from 2010-01-04 to 2025-12-02 (n=4004)\n",
      "[OK] Saved daily portfolios → level58_hrp_portfolios.csv\n",
      "[OK] Saved summary → level58_hrp_summary.json\n",
      "\n",
      "[SUMMARY — Annualized stats]\n",
      "  ret_ew  : CAGR=11.87%, Vol=10.41%, Sharpe=1.14, MaxDD=-25.15%\n",
      "  ret_iv  : CAGR=10.60%, Vol=9.55%, Sharpe=1.11, MaxDD=-24.61%\n",
      "  ret_hrp : CAGR=9.40%, Vol=9.39%, Sharpe=1.00, MaxDD=-24.39%\n",
      "\n",
      "[WEIGHTS]\n",
      "  EW : SPY=25.00%, QQQ=25.00%, TLT=25.00%, GLD=25.00%\n",
      "  IV : SPY=23.89%, QQQ=16.61%, TLT=31.04%, GLD=28.46%\n",
      "  HRP: SPY=17.43%, QQQ=12.12%, TLT=36.75%, GLD=33.70%\n"
     ]
    }
   ],
   "execution_count": 1
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
