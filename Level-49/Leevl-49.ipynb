{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-19T01:01:52.943566Z",
     "start_time": "2025-11-19T01:01:19.724192Z"
    }
   },
   "source": [
    "# level49_shap_explainability.py\n",
    "#\n",
    "# Level-49 — SHAP Explainability for Signals\n",
    "#\n",
    "# Pipeline:\n",
    "#   1) Load daily prices (yfinance → fallback synthetic GBM).\n",
    "#   2) Build forward-horizon labels (imbalanced).\n",
    "#   3) Create lagged-return and rolling-vol features.\n",
    "#   4) Use Purged K-Fold + embargo to evaluate a tree classifier.\n",
    "#   5) Fit final GradientBoosting model on full sample.\n",
    "#   6) Compute SHAP attributions (if 'shap' is installed) on a sample of rows.\n",
    "#   7) Save:\n",
    "#        - events/labels CSV\n",
    "#        - feature matrix CSV\n",
    "#        - SHAP attributions CSV (if available)\n",
    "#        - feature importance CSV\n",
    "#\n",
    "# DSA Concept (SHAP + top-k attributions):\n",
    "#   - Features matrix X is an n×d array (n samples, d features).\n",
    "#   - A tree model partitions feature space; SHAP assigns a contribution\n",
    "#     φ_{i,j} for each (sample i, feature j) so that:\n",
    "#         f(x_i) ≈ φ_{i,0} + φ_{i,1} + ... + φ_{i,d}\n",
    "#     where φ_{i,0} is a \"base value\" (average prediction).\n",
    "#   - Internally, TreeSHAP walks each tree and uses dynamic programming to\n",
    "#     accumulate contributions from splits touching feature j, in O(T*L^2)\n",
    "#     where T is number of trees and L tree depth.\n",
    "#   - We then:\n",
    "#       * Aggregate |φ_{i,j}| across i to get global importance scores:\n",
    "#           I_j = (1/n) * Σ_i |φ_{i,j}|\n",
    "#       * For each row, sort features by |φ_{i,j}| and keep top-k drivers.\n",
    "#     These are basically top-k operations (arg-sort, slicing) on columns,\n",
    "#     which are vectorized on NumPy arrays and run in O(d log d) per row.\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "from dataclasses import dataclass\n",
    "from typing import Iterator, Tuple, Dict, Any, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.base import BaseEstimator, clone\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from sklearn.model_selection import BaseCrossValidator\n",
    "\n",
    "# Optional dependencies\n",
    "try:\n",
    "    import yfinance as yf\n",
    "except ImportError:\n",
    "    yf = None\n",
    "\n",
    "try:\n",
    "    import shap\n",
    "except ImportError:\n",
    "    shap = None\n",
    "\n",
    "\n",
    "# -------------------- Config -------------------- #\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    symbol: str = \"SPY\"\n",
    "    start: str = \"2010-01-01\"\n",
    "\n",
    "    # Labeling\n",
    "    horizon: int = 20              # forward horizon in daily bars\n",
    "    pos_threshold: float = 0.005   # label 1 if fwd_ret > this\n",
    "\n",
    "    # Features\n",
    "    max_lag: int = 5\n",
    "    roll_vol_window: int = 20\n",
    "\n",
    "    # CV / leakage guard\n",
    "    n_splits: int = 5\n",
    "    embargo_pct: float = 0.01\n",
    "\n",
    "    # Synthetic fallback\n",
    "    synthetic_len: int = 252 * 10\n",
    "    random_state: int = 42\n",
    "\n",
    "    # SHAP sampling (for speed)\n",
    "    shap_sample_size: int = 500\n",
    "    top_k_per_row: int = 3\n",
    "\n",
    "    # Outputs\n",
    "    out_events_csv: str = \"level49_events.csv\"\n",
    "    out_features_csv: str = \"level49_features.csv\"\n",
    "    out_cv_csv: str = \"level49_cv_results.csv\"\n",
    "    out_shap_csv: str = \"level49_shap_values.csv\"\n",
    "    out_importance_csv: str = \"level49_feature_importance.csv\"\n",
    "    out_summary_json: str = \"level49_summary.json\"\n",
    "\n",
    "\n",
    "# -------------------- Data & Labels -------------------- #\n",
    "\n",
    "def generate_synthetic_series(cfg: Config) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Synthetic GBM-like daily series: offline fallback.\n",
    "\n",
    "    DSA:\n",
    "    - Vectorized generation of log-returns and cumulative sum.\n",
    "    \"\"\"\n",
    "    np.random.seed(cfg.random_state)\n",
    "    n = cfg.synthetic_len\n",
    "    idx = pd.date_range(start=cfg.start, periods=n, freq=\"B\")\n",
    "\n",
    "    mu = 0.08 / 252.0\n",
    "    sigma = 0.20 / np.sqrt(252.0)\n",
    "    ret = np.random.normal(mu, sigma, size=n)\n",
    "\n",
    "    price0 = 100.0\n",
    "    price = price0 * np.exp(np.cumsum(ret))\n",
    "\n",
    "    df = pd.DataFrame({\"close\": price, \"ret\": ret}, index=idx)\n",
    "    print(\"[WARN] Using synthetic GBM-like series instead of real market data.\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_price_series(cfg: Config) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Try yfinance; if it fails, fall back to synthetic series.\n",
    "\n",
    "    Returns DataFrame with ['close', 'ret'].\n",
    "    \"\"\"\n",
    "    if yf is None:\n",
    "        print(\"[WARN] yfinance not installed. Falling back to synthetic time series.\")\n",
    "        return generate_synthetic_series(cfg)\n",
    "\n",
    "    try:\n",
    "        px = yf.download(cfg.symbol, start=cfg.start, auto_adjust=True, progress=False)\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] yfinance download failed ({e}). Falling back to synthetic series.\")\n",
    "        return generate_synthetic_series(cfg)\n",
    "\n",
    "    if px is None or px.empty:\n",
    "        print(\"[WARN] yfinance returned empty DataFrame. Falling back to synthetic series.\")\n",
    "        return generate_synthetic_series(cfg)\n",
    "\n",
    "    # Try 'Close'; fall back to 'Adj Close' if needed\n",
    "    if \"Close\" in px.columns:\n",
    "        close_obj = px[\"Close\"]\n",
    "    elif \"Adj Close\" in px.columns:\n",
    "        close_obj = px[\"Adj Close\"]\n",
    "    else:\n",
    "        print(\"[WARN] No 'Close'/'Adj Close' column. Falling back to synthetic series.\")\n",
    "        return generate_synthetic_series(cfg)\n",
    "\n",
    "    if isinstance(close_obj, pd.DataFrame):\n",
    "        close_series = close_obj.iloc[:, 0].astype(float)\n",
    "    else:\n",
    "        close_series = close_obj.astype(float)\n",
    "\n",
    "    close_series = close_series.rename(\"close\")\n",
    "    ret = np.log(close_series).diff().rename(\"ret\")\n",
    "\n",
    "    df = pd.concat([close_series, ret], axis=1).dropna()\n",
    "    return df\n",
    "\n",
    "\n",
    "def build_forward_labels(df: pd.DataFrame, cfg: Config) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Forward horizon labels with explicit threshold.\n",
    "\n",
    "    y = 1 if fwd_ret > cfg.pos_threshold else 0\n",
    "\n",
    "    DSA:\n",
    "    - rolling(window=h).sum() is effectively a sliding window / prefix-sum.\n",
    "    - shift(-h + 1) aligns the end of the forward window with current time.\n",
    "    \"\"\"\n",
    "    h = cfg.horizon\n",
    "\n",
    "    fwd_ret = df[\"ret\"].rolling(window=h).sum().shift(-h + 1)\n",
    "    fwd_ret = fwd_ret.rename(\"fwd_ret\")\n",
    "\n",
    "    t1 = df.index.to_series().shift(-h + 1)\n",
    "    t1.name = \"t1\"\n",
    "\n",
    "    events = pd.concat([df[\"close\"], df[\"ret\"], fwd_ret, t1], axis=1)\n",
    "    events = events.dropna(subset=[\"fwd_ret\", \"t1\"])\n",
    "\n",
    "    y = (events[\"fwd_ret\"] > cfg.pos_threshold).astype(int).rename(\"label\")\n",
    "    events = events.join(y)\n",
    "\n",
    "    return events\n",
    "\n",
    "\n",
    "def build_features(events: pd.DataFrame, cfg: Config) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build basic price-based features:\n",
    "      - Lagged returns: ret_lag1 .. ret_lag{max_lag}\n",
    "      - Rolling volatility: roll_vol{window}\n",
    "\n",
    "    DSA:\n",
    "    - Each lag is a shift (O(n)).\n",
    "    - Rolling std uses an online formula internally (O(n)), conceptually\n",
    "      equivalent to a sliding window with prefix sums of x and x^2.\n",
    "    \"\"\"\n",
    "    cols: List[pd.Series] = []\n",
    "\n",
    "    for lag in range(1, cfg.max_lag + 1):\n",
    "        col = events[\"ret\"].shift(lag).rename(f\"ret_lag{lag}\")\n",
    "        cols.append(col)\n",
    "\n",
    "    roll_vol = events[\"ret\"].rolling(window=cfg.roll_vol_window).std().rename(\n",
    "        f\"roll_vol{cfg.roll_vol_window}\"\n",
    "    )\n",
    "    cols.append(roll_vol)\n",
    "\n",
    "    X = pd.concat(cols, axis=1).dropna()\n",
    "    return X\n",
    "\n",
    "\n",
    "# -------------------- Purged K-Fold -------------------- #\n",
    "\n",
    "class PurgedKFold(BaseCrossValidator):\n",
    "    \"\"\"\n",
    "    Purged K-Fold with embargo for event-based time-series data.\n",
    "\n",
    "    DSA:\n",
    "      - We have n samples (nodes) with intervals [index[i], t1[i]].\n",
    "      - For each fold:\n",
    "          * Mark a contiguous block as test.\n",
    "          * Apply embargo (mask future indices after test).\n",
    "          * For candidate train indices, check interval overlap:\n",
    "              start_train <= max_end_test AND end_train >= start_test\n",
    "            done with vectorized boolean ops on arrays (O(n)).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_splits: int = 5, embargo_pct: float = 0.0, t1: pd.Series | None = None):\n",
    "        if n_splits < 2:\n",
    "            raise ValueError(\"n_splits must be at least 2.\")\n",
    "        self.n_splits = n_splits\n",
    "        self.embargo_pct = float(embargo_pct)\n",
    "        self.t1 = t1\n",
    "\n",
    "    def get_n_splits(self, X=None, y=None, groups=None) -> int:\n",
    "        return self.n_splits\n",
    "\n",
    "    def split(self, X, y=None, groups=None) -> Iterator[Tuple[np.ndarray, np.ndarray]]:\n",
    "        if self.t1 is None:\n",
    "            raise ValueError(\"PurgedKFold requires t1 to be set.\")\n",
    "        if not isinstance(X, (pd.DataFrame, pd.Series)):\n",
    "            raise TypeError(\"X must be a pandas DataFrame/Series with an Index.\")\n",
    "\n",
    "        idx = np.array(X.index)\n",
    "        t1 = self.t1.reindex(idx)\n",
    "        if t1.isna().any():\n",
    "            raise ValueError(\"t1 must have non-null values for all X indices.\")\n",
    "\n",
    "        n = len(idx)\n",
    "        indices = np.arange(n)\n",
    "\n",
    "        fold_sizes = np.full(self.n_splits, n // self.n_splits, dtype=int)\n",
    "        fold_sizes[: n % self.n_splits] += 1\n",
    "\n",
    "        current = 0\n",
    "        n_embargo = int(np.ceil(self.embargo_pct * n))\n",
    "\n",
    "        for fold_size in fold_sizes:\n",
    "            start = current\n",
    "            stop = current + fold_size\n",
    "            current = stop\n",
    "\n",
    "            test_mask = np.zeros(n, dtype=bool)\n",
    "            test_mask[start:stop] = True\n",
    "            test_idx = indices[test_mask]\n",
    "\n",
    "            embargo_mask = np.zeros(n, dtype=bool)\n",
    "            if n_embargo > 0:\n",
    "                emb_start = stop\n",
    "                emb_end = min(n, stop + n_embargo)\n",
    "                embargo_mask[emb_start:emb_end] = True\n",
    "\n",
    "            train_mask = ~(test_mask | embargo_mask)\n",
    "            train_candidates = indices[train_mask]\n",
    "\n",
    "            test_start_time = idx[start]\n",
    "            test_end_time = t1.iloc[test_idx].max()\n",
    "\n",
    "            train_starts = idx[train_candidates]\n",
    "            train_ends = t1.iloc[train_candidates].values\n",
    "\n",
    "            overlap = (train_starts <= test_end_time) & (train_ends >= test_start_time)\n",
    "            final_train = train_candidates[~overlap]\n",
    "\n",
    "            yield final_train, test_idx\n",
    "\n",
    "\n",
    "# -------------------- Metrics -------------------- #\n",
    "\n",
    "def sharpe_ratio(returns: pd.Series, ann_factor: int = 252) -> float:\n",
    "    if returns is None or len(returns) < 2:\n",
    "        return 0.0\n",
    "    mu = float(returns.mean())\n",
    "    sigma = float(returns.std())\n",
    "    if sigma <= 0:\n",
    "        return 0.0\n",
    "    return mu / sigma * np.sqrt(ann_factor)\n",
    "\n",
    "\n",
    "def evaluate_model_cv(\n",
    "    model: BaseEstimator,\n",
    "    X: pd.DataFrame,\n",
    "    y: np.ndarray,\n",
    "    cv: BaseCrossValidator,\n",
    "    label: str = \"\"\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Purged K-Fold evaluation:\n",
    "      - ROC-AUC, PR-AUC.\n",
    "    \"\"\"\n",
    "    metrics = []\n",
    "    fold_id = 1\n",
    "\n",
    "    for train_idx, test_idx in cv.split(X, y):\n",
    "        clf = clone(model)\n",
    "\n",
    "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "        clf.fit(X_train, y_train)\n",
    "        prob = clf.predict_proba(X_test)[:, 1]\n",
    "        roc = roc_auc_score(y_test, prob)\n",
    "        pr = average_precision_score(y_test, prob)\n",
    "\n",
    "        metrics.append({\"fold\": fold_id, \"roc_auc\": roc, \"pr_auc\": pr})\n",
    "        fold_id += 1\n",
    "\n",
    "    df = pd.DataFrame(metrics)\n",
    "    df[\"roc_auc_mean\"] = df[\"roc_auc\"].mean()\n",
    "    df[\"roc_auc_std\"] = df[\"roc_auc\"].std()\n",
    "    df[\"pr_auc_mean\"] = df[\"pr_auc\"].mean()\n",
    "    df[\"pr_auc_std\"] = df[\"pr_auc\"].std()\n",
    "    df[\"model\"] = label\n",
    "\n",
    "    return {\n",
    "        \"df\": df,\n",
    "        \"metrics\": {\n",
    "            \"roc_auc_mean\": float(df['roc_auc_mean'].iloc[0]),\n",
    "            \"roc_auc_std\": float(df['roc_auc_std'].iloc[0]),\n",
    "            \"pr_auc_mean\": float(df['pr_auc_mean'].iloc[0]),\n",
    "            \"pr_auc_std\": float(df['pr_auc_std'].iloc[0]),\n",
    "        },\n",
    "    }\n",
    "\n",
    "\n",
    "# -------------------- SHAP Helpers -------------------- #\n",
    "\n",
    "def compute_tree_shap(\n",
    "    model: GradientBoostingClassifier,\n",
    "    X: pd.DataFrame,\n",
    "    cfg: Config\n",
    ") -> Tuple[pd.DataFrame, Dict[str, float], float]:\n",
    "    \"\"\"\n",
    "    Compute SHAP attributions on a sampled subset of X.\n",
    "\n",
    "    Returns:\n",
    "      shap_df: per-row per-feature SHAP values (+ base_value, pred_proba)\n",
    "      global_importance: feature → mean(|phi_j|)\n",
    "      base_value: scalar base value used by TreeExplainer\n",
    "\n",
    "    DSA:\n",
    "      - shap.TreeExplainer(model).shap_values(X_sample) yields an n×d matrix\n",
    "        (for binary classification, we use the positive class).\n",
    "      - global_importance[j] = mean over i of |phi_{i,j}|.\n",
    "      - All operations are vectorized across samples/features.\n",
    "    \"\"\"\n",
    "    if shap is None:\n",
    "        print(\"[WARN] 'shap' library not installed. Skipping SHAP computation.\")\n",
    "        return pd.DataFrame(index=X.index), {}, 0.0\n",
    "\n",
    "    # Sample (for speed) if needed\n",
    "    if len(X) > cfg.shap_sample_size:\n",
    "        X_sample = X.sample(cfg.shap_sample_size, random_state=cfg.random_state)\n",
    "    else:\n",
    "        X_sample = X.copy()\n",
    "\n",
    "    explainer = shap.TreeExplainer(model)\n",
    "    shap_vals_raw = explainer.shap_values(X_sample)\n",
    "\n",
    "    # For binary classifier, shap_values may be:\n",
    "    #   - array (n, d) for positive class\n",
    "    #   - or list [neg_class, pos_class]; we pick pos_class\n",
    "    if isinstance(shap_vals_raw, list):\n",
    "        shap_vals = np.array(shap_vals_raw[1])\n",
    "    else:\n",
    "        shap_vals = np.array(shap_vals_raw)\n",
    "\n",
    "    if shap_vals.ndim == 1:\n",
    "        shap_vals = shap_vals.reshape(-1, 1)\n",
    "\n",
    "    base_value = explainer.expected_value\n",
    "    if isinstance(base_value, (list, np.ndarray)):\n",
    "        base_value = float(base_value[1]) if len(np.atleast_1d(base_value)) > 1 else float(base_value[0])\n",
    "    else:\n",
    "        base_value = float(base_value)\n",
    "\n",
    "    feature_names = list(X_sample.columns)\n",
    "    shap_df = pd.DataFrame(shap_vals, index=X_sample.index, columns=feature_names)\n",
    "\n",
    "    # Global importance: mean absolute SHAP per feature\n",
    "    mean_abs = np.abs(shap_vals).mean(axis=0)\n",
    "    global_importance = {\n",
    "        name: float(val) for name, val in zip(feature_names, mean_abs)\n",
    "    }\n",
    "\n",
    "    # Add metadata columns for convenience\n",
    "    with np.errstate(over=\"ignore\", invalid=\"ignore\"):\n",
    "        pred_proba = model.predict_proba(X_sample)[:, 1]\n",
    "\n",
    "    shap_df[\"base_value\"] = base_value\n",
    "    shap_df[\"pred_proba\"] = pred_proba\n",
    "\n",
    "    return shap_df, global_importance, base_value\n",
    "\n",
    "\n",
    "def build_top_k_driver_table(\n",
    "    shap_df: pd.DataFrame,\n",
    "    cfg: Config\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build a compact table of top-k drivers per sample.\n",
    "\n",
    "    For each index:\n",
    "       - Take the row's SHAP values (features cols only),\n",
    "       - Sort by |phi_{i,j}| descending,\n",
    "       - Keep top-k features and their SHAP values.\n",
    "\n",
    "    DSA:\n",
    "      - For each row, this is an arg-sort + slicing (O(d log d)).\n",
    "      - Implemented via vectorized operations over columns and\n",
    "        a row-wise apply (fine at d ~ 6–20).\n",
    "\n",
    "    Returns:\n",
    "      DataFrame with columns:\n",
    "        ['timestamp', 'pred_proba', 'feature_1', 'shap_1', 'feature_2', 'shap_2', ...]\n",
    "    \"\"\"\n",
    "    if shap_df.empty:\n",
    "        return shap_df\n",
    "\n",
    "    feature_cols = [c for c in shap_df.columns if c not in (\"base_value\", \"pred_proba\")]\n",
    "    out_rows = []\n",
    "\n",
    "    for idx, row in shap_df[feature_cols + [\"pred_proba\"]].iterrows():\n",
    "        vals = row[feature_cols].values.astype(float)\n",
    "        order = np.argsort(-np.abs(vals))  # descending by |SHAP|\n",
    "        top_idx = order[: cfg.top_k_per_row]\n",
    "\n",
    "        record: Dict[str, Any] = {\n",
    "            \"timestamp\": idx,\n",
    "            \"pred_proba\": float(row[\"pred_proba\"]),\n",
    "        }\n",
    "\n",
    "        for rank, j in enumerate(top_idx, start=1):\n",
    "            fname = feature_cols[j]\n",
    "            sval = float(vals[j])\n",
    "            record[f\"feature_{rank}\"] = fname\n",
    "            record[f\"shap_{rank}\"] = sval\n",
    "\n",
    "        out_rows.append(record)\n",
    "\n",
    "    topk_df = pd.DataFrame(out_rows).set_index(\"timestamp\")\n",
    "    return topk_df\n",
    "\n",
    "\n",
    "# -------------------- Main Pipeline -------------------- #\n",
    "\n",
    "def run_demo(cfg: Config) -> None:\n",
    "    # 1) Load data\n",
    "    df = load_price_series(cfg)\n",
    "\n",
    "    # 2) Build labels/events\n",
    "    events = build_forward_labels(df, cfg)\n",
    "    print(\"[INFO] Label distribution:\")\n",
    "    print(events[\"label\"].value_counts(dropna=False))\n",
    "\n",
    "    # 3) Features\n",
    "    X_full = build_features(events, cfg)\n",
    "    y = events.loc[X_full.index, \"label\"].values\n",
    "    t1 = events.loc[X_full.index, \"t1\"]\n",
    "\n",
    "    # 4) CV splitter\n",
    "    cv = PurgedKFold(\n",
    "        n_splits=cfg.n_splits,\n",
    "        embargo_pct=cfg.embargo_pct,\n",
    "        t1=t1\n",
    "    )\n",
    "\n",
    "    # 5) Tree model (Gradient Boosting)\n",
    "    model = GradientBoostingClassifier(\n",
    "        n_estimators=200,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=3,\n",
    "        subsample=0.8,\n",
    "        random_state=cfg.random_state\n",
    "    )\n",
    "\n",
    "    # 6) Purged K-Fold evaluation\n",
    "    res_cv = evaluate_model_cv(\n",
    "        model=model,\n",
    "        X=X_full,\n",
    "        y=y,\n",
    "        cv=cv,\n",
    "        label=\"gbm_for_shap\"\n",
    "    )\n",
    "\n",
    "    df_cv = res_cv[\"df\"]\n",
    "    df_cv.to_csv(cfg.out_cv_csv, index=False)\n",
    "    print(f\"[OK] Saved CV results → {cfg.out_cv_csv}\")\n",
    "\n",
    "    # 7) Fit final model on full data\n",
    "    model_full = clone(model)\n",
    "    model_full.fit(X_full, y)\n",
    "\n",
    "    # 8) SHAP explainability\n",
    "    shap_df, global_importance, base_value = compute_tree_shap(\n",
    "        model=model_full,\n",
    "        X=X_full,\n",
    "        cfg=cfg\n",
    "    )\n",
    "\n",
    "    # 9) Save raw artifacts: events & features & SHAP\n",
    "    events.to_csv(cfg.out_events_csv, index_label=\"timestamp\")\n",
    "    X_full.to_csv(cfg.out_features_csv, index_label=\"timestamp\")\n",
    "    print(f\"[OK] Saved events → {cfg.out_events_csv}\")\n",
    "    print(f\"[OK] Saved features → {cfg.out_features_csv}\")\n",
    "\n",
    "    if not shap_df.empty:\n",
    "        shap_df.to_csv(cfg.out_shap_csv, index_label=\"timestamp\")\n",
    "        print(f\"[OK] Saved SHAP values → {cfg.out_shap_csv}\")\n",
    "\n",
    "        # Global importance as DataFrame\n",
    "        imp_df = pd.DataFrame(\n",
    "            {\"feature\": list(global_importance.keys()),\n",
    "             \"mean_abs_shap\": list(global_importance.values())}\n",
    "        ).sort_values(\"mean_abs_shap\", ascending=False)\n",
    "        imp_df.to_csv(cfg.out_importance_csv, index=False)\n",
    "        print(f\"[OK] Saved feature importance → {cfg.out_importance_csv}\")\n",
    "\n",
    "        # Top-k driver table (compact)\n",
    "        topk_df = build_top_k_driver_table(shap_df, cfg)\n",
    "        # Save this as a convenience CSV\n",
    "        topk_path = cfg.out_shap_csv.replace(\".csv\", \"_topk.csv\")\n",
    "        topk_df.to_csv(topk_path, index_label=\"timestamp\")\n",
    "        print(f\"[OK] Saved top-{cfg.top_k_per_row} driver table → {topk_path}\")\n",
    "\n",
    "        # Quick importance bar plot\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        imp_df.head(10).set_index(\"feature\")[\"mean_abs_shap\"].plot(kind=\"bar\")\n",
    "        plt.title(\"Level-49: Top Features by mean(|SHAP|)\")\n",
    "        plt.ylabel(\"mean(|SHAP|)\")\n",
    "        plt.grid(axis=\"y\", alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"[INFO] SHAP results are empty (likely no 'shap' installed).\")\n",
    "\n",
    "    # 10) Save JSON summary (metrics + config + base_value)\n",
    "    summary = {\n",
    "        \"cv_metrics\": res_cv[\"metrics\"],\n",
    "        \"base_value\": base_value,\n",
    "        \"symbol\": cfg.symbol,\n",
    "        \"start\": cfg.start,\n",
    "        \"horizon\": cfg.horizon,\n",
    "        \"pos_threshold\": cfg.pos_threshold,\n",
    "        \"max_lag\": cfg.max_lag,\n",
    "        \"roll_vol_window\": cfg.roll_vol_window,\n",
    "        \"n_splits\": cfg.n_splits,\n",
    "        \"embargo_pct\": cfg.embargo_pct,\n",
    "    }\n",
    "    with open(cfg.out_summary_json, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "    print(f\"[OK] Saved summary → {cfg.out_summary_json}\")\n",
    "\n",
    "    print(\"\\n=== CV Summary ===\")\n",
    "    m = res_cv[\"metrics\"]\n",
    "    print(\n",
    "        f\"GBM | ROC-AUC: {m['roc_auc_mean']:.3f}±{m['roc_auc_std']:.3f} \"\n",
    "        f\"| PR-AUC: {m['pr_auc_mean']:.3f}±{m['pr_auc_std']:.3f}\"\n",
    "    )\n",
    "\n",
    "\n",
    "# -------------------- Main -------------------- #\n",
    "\n",
    "def main():\n",
    "    cfg = Config()\n",
    "    run_demo(cfg)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Jupyter / PyCharm-safe: strip any '-f kernel-xxxx.json' args\n",
    "    import sys\n",
    "    sys.argv = [sys.argv[0]]\n",
    "    main()\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Label distribution:\n",
      "label\n",
      "1    2557\n",
      "0    1418\n",
      "Name: count, dtype: int64\n",
      "[OK] Saved CV results → level49_cv_results.csv\n",
      "[WARN] 'shap' library not installed. Skipping SHAP computation.\n",
      "[OK] Saved events → level49_events.csv\n",
      "[OK] Saved features → level49_features.csv\n",
      "[INFO] SHAP results are empty (likely no 'shap' installed).\n",
      "[OK] Saved summary → level49_summary.json\n",
      "\n",
      "=== CV Summary ===\n",
      "GBM | ROC-AUC: 0.446±0.009 | PR-AUC: 0.608±0.033\n"
     ]
    }
   ],
   "execution_count": 1
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
