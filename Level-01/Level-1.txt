This Level-1 SMA Crossover engine is a perfect example of how a simple DSA idea — **sliding windows over a time series** — turns into a complete, production-style quant research script: data loader, signal engine, backtester, metrics, and CLI.

I’ll walk through three layers:

1. **DSA concept**: sliding window / rolling stats and why they matter
2. **Code architecture**: function by function, how the engine works
3. **Real-time quant usage**: how you’d actually use this on SPY/BTC, pitfalls, and extensions

---

## 1. DSA Concept: Sliding Window over a Price Series

At the heart of this model is the **simple moving average**: for each day ( t ), you take the last ( N ) closing prices and average them. In DSA terms, this is a **sliding window** (or rolling window) over an array.

Naively, for each day you could sum the last ( N ) elements again and again, which is (O(N)) per day and (O(NT)) across (T) days. In algorithm design, we improve this using:

* **Prefix sums**: keep a cumulative sum array and subtract to get window sums in O(1).
* Or a **deque / circular buffer**: push the new value, pop the old one, maintain current sum.

In your code, you lean on **pandas’ `rolling().mean()`**, which internally does efficient sliding window operations in vectorized form. Conceptually, though, it’s exactly a sliding window:

* For the **short SMA** (20-day), the window is small and reacts quickly to recent price moves.
* For the **long SMA** (100-day), the window is larger and smoother, capturing long-term trend.

The **signal** emerges from comparing these two windowed averages:

* If **short SMA > long SMA** → price is above long-term trend → **uptrend → +1**.
* If **short SMA < long SMA** → **downtrend → −1**.
  This is classic “moving average crossover” momentum.

So DSA-wise:

* Data structure: arrays (Series) of floats
* Operation: rolling mean (sliding window)
* Complexity: linear (O(T)) with a constant factor, good enough for daily bars over decades and many symbols.

---

## 2. Code Architecture and Flow

### Config dataclass

```python
@dataclass
class Config:
    symbol: str = "SPY"
    short: int = 20
    long: int = 100
    tc_bps: float = 10
    start: str = "2010-01-01"
    out_csv: str = "level1_sma_crossover.csv"
    out_json: str = "level1_sma_summary.json"
```

You encapsulate all tunable parameters into a `Config` dataclass:

* `symbol`: ticker, default SPY
* `short`/`long`: SMA windows
* `tc_bps`: **transaction cost** in basis points (round-trip) — big win for realism
* `start`: data start date
* `out_csv` / `out_json`: output filenames

This is already “research-grade”: no magic numbers scattered around, just one structured config.

---

### Data loader

```python
def load_data(cfg: Config):
    df = yf.download(cfg.symbol, start=cfg.start, auto_adjust=True, progress=False)
    if df.empty:
        raise RuntimeError("Error loading data.")
    df = df[["Close"]].rename(columns={"Close": "close"})
    return df
```

* Uses **yfinance** to pull historical OHLCV.
* `auto_adjust=True` ensures prices are adjusted for splits/dividends — essential for backtests.
* You keep only the **Close** and rename it to `close` for convenience.

This function abstracts away data acquisition, so the rest of the pipeline assumes a clean DataFrame with a `close` column indexed by date.

---

### Signal engine: SMA and crossover

```python
def compute_signals(df: pd.DataFrame, cfg: Config):
    df = df.copy()
    df["sma_s"] = df["close"].rolling(cfg.short).mean()
    df["sma_l"] = df["close"].rolling(cfg.long).mean()
    df["signal"] = np.where(df["sma_s"] > df["sma_l"], 1,
                    np.where(df["sma_s"] < df["sma_l"], -1, 0))
    df["signal"] = df["signal"].ffill().fillna(0)
    return df
```

Key steps:

1. `rolling(cfg.short).mean()` and `rolling(cfg.long).mean()` are the **sliding windows** over the `close` series, computing short and long SMAs.

2. The nested `np.where` encodes the rule:

   * Short > long → **+1** (long)
   * Short < long → **−1** (short)
   * Equal → **0** (flat)

3. `ffill()` forward-fills the last known signal across NaNs (which occur during the warm-up window when SMA isn’t fully defined), then `fillna(0)` for the very early dates.

This transforms a raw price series into a **position signal time series** ( s_t \in {-1,0,1} ), which is exactly what a backtester needs.

---

### Backtest engine: returns, costs, and equity curve

```python
def backtest(df: pd.DataFrame, cfg: Config):
    df = df.copy()
    df["ret"] = df["close"].pct_change().fillna(0)

    # transaction cost: apply when signal flips
    df["flip"] = df["signal"].diff().abs()
    df["tc"] = df["flip"] * (cfg.tc_bps * 1e-4)

    df["strategy_ret"] = df["signal"].shift(1) * df["ret"] - df["tc"]

    df["equity"] = (1 + df["strategy_ret"]).cumprod()
    return df
```

This is where it becomes a **full strategy**, not just a signal:

1. `ret`: daily returns of the asset, ( r_t = \frac{P_t}{P_{t-1}} - 1 ).

2. `flip`: detect **changes in position**:

   * `signal.diff()` is non-zero when you change from +1 to −1, 0 to +1, etc.
   * `.abs()` gives the magnitude (1 for flat→long, 2 for long→short, etc.).
   * You multiply this by the cost per flip.

3. `tc`: transaction cost per day:

   * `cfg.tc_bps * 1e-4` converts **basis points** to a decimal fraction.

     * 10 bps → 0.001 = 0.1% per round-trip flip.
   * Multiply by `flip` so cost only applies when you actually trade.

4. `strategy_ret`:

   * `df["signal"].shift(1) * df["ret"]` means you use **yesterday’s signal** to trade **today’s return** — guards against lookahead bias.
   * Subtract `tc` for that day’s cost.
   * This gives daily PnL of the strategy, net of trading costs.

5. `equity`: cumulative product of `(1 + strategy_ret)` gives the **equity curve**, assuming you start with 1.0 capital.

Algorithmically, this is all vectorized on columns, avoiding explicit Python loops: (O(T)) operations on arrays.

---

### Reporting: saving CSV/JSON and risk metrics

```python
def save_outputs(df: pd.DataFrame, cfg: Config):
    df.to_csv(cfg.out_csv)

    final_equity = float(df["equity"].iloc[-1])
    cagr = (final_equity ** (252 / len(df))) - 1
    sharpe = np.sqrt(252) * df["strategy_ret"].mean() / df["strategy_ret"].std()
```

* You persist the **full path** (dates, prices, signals, returns, equity) to CSV — great for audit and inspection.
* `final_equity`: ending value of the equity curve.
* `CAGR` (compound annual growth rate) is approximated by scaling the total growth over the number of trading days.
* `Sharpe` ratio annualizes the mean/vol of `strategy_ret` assuming ~252 trading days per year.

Then you write a JSON summary with config + metrics and print them — that’s exactly what you’d want in a research pipeline: a human-readable log and a machine-readable summary.

---

### CLI + Jupyter shim

`parse_args()` uses `argparse` to make this a **command-line tool**:

* `--symbol`, `--short`, `--long`, `--tc-bps`, `--start`, `--csv`, `--json`.

`main()` wires everything together, and the Jupyter shim strips out noisy kernel args so you can run:

```bash
python level1_sma_crossover.py --symbol BTC-USD --short 10 --long 50
```

or just run the cell in a notebook.

The `plt.show()` at the end gives a quick visualization of the equity curve.

---

## 3. Real-Time Quant Examples and Extensions

In a real quant/research environment, this script is a **baseline momentum engine**:

* For **SPY** (broad US equity market), the SMA crossover gives a long-term timing model:

  * Often out of the market during prolonged bear markets when price stays below the long SMA.
  * In during sustained bull regimes.
* For **BTC-USD**, the same logic becomes a crude trend-following crypto strategy. You can switch to shorter windows (e.g., 10/50) to deal with higher volatility.

Typical production use:

1. **Daily job**: A cron or scheduler runs this script after market close.
2. **Outputs**:

   * CSV: stored in a research data folder or S3.
   * JSON: ingested by a dashboard to show current position, equity, and performance stats.
3. **Signal consumption**:

   * You can read `df["signal"].iloc[-1]` to see if you should be long, short, or flat tomorrow.
   * You could wire that into an email alert: “SPY SMA crossover flipped from long to flat” with associated metrics.

Some **pitfalls and lessons** this teaches:

* **Lag vs noise**: shorter SMA reacts faster but whipsaws more; longer SMA smoother but delayed. This is a direct DSA tradeoff: shorter sliding window vs longer.
* **Parameter overfitting**: sweeping many SMA pairs (5/20, 10/50, 50/200, etc.) and picking the best Sharpe on the same data is a classic form of overfit. Later models in your roadmap (e.g., adaptive lookback, robust CV) will address this.
* **Transaction costs**: your simple bps model already teaches how **flip frequency** kills Sharpe. If you shorten windows, `flip` increases, `tc` burns performance.
* **Risk metrics**: adding Max Drawdown, volatility, and hit-rate would be natural next steps.

From here, you can naturally extend:

* Wrap this with **volatility-targeted sizing** (your Model #4), i.e., scale the signal by inverse vol to stabilize risk.
* Add **TC-aware comparisons**: run once with `tc_bps=0` and once with realistic costs to quantify slippage impact.
* Turn this into a **multi-asset engine**: loop over a universe of tickers, compute cross-sectional stats (e.g., which assets are trending strongest).

Overall, this Level-1 engine nails exactly what a first quant model should do: it’s DSA-grounded (sliding windows), realistic (transaction costs, no lookahead), reproducible (CSV/JSON outputs), and ready to be chained into more advanced models. When you move to later levels — meta-labeling, Kelly sizing, HRP allocation — this will be the simplest building block you compare against.
