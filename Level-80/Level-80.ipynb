{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-18T03:12:14.388817Z",
     "start_time": "2025-12-18T02:57:54.550675Z"
    }
   },
   "source": [
    "# level80_tcopula_var_es_fast.py\n",
    "# Level-80: FAST t-Copula Portfolio VaR/ES (SciPy optional) + Rolling Backtest\n",
    "#\n",
    "# Fixes you hit previously:\n",
    "# - No np.trapzoid typo (uses np.trapezoid if available, else np.trapz)\n",
    "# - Avoids DataFrame.rename(str) bug (uses Series.name = symbol)\n",
    "# - Robust yfinance column handling (handles MultiIndex columns safely)\n",
    "# - Avoids deprecated Pandas resample('M') usage (not needed here)\n",
    "#\n",
    "# Outputs:\n",
    "#   - level80_tcopula_panel.csv   (daily returns + pnl + rolling VaR/ES)\n",
    "#   - level80_tcopula_summary.json\n",
    "#\n",
    "# Run:\n",
    "#   python level80_tcopula_var_es_fast.py\n",
    "#   python level80_tcopula_var_es_fast.py --sims 100000 --alphas 0.95 0.99\n",
    "#   python level80_tcopula_var_es_fast.py --no-rolling\n",
    "#   python level80_tcopula_var_es_fast.py --weights 0.2 0.2 0.2 0.2 0.1 0.05 0.025 0.025\n",
    "\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "import argparse\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import Tuple, List, Optional, Dict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "\n",
    "# ----------------------------- SciPy optional -----------------------------\n",
    "try:\n",
    "    from scipy import stats  # type: ignore\n",
    "    SCIPY_OK = True\n",
    "except Exception:\n",
    "    SCIPY_OK = False\n",
    "\n",
    "\n",
    "# ----------------------------- Config -----------------------------\n",
    "@dataclass\n",
    "class Config:\n",
    "    symbols: Tuple[str, ...] = (\"SPY\", \"QQQ\", \"IWM\", \"EFA\", \"EEM\", \"TLT\", \"LQD\", \"GLD\")\n",
    "    start: str = \"2010-01-01\"\n",
    "\n",
    "    nu_grid: Tuple[int, ...] = (4, 6, 8, 10, 15, 20)\n",
    "    corr_shrink: float = 0.05\n",
    "\n",
    "    sims: int = 50000\n",
    "    seed: int = 42\n",
    "    alphas: Tuple[float, ...] = (0.95, 0.99)\n",
    "\n",
    "    weights: Optional[List[float]] = None\n",
    "    notional: float = 1_000_000.0\n",
    "\n",
    "    # Rolling backtest\n",
    "    rolling_window: int = 750\n",
    "    roll_step: int = 5\n",
    "    do_rolling: bool = True\n",
    "\n",
    "    out_csv: str = \"level80_tcopula_panel.csv\"\n",
    "    out_json: str = \"level80_tcopula_summary.json\"\n",
    "\n",
    "\n",
    "# ----------------------------- Numeric helpers -----------------------------\n",
    "def trapz_compat(y: np.ndarray, x: np.ndarray) -> float:\n",
    "    # numpy has trapezoid (new) and trapz (old); we support both\n",
    "    if hasattr(np, \"trapezoid\"):\n",
    "        return float(np.trapezoid(y, x))\n",
    "    return float(np.trapz(y, x))\n",
    "\n",
    "\n",
    "def ensure_pos_def_corr(corr: np.ndarray, shrink: float = 0.05) -> np.ndarray:\n",
    "    n = corr.shape[0]\n",
    "    corr = (1.0 - shrink) * corr + shrink * np.eye(n)\n",
    "    corr = 0.5 * (corr + corr.T)\n",
    "\n",
    "    vals, vecs = np.linalg.eigh(corr)\n",
    "    vals = np.clip(vals, 1e-8, None)\n",
    "    corr_pd = vecs @ np.diag(vals) @ vecs.T\n",
    "\n",
    "    d = np.sqrt(np.diag(corr_pd))\n",
    "    corr_pd = corr_pd / np.outer(d, d)\n",
    "    corr_pd = np.clip(corr_pd, -0.9999, 0.9999)\n",
    "    np.fill_diagonal(corr_pd, 1.0)\n",
    "    return corr_pd\n",
    "\n",
    "\n",
    "def rank_to_uniform(x: np.ndarray) -> np.ndarray:\n",
    "    n, k = x.shape\n",
    "    u = np.empty((n, k), dtype=float)\n",
    "    for j in range(k):\n",
    "        order = np.argsort(x[:, j])\n",
    "        ranks = np.empty(n, dtype=float)\n",
    "        ranks[order] = np.arange(1, n + 1, dtype=float)\n",
    "        u[:, j] = ranks / (n + 1.0)\n",
    "    return np.clip(u, 1e-12, 1.0 - 1e-12)\n",
    "\n",
    "\n",
    "# ----------------------------- Data loader (robust yfinance) -----------------------------\n",
    "def _safe_close_series(px: pd.DataFrame, symbol: str) -> pd.Series:\n",
    "    \"\"\"\n",
    "    yfinance sometimes returns columns as:\n",
    "      - single-level: ['Open','High','Low','Close',...]\n",
    "      - multi-level:  (PriceField, Symbol) or (Symbol, PriceField)\n",
    "    We extract the Close series robustly.\n",
    "    \"\"\"\n",
    "    if isinstance(px.columns, pd.MultiIndex):\n",
    "        # try common patterns\n",
    "        for key in [(\"Close\", symbol), (symbol, \"Close\"), (\"Adj Close\", symbol), (symbol, \"Adj Close\")]:\n",
    "            if key in px.columns:\n",
    "                s = px[key].copy()\n",
    "                s.name = symbol\n",
    "                return s\n",
    "\n",
    "        # fallback: find any column whose second level matches symbol and first level contains Close\n",
    "        cols = [c for c in px.columns if (symbol in c and (\"Close\" in c or \"Adj Close\" in c))]\n",
    "        if cols:\n",
    "            s = px[cols[0]].copy()\n",
    "            s.name = symbol\n",
    "            return s\n",
    "\n",
    "        raise RuntimeError(f\"Could not locate Close column for {symbol} in MultiIndex columns: {px.columns}\")\n",
    "\n",
    "    # single-level\n",
    "    if \"Close\" in px.columns:\n",
    "        s = px[\"Close\"].copy()\n",
    "        s.name = symbol\n",
    "        return s\n",
    "    if \"Adj Close\" in px.columns:\n",
    "        s = px[\"Adj Close\"].copy()\n",
    "        s.name = symbol\n",
    "        return s\n",
    "\n",
    "    raise RuntimeError(f\"'Close' column missing for {symbol}. Columns: {list(px.columns)}\")\n",
    "\n",
    "\n",
    "def load_prices(symbols: Tuple[str, ...], start: str) -> pd.DataFrame:\n",
    "    frames = []\n",
    "    for s in symbols:\n",
    "        px = yf.download(s, start=start, auto_adjust=True, progress=False)\n",
    "        if px is None or px.empty:\n",
    "            raise RuntimeError(f\"No data returned for symbol: {s}\")\n",
    "        close = _safe_close_series(px, s)\n",
    "        frames.append(close)\n",
    "\n",
    "    prices = pd.concat(frames, axis=1).sort_index()\n",
    "    prices = prices.dropna(how=\"any\")\n",
    "    return prices\n",
    "\n",
    "\n",
    "def compute_log_returns(prices: pd.DataFrame) -> pd.DataFrame:\n",
    "    rets = np.log(prices).diff().dropna()\n",
    "    rets = rets.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "    return rets\n",
    "\n",
    "\n",
    "# ----------------------------- Student-t pdf/cdf/ppf (SciPy optional) -----------------------------\n",
    "def t_log_pdf_np(x: np.ndarray, nu: float) -> np.ndarray:\n",
    "    a = math.lgamma((nu + 1.0) / 2.0) - math.lgamma(nu / 2.0)\n",
    "    b = -0.5 * math.log(nu * math.pi)\n",
    "    c = -((nu + 1.0) / 2.0) * np.log1p((x * x) / nu)\n",
    "    return a + b + c\n",
    "\n",
    "\n",
    "def t_cdf_scalar_np(x: float, nu: float, n_steps: int = 4001) -> float:\n",
    "    if x == 0.0:\n",
    "        return 0.5\n",
    "    sign = 1.0 if x > 0 else -1.0\n",
    "    ax = abs(x)\n",
    "    xs = np.linspace(0.0, ax, int(n_steps))\n",
    "    f = np.exp(t_log_pdf_np(xs, nu))\n",
    "    area = trapz_compat(f, xs)\n",
    "    cdf = 0.5 + sign * area\n",
    "    return float(np.clip(cdf, 1e-12, 1.0 - 1e-12))\n",
    "\n",
    "\n",
    "def t_ppf_scalar_np(u: float, nu: float) -> float:\n",
    "    u = float(np.clip(u, 1e-12, 1.0 - 1e-12))\n",
    "    if u == 0.5:\n",
    "        return 0.0\n",
    "    if u < 0.5:\n",
    "        return -t_ppf_scalar_np(1.0 - u, nu)\n",
    "\n",
    "    lo, hi = 0.0, 10.0\n",
    "    while t_cdf_scalar_np(hi, nu) < u:\n",
    "        hi *= 2.0\n",
    "        if hi > 200.0:\n",
    "            break\n",
    "\n",
    "    for _ in range(80):\n",
    "        mid = 0.5 * (lo + hi)\n",
    "        cmid = t_cdf_scalar_np(mid, nu)\n",
    "        if cmid < u:\n",
    "            lo = mid\n",
    "        else:\n",
    "            hi = mid\n",
    "    return 0.5 * (lo + hi)\n",
    "\n",
    "\n",
    "def t_cdf(x: np.ndarray, nu: float) -> np.ndarray:\n",
    "    if SCIPY_OK:\n",
    "        return stats.t.cdf(x, df=nu)  # type: ignore\n",
    "    out = np.empty_like(x, dtype=float)\n",
    "    it = np.nditer(x, flags=[\"multi_index\"])\n",
    "    while not it.finished:\n",
    "        out[it.multi_index] = t_cdf_scalar_np(float(it[0]), nu)\n",
    "        it.iternext()\n",
    "    return out\n",
    "\n",
    "\n",
    "def t_ppf(u: np.ndarray, nu: float) -> np.ndarray:\n",
    "    u = np.clip(u, 1e-12, 1.0 - 1e-12)\n",
    "    if SCIPY_OK:\n",
    "        return stats.t.ppf(u, df=nu)  # type: ignore\n",
    "    out = np.empty_like(u, dtype=float)\n",
    "    it = np.nditer(u, flags=[\"multi_index\"])\n",
    "    while not it.finished:\n",
    "        out[it.multi_index] = t_ppf_scalar_np(float(it[0]), nu)\n",
    "        it.iternext()\n",
    "    return out\n",
    "\n",
    "\n",
    "# ----------------------------- Copula calibration -----------------------------\n",
    "def pseudo_log_likelihood_tcopula(U: np.ndarray, corr: np.ndarray, nu: float) -> float:\n",
    "    Z = t_ppf(U, nu)  # T x N\n",
    "    corr = ensure_pos_def_corr(corr, shrink=0.0)\n",
    "    try:\n",
    "        L = np.linalg.cholesky(corr)\n",
    "    except np.linalg.LinAlgError:\n",
    "        corr = ensure_pos_def_corr(corr, shrink=0.10)\n",
    "        L = np.linalg.cholesky(corr)\n",
    "\n",
    "    Y = np.linalg.solve(L, Z.T).T\n",
    "    q = np.sum(Y * Y, axis=1)\n",
    "    n = Z.shape[1]\n",
    "    logdet = 2.0 * np.sum(np.log(np.diag(L)))\n",
    "    mv_part = -0.5 * logdet - ((nu + n) / 2.0) * np.log1p(q / nu)\n",
    "\n",
    "    if SCIPY_OK:\n",
    "        uni_sum = np.sum(stats.t.logpdf(Z, df=nu), axis=1)  # type: ignore\n",
    "    else:\n",
    "        uni_sum = np.sum(t_log_pdf_np(Z, nu), axis=1)\n",
    "\n",
    "    return float(np.sum(mv_part - uni_sum))\n",
    "\n",
    "\n",
    "def calibrate_tcopula(rets: pd.DataFrame, nu_grid: Tuple[int, ...], corr_shrink: float) -> Dict:\n",
    "    X = rets.values\n",
    "    U = rank_to_uniform(X)\n",
    "\n",
    "    best = {\"nu\": None, \"pll\": -np.inf, \"corr\": None}\n",
    "    for nu in nu_grid:\n",
    "        Z = t_ppf(U, float(nu))\n",
    "        corr = np.corrcoef(Z, rowvar=False)\n",
    "        corr = ensure_pos_def_corr(corr, shrink=corr_shrink)\n",
    "        pll = pseudo_log_likelihood_tcopula(U, corr, float(nu))\n",
    "        if pll > best[\"pll\"]:\n",
    "            best = {\"nu\": int(nu), \"pll\": float(pll), \"corr\": corr}\n",
    "    return best\n",
    "\n",
    "\n",
    "# ----------------------------- Simulation from fitted t-copula -----------------------------\n",
    "def simulate_tcopula_returns(hist_rets: pd.DataFrame, corr: np.ndarray, nu: float, sims: int, seed: int) -> np.ndarray:\n",
    "    rng = np.random.default_rng(seed)\n",
    "    n = hist_rets.shape[1]\n",
    "    corr = ensure_pos_def_corr(corr, shrink=0.0)\n",
    "    L = np.linalg.cholesky(corr)\n",
    "\n",
    "    # Empirical marginals\n",
    "    hist_sorted = np.sort(hist_rets.values, axis=0)\n",
    "    T = hist_sorted.shape[0]\n",
    "\n",
    "    # Gaussian core\n",
    "    g = rng.standard_normal(size=(sims, n))\n",
    "    z = g @ L.T\n",
    "\n",
    "    # Scale mixture for multivariate t\n",
    "    w = rng.gamma(shape=nu / 2.0, scale=2.0, size=(sims, 1))\n",
    "    y = z / np.sqrt(w / nu)\n",
    "\n",
    "    # Map to uniforms then to empirical marginals\n",
    "    U = t_cdf(y, nu)\n",
    "    idx = np.floor(U * (T - 1)).astype(int)\n",
    "    idx = np.clip(idx, 0, T - 1)\n",
    "\n",
    "    sim = np.empty_like(U, dtype=float)\n",
    "    for j in range(n):\n",
    "        sim[:, j] = hist_sorted[idx[:, j], j]\n",
    "    return sim\n",
    "\n",
    "\n",
    "# ----------------------------- Risk metrics -----------------------------\n",
    "def var_es(pnl: np.ndarray, alpha: float) -> Dict[str, float]:\n",
    "    losses = -pnl\n",
    "    v = float(np.quantile(losses, alpha))\n",
    "    tail = losses[losses >= v]\n",
    "    es = float(np.mean(tail)) if tail.size else v\n",
    "    return {\"VaR\": v, \"ES\": es}\n",
    "\n",
    "\n",
    "def kupiec_pof(exceed: np.ndarray, alpha: float) -> Dict[str, float]:\n",
    "    n = exceed.size\n",
    "    x = int(np.sum(exceed))\n",
    "    p = 1.0 - float(alpha)\n",
    "    if n == 0:\n",
    "        return {\"LR\": float(\"nan\"), \"p_value\": float(\"nan\"), \"fail_rate\": float(\"nan\"), \"fails\": 0.0, \"n\": 0.0}\n",
    "\n",
    "    eps = 1e-12\n",
    "    phat = np.clip(x / n, eps, 1.0 - eps)\n",
    "    p = np.clip(p, eps, 1.0 - eps)\n",
    "\n",
    "    lr = -2.0 * ((n - x) * math.log((1.0 - p) / (1.0 - phat)) + x * math.log(p / phat))\n",
    "\n",
    "    if SCIPY_OK:\n",
    "        pval = 1.0 - stats.chi2.cdf(lr, df=1)  # type: ignore\n",
    "    else:\n",
    "        pval = float(\"nan\")\n",
    "\n",
    "    return {\"LR\": float(lr), \"p_value\": float(pval), \"fail_rate\": float(x / n), \"fails\": float(x), \"n\": float(n)}\n",
    "\n",
    "\n",
    "# ----------------------------- Rolling backtest -----------------------------\n",
    "def rolling_backtest(cfg: Config, rets: pd.DataFrame, w: np.ndarray) -> pd.DataFrame:\n",
    "    n = len(rets)\n",
    "    if n <= cfg.rolling_window + 5:\n",
    "        raise ValueError(\"Not enough data for rolling backtest window.\")\n",
    "\n",
    "    out = pd.DataFrame(index=rets.index)\n",
    "    out[\"port_ret\"] = rets.values @ w\n",
    "    out[\"pnl\"] = cfg.notional * out[\"port_ret\"]\n",
    "\n",
    "    for a in cfg.alphas:\n",
    "        out[f\"VaR_{a}\"] = np.nan\n",
    "        out[f\"ES_{a}\"] = np.nan\n",
    "        out[f\"exceed_{a}\"] = False\n",
    "\n",
    "    last_fit = -10**9\n",
    "    calib_cache = None\n",
    "\n",
    "    for t in range(cfg.rolling_window, n):\n",
    "        if (t - last_fit) >= cfg.roll_step or calib_cache is None:\n",
    "            hist = rets.iloc[t - cfg.rolling_window:t]\n",
    "            calib_cache = calibrate_tcopula(hist, cfg.nu_grid, cfg.corr_shrink)\n",
    "            last_fit = t\n",
    "\n",
    "        nu_hat = float(calib_cache[\"nu\"])\n",
    "        corr_hat = calib_cache[\"corr\"]\n",
    "        hist = rets.iloc[t - cfg.rolling_window:t]\n",
    "\n",
    "        sim = simulate_tcopula_returns(hist, corr_hat, nu_hat, cfg.sims, cfg.seed + t)\n",
    "        pnl_sim = cfg.notional * (sim @ w)\n",
    "\n",
    "        loss_real = -float(out.iloc[t][\"pnl\"])\n",
    "        for a in cfg.alphas:\n",
    "            r = var_es(pnl_sim, float(a))\n",
    "            out.iloc[t, out.columns.get_loc(f\"VaR_{a}\")] = r[\"VaR\"]\n",
    "            out.iloc[t, out.columns.get_loc(f\"ES_{a}\")] = r[\"ES\"]\n",
    "            out.iloc[t, out.columns.get_loc(f\"exceed_{a}\")] = bool(loss_real > r[\"VaR\"])\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "# ----------------------------- Pipeline -----------------------------\n",
    "def run_pipeline(cfg: Config) -> Tuple[pd.DataFrame, Dict]:\n",
    "    print(f\"[INFO] SciPy available: {SCIPY_OK}\")\n",
    "    print(f\"[INFO] Downloading prices for {cfg.symbols} from {cfg.start} ...\")\n",
    "    prices = load_prices(cfg.symbols, cfg.start)\n",
    "    rets = compute_log_returns(prices)\n",
    "    print(f\"[INFO] Got {len(prices)} price rows, {len(rets)} return rows, assets={rets.shape[1]}\")\n",
    "\n",
    "    n_assets = rets.shape[1]\n",
    "    if cfg.weights is None:\n",
    "        w = np.ones(n_assets, dtype=float) / n_assets\n",
    "    else:\n",
    "        w = np.asarray(cfg.weights, dtype=float)\n",
    "        if w.size != n_assets:\n",
    "            raise ValueError(f\"--weights length must be {n_assets}, got {w.size}\")\n",
    "        s = float(np.sum(w))\n",
    "        if abs(s) < 1e-12:\n",
    "            raise ValueError(\"weights sum to zero\")\n",
    "        w = w / s\n",
    "\n",
    "    # Full-sample fit\n",
    "    print(f\"[INFO] Calibrating full-sample t-copula nu over grid: {cfg.nu_grid} ...\")\n",
    "    calib = calibrate_tcopula(rets, cfg.nu_grid, cfg.corr_shrink)\n",
    "    nu_hat = float(calib[\"nu\"])\n",
    "    corr_hat = calib[\"corr\"]\n",
    "    print(f\"[INFO] Best nu={int(nu_hat)} (pseudo-LL={calib['pll']:.2f})\")\n",
    "\n",
    "    # Full-sample risk via simulation\n",
    "    sim = simulate_tcopula_returns(rets, corr_hat, nu_hat, cfg.sims, cfg.seed)\n",
    "    pnl_sim = cfg.notional * (sim @ w)\n",
    "    risk_full = {str(a): var_es(pnl_sim, float(a)) for a in cfg.alphas}\n",
    "\n",
    "    # Tail dependence proxy on historical ranks (q=5%)\n",
    "    q = 0.05\n",
    "    U_hist = rank_to_uniform(rets.values)\n",
    "    tail_dep = {}\n",
    "    cols = list(rets.columns)\n",
    "    for i in range(len(cols)):\n",
    "        for j in range(i + 1, len(cols)):\n",
    "            pij = float(np.mean((U_hist[:, i] < q) & (U_hist[:, j] < q)))\n",
    "            tail_dep[f\"{cols[i]}-{cols[j]}\"] = pij / q if q > 0 else float(\"nan\")\n",
    "\n",
    "    # Panel (daily)\n",
    "    panel = pd.DataFrame(index=rets.index)\n",
    "    panel[cols] = prices.reindex(panel.index)  # aligned prices\n",
    "    panel[[f\"ret_{c}\" for c in cols]] = rets.add_prefix(\"ret_\")\n",
    "    panel[\"port_ret\"] = rets.values @ w\n",
    "    panel[\"pnl\"] = cfg.notional * panel[\"port_ret\"]\n",
    "\n",
    "    rolling_summary = {}\n",
    "    if cfg.do_rolling:\n",
    "        print(f\"[INFO] Rolling backtest window={cfg.rolling_window}, step={cfg.roll_step}, sims={cfg.sims} ...\")\n",
    "        roll = rolling_backtest(cfg, rets, w)\n",
    "        for a in cfg.alphas:\n",
    "            panel[f\"VaR_{a}\"] = roll[f\"VaR_{a}\"]\n",
    "            panel[f\"ES_{a}\"] = roll[f\"ES_{a}\"]\n",
    "            panel[f\"exceed_{a}\"] = roll[f\"exceed_{a}\"]\n",
    "\n",
    "            valid = roll[f\"VaR_{a}\"].notna()\n",
    "            exc = roll.loc[valid, f\"exceed_{a}\"].values.astype(bool)\n",
    "            rolling_summary[str(a)] = {\"kupiec_pof\": kupiec_pof(exc, float(a))}\n",
    "\n",
    "    summary = {\n",
    "        \"config\": asdict(cfg),\n",
    "        \"scipy_available\": bool(SCIPY_OK),\n",
    "        \"data_window\": {\n",
    "            \"start\": str(rets.index.min().date()),\n",
    "            \"end\": str(rets.index.max().date()),\n",
    "            \"n_returns\": int(len(rets)),\n",
    "        },\n",
    "        \"calibration\": {\n",
    "            \"nu\": int(nu_hat),\n",
    "            \"pseudo_ll\": float(calib[\"pll\"]),\n",
    "            \"corr_shrink\": float(cfg.corr_shrink),\n",
    "        },\n",
    "        \"portfolio\": {\n",
    "            \"symbols\": list(cfg.symbols),\n",
    "            \"weights\": [float(x) for x in w.tolist()],\n",
    "            \"notional\": float(cfg.notional),\n",
    "        },\n",
    "        \"risk_fullsample\": risk_full,\n",
    "        \"tail_dependence_proxy_q05\": tail_dep,\n",
    "        \"rolling_backtest\": rolling_summary,\n",
    "    }\n",
    "    return panel, summary\n",
    "\n",
    "\n",
    "def save_outputs(panel: pd.DataFrame, summary: Dict, cfg: Config) -> None:\n",
    "    os.makedirs(os.path.dirname(cfg.out_csv) or \".\", exist_ok=True)\n",
    "    os.makedirs(os.path.dirname(cfg.out_json) or \".\", exist_ok=True)\n",
    "\n",
    "    panel.to_csv(cfg.out_csv)\n",
    "    with open(cfg.out_json, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "\n",
    "    print(f\"[OK] Saved panel → {cfg.out_csv}\")\n",
    "    print(f\"[OK] Saved summary → {cfg.out_json}\")\n",
    "    for a, d in summary[\"risk_fullsample\"].items():\n",
    "        print(f\"alpha={a}: VaR=${d['VaR']:.2f}, ES=${d['ES']:.2f}\")\n",
    "        if a in summary.get(\"rolling_backtest\", {}):\n",
    "            kp = summary[\"rolling_backtest\"][a][\"kupiec_pof\"]\n",
    "            print(f\"  Kupiec fail_rate={kp['fail_rate']:.4f}, LR={kp['LR']:.3f}, p={kp['p_value']}\")\n",
    "\n",
    "\n",
    "# ----------------------------- CLI -----------------------------\n",
    "def parse_args() -> Config:\n",
    "    p = argparse.ArgumentParser(description=\"Level-80: Fast t-Copula VaR/ES with Rolling Backtest (SciPy optional)\")\n",
    "\n",
    "    p.add_argument(\"--start\", type=str, default=\"2010-01-01\")\n",
    "    p.add_argument(\"--symbols\", nargs=\"+\", default=list(Config.symbols))\n",
    "    p.add_argument(\"--nu-grid\", nargs=\"+\", type=int, default=list(Config.nu_grid))\n",
    "    p.add_argument(\"--corr-shrink\", type=float, default=0.05)\n",
    "\n",
    "    p.add_argument(\"--sims\", type=int, default=50000)\n",
    "    p.add_argument(\"--seed\", type=int, default=42)\n",
    "    p.add_argument(\"--alphas\", nargs=\"+\", type=float, default=[0.95, 0.99])\n",
    "\n",
    "    p.add_argument(\"--weights\", nargs=\"+\", type=float, default=None)\n",
    "    p.add_argument(\"--notional\", type=float, default=1_000_000.0)\n",
    "\n",
    "    p.add_argument(\"--rolling-window\", type=int, default=750)\n",
    "    p.add_argument(\"--roll-step\", type=int, default=5)\n",
    "    p.add_argument(\"--no-rolling\", action=\"store_true\")\n",
    "\n",
    "    p.add_argument(\"--csv\", type=str, default=\"level80_tcopula_panel.csv\")\n",
    "    p.add_argument(\"--json\", type=str, default=\"level80_tcopula_summary.json\")\n",
    "\n",
    "    a = p.parse_args()\n",
    "    return Config(\n",
    "        symbols=tuple(a.symbols),\n",
    "        start=a.start,\n",
    "        nu_grid=tuple(a.nu_grid),\n",
    "        corr_shrink=float(a.corr_shrink),\n",
    "        sims=int(a.sims),\n",
    "        seed=int(a.seed),\n",
    "        alphas=tuple(float(x) for x in a.alphas),\n",
    "        weights=None if a.weights is None else list(float(x) for x in a.weights),\n",
    "        notional=float(a.notional),\n",
    "        rolling_window=int(a.rolling_window),\n",
    "        roll_step=int(a.roll_step),\n",
    "        do_rolling=(not a.no_rolling),\n",
    "        out_csv=a.csv,\n",
    "        out_json=a.json,\n",
    "    )\n",
    "\n",
    "\n",
    "def main() -> None:\n",
    "    cfg = parse_args()\n",
    "    panel, summary = run_pipeline(cfg)\n",
    "    save_outputs(panel, summary, cfg)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Jupyter/PyCharm cell shim: strip \"-f kernel.json\" etc.\n",
    "    import sys\n",
    "    sys.argv = [sys.argv[0]] + [\n",
    "        arg for arg in sys.argv[1:]\n",
    "        if arg != \"-f\" and not (arg.endswith(\".json\") and \"kernel\" in arg)\n",
    "    ]\n",
    "    main()\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] SciPy available: True\n",
      "[INFO] Downloading prices for ('SPY', 'QQQ', 'IWM', 'EFA', 'EEM', 'TLT', 'LQD', 'GLD') from 2010-01-01 ...\n",
      "[INFO] Got 4014 price rows, 4013 return rows, assets=8\n",
      "[INFO] Calibrating full-sample t-copula nu over grid: (4, 6, 8, 10, 15, 20) ...\n",
      "[INFO] Best nu=20 (pseudo-LL=42114.74)\n",
      "[INFO] Rolling backtest window=750, step=5, sims=50000 ...\n",
      "[OK] Saved panel → level80_tcopula_panel.csv\n",
      "[OK] Saved summary → level80_tcopula_summary.json\n",
      "alpha=0.95: VaR=$11218.90, ES=$16737.01\n",
      "  Kupiec fail_rate=0.0500, LR=0.000, p=0.9903854594840464\n",
      "alpha=0.99: VaR=$19750.46, ES=$26898.87\n",
      "  Kupiec fail_rate=0.0107, LR=0.170, p=0.6802400244256757\n"
     ]
    }
   ],
   "execution_count": 1
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
