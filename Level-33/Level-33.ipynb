{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-01T03:26:40.890834Z",
     "start_time": "2025-11-01T03:25:39.621908Z"
    }
   },
   "source": [
    "# Level-33 — Meta-Labeling + Stacking Ensemble + Isotonic Calibration + Fractional Kelly Sizing\n",
    "# Purged KFold with Embargo, triple-barrier labels, robust 1-D handling, no Series truth ambiguity.\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "\n",
    "from datetime import datetime, timedelta, timezone\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# ---------------------------- Config ----------------------------\n",
    "TICKER        = \"AAPL\"\n",
    "YEARS         = 3.0\n",
    "FREQ          = \"1D\"          # daily bars for label generation/features\n",
    "VOL_SPAN      = 50\n",
    "H_BARS        = 10            # vertical barrier in bars\n",
    "CUSUM_GRID    = [0.003, 0.004, 0.006, 0.008, 0.010]\n",
    "UP_M, DN_M    = 3.0, 3.0\n",
    "MIN_EVENTS    = 400\n",
    "\n",
    "KFOLDS        = 5\n",
    "EMBARGO_FRAC  = 0.01          # 1% time-embargo between folds to prevent leakage\n",
    "\n",
    "RNG_SEED      = 42\n",
    "TC_BP         = 5             # transaction cost in basis points\n",
    "THRESH_POLICY = \"median\"      # 'median' of per-fold opt thresholds\n",
    "BET_CAP       = 1.0           # cap Kelly size to avoid extreme leverage\n",
    "KELLY_FRACTION= 0.5           # fraction of Kelly\n",
    "\n",
    "np.random.seed(RNG_SEED)\n",
    "\n",
    "\n",
    "# ---------------------------- Utilities ----------------------------\n",
    "def utc_today():\n",
    "    return datetime.now(timezone.utc).date()\n",
    "\n",
    "def load_prices(ticker, years, freq=\"1D\"):\n",
    "    end = utc_today()\n",
    "    start = (datetime.now(timezone.utc) - timedelta(days=int(365*years + 20))).date()\n",
    "    df = yf.download(ticker, start=start, end=end, auto_adjust=True, progress=False)\n",
    "    if df.empty:\n",
    "        raise SystemExit(\"No price data downloaded.\")\n",
    "    s = df[\"Adj Close\"] if \"Adj Close\" in df.columns else df[\"Close\"]\n",
    "    s = s.asfreq(\"B\").ffill()\n",
    "    s.name = \"Close\"\n",
    "    return s\n",
    "\n",
    "def to_1d_series(x, index=None):\n",
    "    if isinstance(x, pd.Series):\n",
    "        s = x.astype(float).fillna(0.0)\n",
    "        return s\n",
    "    if isinstance(x, pd.DataFrame):\n",
    "        x = x.iloc[:, 0]\n",
    "        return x.astype(float).fillna(0.0)\n",
    "    arr = np.ravel(np.asarray(x, dtype=float))\n",
    "    if index is None:\n",
    "        index = pd.RangeIndex(len(arr))\n",
    "    return pd.Series(arr, index=index, dtype=float).fillna(0.0)\n",
    "\n",
    "def ewma_vol(r, span=50):\n",
    "    r = to_1d_series(r)\n",
    "    return r.ewm(span=span, adjust=False).std()\n",
    "\n",
    "def cusum_filter(r, threshold, index=None):\n",
    "    s = to_1d_series(r, index=index)\n",
    "    idx, vals = s.index, s.to_numpy()\n",
    "    s_pos = s_neg = 0.0\n",
    "    t_events = []\n",
    "    for i, x in enumerate(vals):\n",
    "        s_pos = max(0.0, s_pos + x)\n",
    "        s_neg = min(0.0, s_neg + x)\n",
    "        if s_pos > threshold:\n",
    "            s_pos = 0.0; t_events.append(idx[i])\n",
    "        elif s_neg < -threshold:\n",
    "            s_neg = 0.0; t_events.append(idx[i])\n",
    "    return pd.DatetimeIndex(t_events).unique().sort_values()\n",
    "\n",
    "def get_vertical_barriers(t_events, h, index):\n",
    "    if len(t_events) == 0:\n",
    "        return pd.Series(dtype=\"datetime64[ns]\")\n",
    "    out = {}\n",
    "    for t0 in t_events:\n",
    "        pos = index.get_indexer([t0])[0]\n",
    "        t1_pos = min(pos + h, len(index)-1)\n",
    "        out[t0] = index[t1_pos]\n",
    "    return pd.Series(out)\n",
    "\n",
    "def get_triple_barrier_labels(close, t_events, vbar, up_m, dn_m, daily_vol):\n",
    "    trgt = daily_vol.reindex(t_events).fillna(method=\"bfill\").fillna(method=\"ffill\")\n",
    "    df = pd.DataFrame({\"t1\": vbar.reindex(t_events), \"trgt\": trgt}, index=t_events).dropna()\n",
    "    rows = []\n",
    "    for t0, row in df.iterrows():\n",
    "        t1 = row[\"t1\"]\n",
    "        # c0 robust fetch\n",
    "        loc0 = close.loc[t0]\n",
    "        c0 = float(loc0.iloc[0]) if isinstance(loc0, pd.Series) else float(loc0)\n",
    "\n",
    "        up_lvl = c0 * (1 + up_m * float(row[\"trgt\"]))\n",
    "        dn_lvl = c0 * (1 - dn_m * float(row[\"trgt\"]))\n",
    "        seg = close.loc[t0:t1]\n",
    "        if isinstance(seg, pd.DataFrame):\n",
    "            seg = seg.iloc[:, 0]\n",
    "        path = pd.Series(seg.astype(float), index=seg.index)\n",
    "        if path.empty:\n",
    "            continue\n",
    "        path_up = path >= up_lvl\n",
    "        path_dn = path <= dn_lvl\n",
    "        hit_up = path_up.idxmax() if path_up.to_numpy().any() else None\n",
    "        hit_dn = path_dn.idxmax() if path_dn.to_numpy().any() else None\n",
    "\n",
    "        if (hit_up is not None) and (hit_dn is not None):\n",
    "            lbl = 1 if hit_up <= hit_dn else 0\n",
    "            t_end = hit_up if lbl == 1 else hit_dn\n",
    "        elif hit_up is not None:\n",
    "            lbl, t_end = 1, hit_up\n",
    "        elif hit_dn is not None:\n",
    "            lbl, t_end = 0, hit_dn\n",
    "        else:\n",
    "            c1 = float(path.iloc[-1])\n",
    "            lbl, t_end = (1 if c1 > c0 else 0), t1\n",
    "        rows.append((t0, t_end, lbl, float(row[\"trgt\"])))\n",
    "\n",
    "    if not rows:\n",
    "        return pd.DataFrame(columns=[\"t1\", \"label\", \"trgt\"])\n",
    "    return pd.DataFrame(rows, columns=[\"t0\", \"t1\", \"label\", \"trgt\"]).set_index(\"t0\")\n",
    "\n",
    "def make_features(close):\n",
    "    r = close.pct_change().fillna(0.0)\n",
    "    f = pd.DataFrame(index=close.index)\n",
    "    f[\"r1\"] = r\n",
    "    f[\"r5\"] = close.pct_change(5)\n",
    "    f[\"r10\"] = close.pct_change(10)\n",
    "    f[\"mom5\"] = close / close.shift(5) - 1\n",
    "    f[\"mom10\"] = close / close.shift(10) - 1\n",
    "    f[\"vol10\"] = r.rolling(10).std()\n",
    "    f[\"vol20\"] = r.rolling(20).std()\n",
    "    f[\"z20\"] = (close - close.rolling(20).mean()) / (1e-12 + close.rolling(20).std())\n",
    "    delta = close.diff()\n",
    "    up = delta.clip(lower=0).rolling(14).mean()\n",
    "    dn = (-delta.clip(upper=0)).rolling(14).mean()\n",
    "    rs = up / (1e-12 + dn)\n",
    "    f[\"rsi14\"] = 100 - 100 / (1 + rs)\n",
    "    return f.fillna(0.0)\n",
    "\n",
    "def adaptive_events_and_labels(close, rets, base_H, cusum_grid, vol_span, up_m, dn_m, min_events):\n",
    "    daily_vol = ewma_vol(rets, span=vol_span).clip(lower=1e-8)\n",
    "    used_thr = None\n",
    "    events_idx = pd.DatetimeIndex([])\n",
    "    for thr in cusum_grid:\n",
    "        ev = cusum_filter(rets, thr, index=close.index)\n",
    "        if len(ev) >= min_events:\n",
    "            used_thr, events_idx = thr, ev\n",
    "            break\n",
    "    if used_thr is None:\n",
    "        used_thr = min(cusum_grid)\n",
    "        events_idx = cusum_filter(rets, used_thr, index=close.index)\n",
    "\n",
    "    vbar = get_vertical_barriers(events_idx, base_H, close.index)\n",
    "    labels = get_triple_barrier_labels(close, events_idx, vbar, up_m, dn_m, daily_vol).dropna()\n",
    "    if labels.empty:\n",
    "        raise SystemExit(\"No labeled events. Adjust thresholds.\")\n",
    "    print(f\"[Adaptive] events={len(labels)}  H={base_H} thr≈{used_thr}\")\n",
    "    return labels, daily_vol, events_idx, base_H, float(used_thr)\n",
    "\n",
    "def sharpe_ratio(x):\n",
    "    s = x.std()\n",
    "    return 0.0 if s == 0 or np.isnan(s) else float(np.sqrt(252) * x.mean() / s)\n",
    "\n",
    "def drawdown(x):\n",
    "    cum = (1 + x).cumprod()\n",
    "    return float((cum / cum.cummax() - 1).min())\n",
    "\n",
    "def purged_folds(index, n_splits=5, embargo_frac=0.01):\n",
    "    \"\"\"Time-ordered purged folds with embargo to reduce leakage.\"\"\"\n",
    "    n = len(index)\n",
    "    embargo = int(np.floor(n * embargo_frac))\n",
    "    folds = []\n",
    "    kf = KFold(n_splits=n_splits, shuffle=False)\n",
    "    for tr_idx, va_idx in kf.split(np.arange(n)):\n",
    "        # embargo: remove neighbors around validation from training\n",
    "        va_min, va_max = va_idx.min(), va_idx.max()\n",
    "        cut_l = max(0, va_min - embargo)\n",
    "        cut_r = min(n, va_max + embargo + 1)\n",
    "        tr_mask = np.ones(n, dtype=bool)\n",
    "        tr_mask[cut_l:cut_r] = False\n",
    "        tr_mask[va_idx] = False\n",
    "        tr_idx_purged = np.where(tr_mask)[0]\n",
    "        folds.append((tr_idx_purged, va_idx))\n",
    "    return folds\n",
    "\n",
    "\n",
    "# ---------------------------- Main ----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # 1) Data\n",
    "    close = load_prices(TICKER, YEARS, FREQ)\n",
    "    rets  = close.pct_change().replace([np.inf, -np.inf], 0.0).fillna(0.0)\n",
    "\n",
    "    # 2) Events & Labels\n",
    "    labels, _, _, _, _ = adaptive_events_and_labels(\n",
    "        close, rets, H_BARS, CUSUM_GRID, VOL_SPAN, UP_M, DN_M, MIN_EVENTS\n",
    "    )\n",
    "\n",
    "    # 3) Features aligned to event times\n",
    "    feats = make_features(close)\n",
    "    X = feats.reindex(labels.index).dropna()\n",
    "    y = labels.loc[X.index, \"label\"].astype(int)\n",
    "\n",
    "    # 4) Scaling\n",
    "    scaler = StandardScaler()\n",
    "    Xs = scaler.fit_transform(X)\n",
    "\n",
    "    # 5) Base learners (diverse biases)\n",
    "    gb  = GradientBoostingClassifier(random_state=RNG_SEED)\n",
    "    rf  = RandomForestClassifier(n_estimators=300, max_depth=None, random_state=RNG_SEED, n_jobs=-1)\n",
    "    lr  = LogisticRegression(max_iter=20_000, solver=\"lbfgs\", random_state=RNG_SEED)\n",
    "\n",
    "    # Stacking with logistic meta-learner (will be calibrated later)\n",
    "    stack = StackingClassifier(\n",
    "        estimators=[(\"gb\", gb), (\"rf\", rf)],\n",
    "        final_estimator=LogisticRegression(max_iter=20_000, solver=\"lbfgs\", random_state=RNG_SEED),\n",
    "        passthrough=True, n_jobs=-1\n",
    "    )\n",
    "\n",
    "    # 6) Purged CV + Embargo + Calibration + Threshold search\n",
    "    folds = purged_folds(X.index, n_splits=KFOLDS, embargo_frac=EMBARGO_FRAC)\n",
    "    proba_oof = pd.Series(index=X.index, dtype=float)\n",
    "    thresh_per_fold = []\n",
    "    aucs = []\n",
    "\n",
    "    for tr_idx, va_idx in folds:\n",
    "        X_tr, y_tr = Xs[tr_idx], y.iloc[tr_idx]\n",
    "        X_va, y_va = Xs[va_idx], y.iloc[va_idx]\n",
    "\n",
    "        # Fit stack on train\n",
    "        stack.fit(X_tr, y_tr)\n",
    "\n",
    "        # Isotonic calibration on validation (prefit)\n",
    "        cal = CalibratedClassifierCV(estimator=stack, method=\"isotonic\", cv=\"prefit\")\n",
    "        cal.fit(X_va, y_va)\n",
    "\n",
    "        # Probabilities on fold's validation\n",
    "        p_va = cal.predict_proba(X_va)[:, 1]\n",
    "        proba_oof.iloc[va_idx] = p_va\n",
    "\n",
    "        # AUC for sanity\n",
    "        auc = roc_auc_score(y_va, p_va)\n",
    "        aucs.append(auc)\n",
    "\n",
    "        # Pick threshold maximizing Sharpe (long/flat) on validation\n",
    "        # Evaluate on a fine grid between 0.4..0.65 (typical meta-labeling range)\n",
    "        grid = np.linspace(0.4, 0.65, 26)\n",
    "        best_s, best_t = -np.inf, 0.5\n",
    "\n",
    "        # Next-day forward returns for events (aligned)\n",
    "        ret_next = close.pct_change().shift(-1).reindex(y_va.index).fillna(0.0).astype(float).to_numpy().ravel()\n",
    "        for t in grid:\n",
    "            sig = (p_va >= t).astype(float)\n",
    "            pnl = sig * ret_next\n",
    "            # apply TC on toggles\n",
    "            toggle = np.abs(np.diff(sig, prepend=0.0))\n",
    "            pnl -= toggle * (TC_BP / 1e4)\n",
    "            s = sharpe_ratio(pd.Series(pnl))\n",
    "            if s > best_s:\n",
    "                best_s, best_t = s, float(t)\n",
    "        thresh_per_fold.append(best_t)\n",
    "\n",
    "    # Aggregate CV stats\n",
    "    cv_auc = float(np.mean(aucs))\n",
    "    if THRESH_POLICY == \"median\":\n",
    "        chosen_thr = float(np.median(thresh_per_fold))\n",
    "    else:\n",
    "        chosen_thr = float(np.mean(thresh_per_fold))\n",
    "\n",
    "    print(f\"\\nCV AUC: {cv_auc:.3f}\")\n",
    "    print(f\"Chosen probability threshold ({THRESH_POLICY} of per-fold best): {chosen_thr:.2f}\")\n",
    "\n",
    "    # 7) Refit on all data, calibrate on last fold's validation style (split tail as \"calibration\" slice)\n",
    "    # Split last 15% as calibration (time-ordered)\n",
    "    n = len(X)\n",
    "    cut = int(np.floor(n * 0.85))\n",
    "    X_tr_all, y_tr_all = Xs[:cut], y.iloc[:cut]\n",
    "    X_cal,    y_cal    = Xs[cut:], y.iloc[cut:]\n",
    "\n",
    "    stack.fit(X_tr_all, y_tr_all)\n",
    "    cal_all = CalibratedClassifierCV(estimator=stack, method=\"isotonic\", cv=\"prefit\")\n",
    "    cal_all.fit(X_cal, y_cal)\n",
    "\n",
    "    proba_all = pd.Series(cal_all.predict_proba(Xs)[:, 1], index=X.index)\n",
    "\n",
    "    # 8) Fractional Kelly bet sizing on meta-probability\n",
    "    # Kelly (binary) = (p - q)/b with b=1, q=1-p => 2p-1 ; long-only: clip below 0 -> 0\n",
    "    raw_kelly = 2.0 * proba_all - 1.0\n",
    "    bet = raw_kelly.clip(lower=0.0, upper=BET_CAP) * KELLY_FRACTION\n",
    "\n",
    "    # 9) Backtest on event dates (long-only, fractional bet size), with TC\n",
    "    ret_next_all = close.pct_change().shift(-1).reindex(X.index).fillna(0.0).astype(float)\n",
    "    ret_np = ret_next_all.to_numpy().ravel()\n",
    "    bet_np = bet.to_numpy().ravel()\n",
    "\n",
    "    pnl_gross = bet_np * ret_np\n",
    "    toggle = np.abs(np.diff(bet_np, prepend=0.0))  # turnover proxy\n",
    "    tc = toggle * (TC_BP / 1e4)\n",
    "    pnl_net = pnl_gross - tc\n",
    "\n",
    "    pnl_s = pd.Series(pnl_net, index=X.index)\n",
    "\n",
    "    sharpe = sharpe_ratio(pnl_s)\n",
    "    cum = (1.0 + pnl_s).cumprod()\n",
    "\n",
    "    # CAGR with DatetimeIndex fallback\n",
    "    if isinstance(cum.index, pd.DatetimeIndex):\n",
    "        yrs = (cum.index[-1] - cum.index[0]).days / 365.25\n",
    "    else:\n",
    "        yrs = len(cum) / 252.0\n",
    "    cagr = float(cum.iloc[-1] ** (1 / max(1e-9, yrs)) - 1.0)\n",
    "    mdd = drawdown(pnl_s)\n",
    "    avg_turnover = float(toggle.mean())\n",
    "    avg_tc_bp = float((tc * 1e4).mean())\n",
    "\n",
    "    hit = float(((bet_np > 0) & (ret_np > 0)).sum() / max(1, (bet_np > 0).sum()))\n",
    "\n",
    "    print(\"\\n=== Level-33 — Meta-Labeling Stacking Backtest (long-only fractional Kelly) ===\")\n",
    "    print(f\"Sharpe: {sharpe:.3f}  CAGR: {100*cagr:.2f}%  MaxDD: {100*mdd:.2f}%  Hit%: {100*hit:.2f}%\")\n",
    "    print(f\"Avg turnover: {avg_turnover:.3f}  Avg TC (bp): {avg_tc_bp:.2f}\")\n",
    "\n",
    "    # 10) Save artifacts\n",
    "    cv_summary = pd.DataFrame({\n",
    "        \"fold\": np.arange(1, KFOLDS+1),\n",
    "        \"auc\": aucs,\n",
    "        \"best_thresh\": thresh_per_fold\n",
    "    })\n",
    "    cv_summary.to_csv(f\"{TICKER}_level33_cv_summary.csv\", index=False)\n",
    "\n",
    "    out = pd.DataFrame({\n",
    "        \"proba\": proba_all,\n",
    "        \"bet\": bet,\n",
    "        \"ret_next\": ret_next_all,\n",
    "        \"pnl_net\": pnl_s\n",
    "    }, index=X.index)\n",
    "    out.to_csv(f\"{TICKER}_level33_backtest.csv\")\n",
    "\n",
    "    print(f\"\\nSaved: {TICKER}_level33_cv_summary.csv, {TICKER}_level33_backtest.csv\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Adaptive] events=627  H=10 thr≈0.003\n",
      "\n",
      "CV AUC: 0.610\n",
      "Chosen probability threshold (median of per-fold best): 0.40\n",
      "\n",
      "=== Level-33 — Meta-Labeling Stacking Backtest (long-only fractional Kelly) ===\n",
      "Sharpe: -2.019  CAGR: -11.96%  MaxDD: -36.54%  Hit%: 51.29%\n",
      "Avg turnover: 0.090  Avg TC (bp): 0.45\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Data must be 1-dimensional, got ndarray of shape (627, 1) instead",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mValueError\u001B[39m                                Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[1]\u001B[39m\u001B[32m, line 343\u001B[39m\n\u001B[32m    336\u001B[39m cv_summary = pd.DataFrame({\n\u001B[32m    337\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mfold\u001B[39m\u001B[33m\"\u001B[39m: np.arange(\u001B[32m1\u001B[39m, KFOLDS+\u001B[32m1\u001B[39m),\n\u001B[32m    338\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mauc\u001B[39m\u001B[33m\"\u001B[39m: aucs,\n\u001B[32m    339\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mbest_thresh\u001B[39m\u001B[33m\"\u001B[39m: thresh_per_fold\n\u001B[32m    340\u001B[39m })\n\u001B[32m    341\u001B[39m cv_summary.to_csv(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mTICKER\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m_level33_cv_summary.csv\u001B[39m\u001B[33m\"\u001B[39m, index=\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[32m--> \u001B[39m\u001B[32m343\u001B[39m out = \u001B[43mpd\u001B[49m\u001B[43m.\u001B[49m\u001B[43mDataFrame\u001B[49m\u001B[43m(\u001B[49m\u001B[43m{\u001B[49m\n\u001B[32m    344\u001B[39m \u001B[43m    \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mproba\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mproba_all\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    345\u001B[39m \u001B[43m    \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mbet\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mbet\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    346\u001B[39m \u001B[43m    \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mret_next\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mret_next_all\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    347\u001B[39m \u001B[43m    \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mpnl_net\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mpnl_s\u001B[49m\n\u001B[32m    348\u001B[39m \u001B[43m\u001B[49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mindex\u001B[49m\u001B[43m=\u001B[49m\u001B[43mX\u001B[49m\u001B[43m.\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    349\u001B[39m out.to_csv(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mTICKER\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m_level33_backtest.csv\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m    351\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33mSaved: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mTICKER\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m_level33_cv_summary.csv, \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mTICKER\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m_level33_backtest.csv\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PyCharmMiscProject\\.venv\\Lib\\site-packages\\pandas\\core\\frame.py:778\u001B[39m, in \u001B[36mDataFrame.__init__\u001B[39m\u001B[34m(self, data, index, columns, dtype, copy)\u001B[39m\n\u001B[32m    772\u001B[39m     mgr = \u001B[38;5;28mself\u001B[39m._init_mgr(\n\u001B[32m    773\u001B[39m         data, axes={\u001B[33m\"\u001B[39m\u001B[33mindex\u001B[39m\u001B[33m\"\u001B[39m: index, \u001B[33m\"\u001B[39m\u001B[33mcolumns\u001B[39m\u001B[33m\"\u001B[39m: columns}, dtype=dtype, copy=copy\n\u001B[32m    774\u001B[39m     )\n\u001B[32m    776\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data, \u001B[38;5;28mdict\u001B[39m):\n\u001B[32m    777\u001B[39m     \u001B[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m778\u001B[39m     mgr = \u001B[43mdict_to_mgr\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mindex\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcolumns\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcopy\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcopy\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtyp\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmanager\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    779\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data, ma.MaskedArray):\n\u001B[32m    780\u001B[39m     \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mnumpy\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mma\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m mrecords\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PyCharmMiscProject\\.venv\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:503\u001B[39m, in \u001B[36mdict_to_mgr\u001B[39m\u001B[34m(data, index, columns, dtype, typ, copy)\u001B[39m\n\u001B[32m    499\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    500\u001B[39m         \u001B[38;5;66;03m# dtype check to exclude e.g. range objects, scalars\u001B[39;00m\n\u001B[32m    501\u001B[39m         arrays = [x.copy() \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(x, \u001B[33m\"\u001B[39m\u001B[33mdtype\u001B[39m\u001B[33m\"\u001B[39m) \u001B[38;5;28;01melse\u001B[39;00m x \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m arrays]\n\u001B[32m--> \u001B[39m\u001B[32m503\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43marrays_to_mgr\u001B[49m\u001B[43m(\u001B[49m\u001B[43marrays\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcolumns\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mindex\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtyp\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtyp\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconsolidate\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcopy\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PyCharmMiscProject\\.venv\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:119\u001B[39m, in \u001B[36marrays_to_mgr\u001B[39m\u001B[34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001B[39m\n\u001B[32m    116\u001B[39m         index = ensure_index(index)\n\u001B[32m    118\u001B[39m     \u001B[38;5;66;03m# don't force copy because getting jammed in an ndarray anyway\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m119\u001B[39m     arrays, refs = \u001B[43m_homogenize\u001B[49m\u001B[43m(\u001B[49m\u001B[43marrays\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mindex\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    120\u001B[39m     \u001B[38;5;66;03m# _homogenize ensures\u001B[39;00m\n\u001B[32m    121\u001B[39m     \u001B[38;5;66;03m#  - all(len(x) == len(index) for x in arrays)\u001B[39;00m\n\u001B[32m    122\u001B[39m     \u001B[38;5;66;03m#  - all(x.ndim == 1 for x in arrays)\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    125\u001B[39m \n\u001B[32m    126\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    127\u001B[39m     index = ensure_index(index)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PyCharmMiscProject\\.venv\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:629\u001B[39m, in \u001B[36m_homogenize\u001B[39m\u001B[34m(data, index, dtype)\u001B[39m\n\u001B[32m    626\u001B[39m         val = \u001B[38;5;28mdict\u001B[39m(val)\n\u001B[32m    627\u001B[39m     val = lib.fast_multiget(val, oindex._values, default=np.nan)\n\u001B[32m--> \u001B[39m\u001B[32m629\u001B[39m val = \u001B[43msanitize_array\u001B[49m\u001B[43m(\u001B[49m\u001B[43mval\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mindex\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcopy\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[32m    630\u001B[39m com.require_length_match(val, index)\n\u001B[32m    631\u001B[39m refs.append(\u001B[38;5;28;01mNone\u001B[39;00m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PyCharmMiscProject\\.venv\\Lib\\site-packages\\pandas\\core\\construction.py:633\u001B[39m, in \u001B[36msanitize_array\u001B[39m\u001B[34m(data, index, dtype, copy, allow_2d)\u001B[39m\n\u001B[32m    631\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    632\u001B[39m         data = np.array(data, copy=copy)\n\u001B[32m--> \u001B[39m\u001B[32m633\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43msanitize_array\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    634\u001B[39m \u001B[43m        \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    635\u001B[39m \u001B[43m        \u001B[49m\u001B[43mindex\u001B[49m\u001B[43m=\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    636\u001B[39m \u001B[43m        \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    637\u001B[39m \u001B[43m        \u001B[49m\u001B[43mcopy\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    638\u001B[39m \u001B[43m        \u001B[49m\u001B[43mallow_2d\u001B[49m\u001B[43m=\u001B[49m\u001B[43mallow_2d\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    639\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    641\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    642\u001B[39m     _sanitize_non_ordered(data)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PyCharmMiscProject\\.venv\\Lib\\site-packages\\pandas\\core\\construction.py:659\u001B[39m, in \u001B[36msanitize_array\u001B[39m\u001B[34m(data, index, dtype, copy, allow_2d)\u001B[39m\n\u001B[32m    656\u001B[39m             subarr = cast(np.ndarray, subarr)\n\u001B[32m    657\u001B[39m             subarr = maybe_infer_to_datetimelike(subarr)\n\u001B[32m--> \u001B[39m\u001B[32m659\u001B[39m subarr = \u001B[43m_sanitize_ndim\u001B[49m\u001B[43m(\u001B[49m\u001B[43msubarr\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mindex\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mallow_2d\u001B[49m\u001B[43m=\u001B[49m\u001B[43mallow_2d\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    661\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(subarr, np.ndarray):\n\u001B[32m    662\u001B[39m     \u001B[38;5;66;03m# at this point we should have dtype be None or subarr.dtype == dtype\u001B[39;00m\n\u001B[32m    663\u001B[39m     dtype = cast(np.dtype, dtype)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PyCharmMiscProject\\.venv\\Lib\\site-packages\\pandas\\core\\construction.py:718\u001B[39m, in \u001B[36m_sanitize_ndim\u001B[39m\u001B[34m(result, data, dtype, index, allow_2d)\u001B[39m\n\u001B[32m    716\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m allow_2d:\n\u001B[32m    717\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m result\n\u001B[32m--> \u001B[39m\u001B[32m718\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[32m    719\u001B[39m         \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mData must be 1-dimensional, got ndarray of shape \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdata.shape\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m instead\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    720\u001B[39m     )\n\u001B[32m    721\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m is_object_dtype(dtype) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(dtype, ExtensionDtype):\n\u001B[32m    722\u001B[39m     \u001B[38;5;66;03m# i.e. NumpyEADtype(\"O\")\u001B[39;00m\n\u001B[32m    724\u001B[39m     result = com.asarray_tuplesafe(data, dtype=np.dtype(\u001B[33m\"\u001B[39m\u001B[33mobject\u001B[39m\u001B[33m\"\u001B[39m))\n",
      "\u001B[31mValueError\u001B[39m: Data must be 1-dimensional, got ndarray of shape (627, 1) instead"
     ]
    }
   ],
   "execution_count": 1
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
